{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16878,"status":"ok","timestamp":1733218979914,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"5_LlJGDiiwLR","outputId":"93724558-dbf4-498f-a8c6-7e574d51e974"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Google Drive를 마운트하여 데이터셋 파일에 접근\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2833,"status":"ok","timestamp":1733218989400,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"KAFsWy4Ei8ZU","outputId":"a10ccfa0-dc62-4cad-c7af-fcfa5c4f2f9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"]}],"source":["!pip install huggingface_hub"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9771,"status":"ok","timestamp":1733219030588,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"MNhW1mLCjAqK","outputId":"61407a5f-8050-4efb-c9ca-72e8e351e050"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) y\n","Token is valid (permission: write).\n","The token `token` has been saved to /root/.cache/huggingface/stored_tokens\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `token`\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","source":["from huggingface_hub import HfApi, HfFolder, Repository\n","\n","# Hugging Face에 저장소 생성\n","repository_name = \"machine_anomaly_detection_model\"  # 저장소 이름 지정\n","api = HfApi()\n","repo_url = api.create_repo(repo_id=repository_name, private=False)\n","\n","# 저장소 클론\n","repo = Repository(local_dir=repository_name, clone_from=repo_url)\n","\n","# 필요한 파일 복사\n","import shutil\n","\n","files_to_upload = [\n","    '/content/drive/MyDrive/projects/carecruise_intern/model_transfer_pretrain_v3/pretrain_only_fan-6.h5',\n","    '/content/drive/MyDrive/projects/carecruise_intern/model_transfer_pretrain_v3/pretrain_only_pump-6.h5',\n","    '/content/drive/MyDrive/projects/carecruise_intern/model_transfer_pretrain_v3/pretrain_only_slider-6.h5',\n","    '/content/drive/MyDrive/projects/carecruise_intern/model_transfer_pretrain_v3/pretrain_only_valve-6.h5'\n","]\n","for file_path in files_to_upload:\n","    shutil.copy(file_path, repository_name)\n","\n","\n","\n","# Hugging Face에 파일 업로드\n","repo.push_to_hub(commit_message=\"Add machine_anomaly_detection_model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"HNMbhDKC58Kc","executionInfo":{"status":"ok","timestamp":1733219618862,"user_tz":-540,"elapsed":8337,"user":{"displayName":"최혜정","userId":"05709380919630694733"}},"outputId":"e2e13f4b-f246-4c8d-de3b-515396eef9dd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n","For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n","  warnings.warn(warning_message, FutureWarning)\n","Cloning https://huggingface.co/choihj0706/machine_anomaly_detection_model into local empty directory.\n","WARNING:huggingface_hub.repository:Cloning https://huggingface.co/choihj0706/machine_anomaly_detection_model into local empty directory.\n","To https://huggingface.co/choihj0706/machine_anomaly_detection_model\n","   92c1f4f..09e90fe  main -> main\n","\n","WARNING:huggingface_hub.repository:To https://huggingface.co/choihj0706/machine_anomaly_detection_model\n","   92c1f4f..09e90fe  main -> main\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'https://huggingface.co/choihj0706/machine_anomaly_detection_model/commit/09e90fe35712ca250e4eb9b749825a028290c049'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["repo = Repository(local_dir=\"musicgen_finetuned_chj\", clone_from=repo_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"id":"kHDxzAw8_YH7","executionInfo":{"status":"error","timestamp":1733220470301,"user_tz":-540,"elapsed":5,"user":{"displayName":"최혜정","userId":"05709380919630694733"}},"outputId":"c6e19dd0-0448-4fac-fd93-b2903a118469"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n","For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"error","ename":"OSError","evalue":"Tried to clone https://huggingface.co/username/musicgen_finetuned_chj in an unrelated git repository.\nIf you believe this is an error, please add a remote with the following URL: https://huggingface.co/username/musicgen_finetuned_chj.\nLocal path has its origin defined as: https://huggingface.co/choihj0706/machine_anomaly_detection_model\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-22b744ebf34f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"musicgen_finetuned_chj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mwarning_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarning_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclone_from\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, token)\u001b[0m\n\u001b[1;32m    722\u001b[0m                         \u001b[0mclean_local_remote_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"https://.*@\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                         \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\nLocal path has its origin defined as: {clean_local_remote_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Tried to clone https://huggingface.co/username/musicgen_finetuned_chj in an unrelated git repository.\nIf you believe this is an error, please add a remote with the following URL: https://huggingface.co/username/musicgen_finetuned_chj.\nLocal path has its origin defined as: https://huggingface.co/choihj0706/machine_anomaly_detection_model\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228,"referenced_widgets":["b3a84c5a7a49456f86cc94753a1116e6","5825f23a0ec248e09943910922cb194d","8e0f45393cbd406897a8cce44f04870f","6b7e5c39e1164cdf8f2a58565f752554","60dd222cfb1f4e0290ecfa049a8dd0f2","01cce141af454b6bbffc413ed5aaf67e","e42384a0b13545c1b32d1129ee7969f3","4aa41cf499d04e5c9a57fbabb89870ab","e01f844a06924068881a0b0417547410","1dc0e953514a49bf81861856b7c76447","6aec00a7bfb54364a7c7d811b68a4297"]},"executionInfo":{"elapsed":21343,"status":"ok","timestamp":1733220789403,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"bdvrhXXCjRfw","outputId":"db687835-d559-45d4-af62-653ef1ec43fa"},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a84c5a7a49456f86cc94753a1116e6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]}],"source":["from huggingface_hub import HfApi\n","\n","api = HfApi()\n","\n","# 파일 업로드\n","files_to_upload = {\n","    \"tokenizer_model.safetensors\": \"/content/drive/MyDrive/projects/carecruise_intern/finetune/t5/model.safetensors\",\n","    \"spiece.model\": \"/content/drive/MyDrive/projects/carecruise_intern/finetune/t5/spiece.model\",\n","    \"tokenizer.json\": \"/content/drive/MyDrive/projects/carecruise_intern/finetune/t5/tokenizer.json\",\n","    \"tokenizer_config.json\": \"/content/drive/MyDrive/projects/carecruise_intern/finetune/t5/tokenizer_config.json\",\n","    \"config.json\": \"/content/drive/MyDrive/projects/carecruise_intern/finetune/config.json\",\n","    \"model.safetensors\": \"/content/drive/MyDrive/projects/carecruise_intern/finetune/model.safetensors\"\n","}\n","\n","for filename, file_path in files_to_upload.items():\n","    api.upload_file(\n","        path_or_fileobj=file_path,\n","        path_in_repo=filename,\n","        repo_id=\"choihj0706/musicgen_finetuned_chj\"\n","    )"]},{"cell_type":"code","source":["!git remote -v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5_FTp2O_oRQ","executionInfo":{"status":"ok","timestamp":1733220539270,"user_tz":-540,"elapsed":325,"user":{"displayName":"최혜정","userId":"05709380919630694733"}},"outputId":"c72c3fde-9fe0-46dd-a9bc-55f860f0840f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":792,"referenced_widgets":["5b918b2e20cc4dbd9fade2c93a0722bc","2aa87deafc174738b09dd59e66c44e1b","5c4f485df6784c2e8f94480ea5f4c6cf","51996b96a93a4aaf89287e1c61a09146","c3868c30833e47bc85ab2669cefe8941","12eee4d7b785495d9dd55c99d6ef119e","3aa4ec6f6bc74dc38c09321506d62bde","3562ec7d1eba4cb6a8ea0563462fa03d","bf1b25ce568d490f90d6d7f34722af26","c666c1df7d7840998684ad53a20257b3","9b999984ba4743e98d3123b2d63ef3db"]},"executionInfo":{"elapsed":30629,"status":"error","timestamp":1733079523365,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"Nw5Ue0nDjuWB","outputId":"c088bc2a-9abb-4ded-a847-1333dd25dba2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b918b2e20cc4dbd9fade2c93a0722bc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/7.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"ValueError","evalue":"Config has to be initialized with text_encoder, audio_encoder and decoder config","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-9d889a9587c7>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Hugging Face MusicGen 모델 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"choihj0706/musicgen_finetuned_chj\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/processing_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;31m# Otherwise, load config, if it can be loaded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    300\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attn_implementation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attn_implementation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pruned_heads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/musicgen/configuration_musicgen.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"text_encoder\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"audio_encoder\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"decoder\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Config has to be initialized with text_encoder, audio_encoder and decoder config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mtext_encoder_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Config has to be initialized with text_encoder, audio_encoder and decoder config"]}],"source":["# Hugging Face 라이브러리 설치\n","!pip install transformers soundfile numpy\n","\n","from transformers import AutoProcessor, AutoModelForCausalLM\n","import numpy as np\n","from IPython.display import Audio\n","\n","# Hugging Face MusicGen 모델 설정\n","model_name = \"choihj0706/musicgen_finetuned_chj\"\n","processor = AutoProcessor.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# GPT 기반 프롬프트 생성 함수 (더미 함수로 대체)\n","def generate_music_prompt(emotion: str, style: str) -> str:\n","    \"\"\"\n","    사용자의 감정 상태와 스타일을 기반으로 음악 생성 프롬프트를 반환.\n","    \"\"\"\n","    return f\"Create a {style} music piece for {emotion} mood.\"\n","\n","# 음악 생성 함수\n","def generate_music_with_model(emotion: str, style: str):\n","    \"\"\"\n","    Hugging Face MusicGen 모델을 사용해 음악을 생성하고 코랩에서 재생.\n","    :param emotion: 사용자의 감정 상태 (예: 스트레스, 불안, 졸림).\n","    :param style: 음악 스타일 (예: 피아노, 백색 소음, 클래식).\n","    :return: 생성된 음악의 NumPy 배열.\n","    \"\"\"\n","    # GPT를 통해 음악 프롬프트 생성\n","    gpt_prompt = generate_music_prompt(emotion, style)\n","    print(f\"Generated Prompt: {gpt_prompt}\")\n","\n","    # MusicGen 모델로 음악 생성\n","    inputs = processor(text=gpt_prompt, return_tensors=\"pt\")  # 프롬프트 처리\n","    outputs = model.generate(**inputs, max_new_tokens=500, do_sample=True)  # 음악 생성\n","\n","    # MusicGen 모델 출력 데이터를 NumPy 배열로 변환\n","    # (주의: 모델 출력 구조에 따라 다를 수 있음. 출력 형태를 확인해야 함)\n","    audio_array = outputs[0].cpu().numpy() if isinstance(outputs, np.ndarray) else np.array(outputs[0].cpu().detach())\n","\n","    # 코랩에서 오디오 재생\n","    print(\"Playing generated music...\")\n","    return Audio(audio_array, rate=32000)  # 32kHz 샘플링 레이트로 재생\n","\n","# 테스트 실행\n","emotion = \"calm\"\n","style = \"piano\"\n","audio_player = generate_music_with_model(emotion, style)\n","audio_player"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":650},"executionInfo":{"elapsed":1225,"status":"error","timestamp":1733079580732,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"sBLc-9r5l688","outputId":"ff1778ac-453a-47d7-9f79-4462d6c232ba"},"outputs":[{"ename":"OSError","evalue":"choihj0706/musicgen_finetuned_chj does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/choihj0706/musicgen_finetuned_chj/tree/main' for available files.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/choihj0706/musicgen_finetuned_chj/resolve/main/preprocessor_config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;31m# If we can't, a HEAD request error is returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m     (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{response.status_code} Client Error.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"Entry Not Found for url: {response.url}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEntryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-674cb21c-488d46734819061b652bc514;076fd570-2887-4133-bf6f-083a2c706989)\n\nEntry Not Found for url: https://huggingface.co/choihj0706/musicgen_finetuned_chj/resolve/main/preprocessor_config.json.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e384b4bb4391>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Processor 및 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arguments_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m         \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m_get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0mattribute_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mfeature_extractor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_extractor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py\u001b[0m in \u001b[0;36mget_feature_extractor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 resolved_feature_extractor_file = cached_file(\n\u001b[0m\u001b[1;32m    510\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                     \u001b[0mfeature_extractor_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{subfolder}/config.json\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;34mf\"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: choihj0706/musicgen_finetuned_chj does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/choihj0706/musicgen_finetuned_chj/tree/main' for available files."]}],"source":["from transformers import MusicgenProcessor, MusicgenForConditionalGeneration\n","\n","# MusicGen 모델 로드\n","model_name = \"choihj0706/musicgen_finetuned_chj\"\n","\n","# Processor 및 모델 로드\n","processor = MusicgenProcessor.from_pretrained(model_name)\n","model = MusicgenForConditionalGeneration.from_pretrained(model_name)\n","\n","# 모델과 Processor 확인\n","print(\"Processor and Model loaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1733079818385,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"NSVzgmETlmLz","outputId":"85847f52-3bd9-4ece-d1c5-1f3de8effc0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["preprocessor_config.json 파일이 저장되었습니다.\n"]}],"source":["import json\n","preprocessor_config = {\n","    \"_name_or_path\": \"choihj0706/musicgen_finetuned_chj\",\n","    \"feature_extractor_type\": \"MusicgenProcessor\",\n","    \"sampling_rate\": 32000,\n","    \"return_attention_mask\": True,\n","    \"audio_format\": \"wav\",\n","    \"channels\": 1\n","}\n","\n","# JSON으로 저장\n","with open(\"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/preprocessor_config.json\", \"w\") as f:\n","    json.dump(preprocessor_config, f, indent=4)\n","\n","print(\"preprocessor_config.json 파일이 저장되었습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":2469,"status":"ok","timestamp":1733079939679,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"gNlInr4RmyKd","outputId":"54c846d7-a9dd-4ac4-a188-31cdaf5ade74"},"outputs":[{"name":"stderr","output_type":"stream","text":["To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   9ff53eb..fdbbb96  main -> main\n","\n","WARNING:huggingface_hub.repository:To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   9ff53eb..fdbbb96  main -> main\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/choihj0706/musicgen_finetuned_chj/commit/fdbbb96fcf3ba10e144459b236f02c93b50b1b4c'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi, HfFolder, Repository\n","\n","# 필요한 파일 복사\n","import shutil\n","\n","files_to_upload = [\n","    '/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/preprocessor_config.json'\n","]\n","for file_path in files_to_upload:\n","    shutil.copy(file_path, repository_name)\n","\n","\n","\n","# Hugging Face에 파일 업로드\n","repo.push_to_hub(commit_message=\"Add fine-tuned MusicGen model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":522,"referenced_widgets":["43a7fddbd2a44a6883ba449e7a71dd09","5c63462ef11a4deab699c0a97d9902dc","9a4c76392fd24d3e845af63107c51326","6a2298fcc4bb4ba596def63e570397e9","2d25baf756bc46638edce4da132b85f5","abc65f3ca99e4d7897287dd881469520","d3d800e788774ef096e3014ec83674a4","c8652ae1921f4f609ec11d90fa09e05f","4511f0011720407ca8680769b9b785a0","fbd031723227448abd28e6eeffa3a78e","c9a6a348142b48f782c24312e88eb23b"]},"executionInfo":{"elapsed":853,"status":"error","timestamp":1733079955701,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"T_CiusnrnQAC","outputId":"62259e73-4817-4cef-a7be-53fae7af5ab8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43a7fddbd2a44a6883ba449e7a71dd09","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OSError","evalue":"Can't load tokenizer for 'choihj0706/musicgen_finetuned_chj'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'choihj0706/musicgen_finetuned_chj' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-e384b4bb4391>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Processor 및 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arguments_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m         \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m_get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0mattribute_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2198\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'choihj0706/musicgen_finetuned_chj'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'choihj0706/musicgen_finetuned_chj' is the correct path to a directory containing all relevant files for a T5TokenizerFast tokenizer."]}],"source":["from transformers import MusicgenProcessor, MusicgenForConditionalGeneration\n","\n","# MusicGen 모델 로드\n","model_name = \"choihj0706/musicgen_finetuned_chj\"\n","\n","# Processor 및 모델 로드\n","processor = MusicgenProcessor.from_pretrained(model_name)\n","model = MusicgenForConditionalGeneration.from_pretrained(model_name)\n","\n","# 모델과 Processor 확인\n","print(\"Processor and Model loaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1733080355583,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"oSuwdYWLnmFk","outputId":"61001ea4-d908-4851-9b52-ce8eaf41b00f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tokenizer_config.json 파일이 저장되었습니다: /content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/tokenizer_config.json\n"]}],"source":["tokenizer_config = {\n","    \"name_or_path\": \"choihj0706/musicgen_finetuned_chj\",\n","    \"tokenizer_class\": \"T5Tokenizer\",\n","    \"special_tokens_map\": {\n","        \"unk_token\": \"<unk>\",\n","        \"bos_token\": \"<s>\",\n","        \"eos_token\": \"</s>\"\n","    },\n","    \"model_max_length\": 512,\n","    \"padding_side\": \"right\",\n","    \"truncation_side\": \"right\",\n","    \"word_dropout\": 0.3\n","}\n","\n","# JSON으로 저장\n","tokenizer_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/tokenizer_config.json\"\n","with open(tokenizer_path, \"w\") as f:\n","    json.dump(tokenizer_config, f, indent=4)\n","\n","print(f\"tokenizer_config.json 파일이 저장되었습니다: {tokenizer_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":2805,"status":"ok","timestamp":1733080412028,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"KVVzLKTdnXNd","outputId":"a27b2a94-fd1c-4c2e-bd9a-63e8cc769f12"},"outputs":[{"name":"stderr","output_type":"stream","text":["To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   fdbbb96..965e14f  main -> main\n","\n","WARNING:huggingface_hub.repository:To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   fdbbb96..965e14f  main -> main\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/choihj0706/musicgen_finetuned_chj/commit/965e14f7389123b38bd61ec7dec48b492ccc4918'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi, HfFolder, Repository\n","\n","# 필요한 파일 복사\n","import shutil\n","\n","files_to_upload = [\n","    '/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/tokenizer_config.json'\n","]\n","for file_path in files_to_upload:\n","    shutil.copy(file_path, repository_name)\n","\n","\n","\n","# Hugging Face에 파일 업로드\n","repo.push_to_hub(commit_message=\"Add fine-tuned MusicGen model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397,"referenced_widgets":["374ccbd720b24a43a803058a9351bdcd","22877f700d44481da7841573481758cb","d84736bbb0c44bc090dec69f1a0d84e7","c7da6872ecb24197a05696cfdfd8edf4","60c9d82417474c3389ebbed28633719a","fed0a4a7cbd142abb4213f4bd0eceee5","919cbdecebfb47a0848456c6942eade6","b1883356adf94d4c822b957cc1d5001c","2f36f83e19624e7a97c21af2b35afa8b","2fab30faf5d04d4fbbc755a9af69a773","696975d59eda4655bf07aa2117b0bb06"]},"executionInfo":{"elapsed":708,"status":"error","timestamp":1733080422820,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"Khw1R5bCpGJn","outputId":"9c1be136-f0dc-402c-f2cc-3ed86fac7d34"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"374ccbd720b24a43a803058a9351bdcd","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"not a string","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-e384b4bb4391>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Processor 및 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arguments_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m         \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m_get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0mattribute_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2211\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2213\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2214\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfrom_slow\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_tokenizer_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslow_tokenizer_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2251\u001b[0;31m             slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n\u001b[0m\u001b[1;32m   2252\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2447\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2448\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m             logger.info(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madditional_special_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_EncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_eos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_unk_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: not a string"]}],"source":["from transformers import MusicgenProcessor, MusicgenForConditionalGeneration\n","\n","# MusicGen 모델 로드\n","model_name = \"choihj0706/musicgen_finetuned_chj\"\n","\n","# Processor 및 모델 로드\n","processor = MusicgenProcessor.from_pretrained(model_name)\n","model = MusicgenForConditionalGeneration.from_pretrained(model_name)\n","\n","# 모델과 Processor 확인\n","print(\"Processor and Model loaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":652,"status":"ok","timestamp":1733081214210,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"48op2GlEpJTW","outputId":"056b20d3-5445-41f5-c643-b440e7356a64"},"outputs":[{"name":"stdout","output_type":"stream","text":["description 필드가 /content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/training_data.txt에 저장되었습니다.\n"]}],"source":["import json\n","\n","# JSONL 파일 경로\n","jsonl_file = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/egs/train/data.jsonl\"\n","output_text_file = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/training_data.txt\"\n","\n","# JSONL에서 description 필드 추출 및 저장\n","with open(jsonl_file, \"r\") as infile, open(output_text_file, \"w\") as outfile:\n","    for line in infile:\n","        data = json.loads(line)  # 각 라인을 JSON 객체로 변환\n","        if \"description\" in data:  # description 필드가 존재하는 경우만 처리\n","            outfile.write(data[\"description\"] + \"\\n\")\n","\n","print(f\"description 필드가 {output_text_file}에 저장되었습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5821,"status":"ok","timestamp":1733081298649,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"f6Z7njkDsc__","outputId":"576d825c-a02f-4fb2-c4df-907dacd88680"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"]}],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYHpeT6PsKhZ"},"outputs":[],"source":["import sentencepiece as spm\n","\n","# 텍스트 데이터 파일 경로\n","training_data_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/training_data.txt\"\n","\n","# SentencePiece 모델 학습\n","spm.SentencePieceTrainer.train(\n","    input=training_data_path,\n","    model_prefix=\"spiece\",\n","    vocab_size=32000,  # 어휘 크기 (조정 가능)\n","    model_type=\"unigram\",  # 모델 타입: unigram, bpe, char, word\n","    character_coverage=1.0,  # 전체 텍스트 커버리지 (1.0은 모든 문자 포함)\n","    pad_id=0,\n","    unk_id=1,\n","    bos_id=2,\n","    eos_id=3\n",")\n","\n","# 학습 결과: spiece.model 및 spiece.vocab 생성\n","print(\"SentencePiece 모델 학습 완료!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":302,"status":"ok","timestamp":1733081525155,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"bEeToTFMs7fw","outputId":"d2999da6-f381-4494-e9bd-7abdf8f81980"},"outputs":[{"name":"stdout","output_type":"stream","text":["SentencePiece 모델 학습 완료!\n","생성된 파일:\n","/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/spiece.model\n","/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/spiece.vocab\n"]}],"source":["import sentencepiece as spm\n","\n","# 학습 데이터 경로\n","training_data_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/training_data.txt\"  # 추출된 텍스트 데이터 파일\n","\n","# 모델 저장 경로 및 파일 이름(prefix)\n","output_dir = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune\"\n","output_prefix = f\"{output_dir}/spiece\"  # spiece.model 및 spiece.vocab 생성\n","\n","# SentencePiece 모델 학습\n","spm.SentencePieceTrainer.train(\n","    input=training_data_path,  # 학습할 텍스트 파일 경로\n","    model_prefix=output_prefix,  # 생성될 모델 파일의 이름(prefix)\n","    vocab_size=1500,  # 어휘 크기 (조정 가능)\n","    model_type=\"unigram\",  # 모델 타입 ('unigram', 'bpe', 'word', 'char')\n","    character_coverage=1.0,  # 텍스트 커버리지 (1.0은 모든 문자 포함)\n","    pad_id=0,  # [PAD] 토큰 ID\n","    unk_id=1,  # [UNK] 토큰 ID\n","    bos_id=2,  # [BOS] 토큰 ID\n","    eos_id=3,  # [EOS] 토큰 ID\n","    user_defined_symbols=[\"<sep>\", \"<cls>\", \"<mask>\"],  # 사용자 정의 토큰\n","    train_extremely_large_corpus=False  # 매우 큰 데이터셋 학습 시 True로 설정\n",")\n","\n","print(\"SentencePiece 모델 학습 완료!\")\n","print(\"생성된 파일:\")\n","print(f\"{output_prefix}.model\")\n","print(f\"{output_prefix}.vocab\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":2627,"status":"ok","timestamp":1733081601108,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"W3mEZgt9s-N8","outputId":"70d5400c-f30c-41ad-af43-932e40f25a9d"},"outputs":[{"name":"stderr","output_type":"stream","text":["To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   965e14f..f2d34c1  main -> main\n","\n","WARNING:huggingface_hub.repository:To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   965e14f..f2d34c1  main -> main\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/choihj0706/musicgen_finetuned_chj/commit/f2d34c1fc8f66d2022a1ff5cd73a5a64ecf1e25b'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi, HfFolder, Repository\n","\n","# 필요한 파일 복사\n","import shutil\n","\n","files_to_upload = [\n","    '/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/spiece.model',\n","    '/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/spiece.vocab'\n","]\n","for file_path in files_to_upload:\n","    shutil.copy(file_path, repository_name)\n","\n","\n","\n","# Hugging Face에 파일 업로드\n","repo.push_to_hub(commit_message=\"Add fine-tuned MusicGen model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452,"referenced_widgets":["328c535197b84b35b59810e24402bf3b","e101c846d3084e83934559e27696ab05","6699f9546e38463aa538e2ae3a7101c8","212e440165ce49f8bb240812c87cae21","f916402be3b8414589de99f17cf3ffe9","b8a4b2fffb1045b6a99ebbfb19fe6b24","efb19dfc296541ac96157016035dbce2","1e8232c8d486405e9ef994a902012561","e75773f1c42b43df8599bf0d3f3cc5ca","fdb22e1bb65541889e0d046d2ba79ddf","f71b0fbf998e4063af5aa8ac4ceff3b4"]},"executionInfo":{"elapsed":1526,"status":"error","timestamp":1733081639376,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"KAivzxajtqCw","outputId":"dd76d243-f6f4-494d-9ee3-2a3434144f62"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"328c535197b84b35b59810e24402bf3b","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/261k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]},{"ename":"ValueError","evalue":"Config has to be initialized with text_encoder, audio_encoder and decoder config","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-b7a71d5e331e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Processor 및 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processor and Model loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1824\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_fast_init\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1826\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3613\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   3614\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m             )\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attn_implementation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attn_implementation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pruned_heads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/musicgen/configuration_musicgen.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"text_encoder\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"audio_encoder\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"decoder\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Config has to be initialized with text_encoder, audio_encoder and decoder config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mtext_encoder_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Config has to be initialized with text_encoder, audio_encoder and decoder config"]}],"source":["from transformers import MusicgenProcessor, MusicgenForConditionalGeneration\n","\n","model_name = \"choihj0706/musicgen_finetuned_chj\"\n","\n","# Processor 및 모델 로드\n","processor = MusicgenProcessor.from_pretrained(model_name)\n","model = MusicgenForConditionalGeneration.from_pretrained(model_name)\n","\n","print(\"Processor and Model loaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1733082479071,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"3vd8Z9sXtyHU","outputId":"d588a75c-ba3f-4a2d-8d1a-9146bfae56fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["JSON 파일이 저장되었습니다: /content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/config.json\n"]}],"source":["import json\n","\n","# 수정된 설정 데이터\n","config_data = {\n","    \"model_type\": \"musicgen\",\n","    \"text_encoder\": {\n","        \"model_type\": \"t5\",\n","        \"name_or_path\": \"t5-base\",\n","        \"type\": \"T5EncoderModel\",\n","        \"config\": {\n","            \"vocab_size\": 32128,\n","            \"d_model\": 1024,\n","            \"num_layers\": 12,\n","            \"num_heads\": 16,\n","            \"dropout_rate\": 0.1\n","        }\n","    },\n","    \"audio_encoder\": {\n","        \"model_type\": \"wav2vec2\",\n","        \"name_or_path\": \"facebook/wav2vec2-large\",\n","        \"type\": \"AudioEncoder\",\n","        \"config\": {\n","            \"sample_rate\": 32000,\n","            \"num_channels\": 1,\n","            \"embedding_size\": 512\n","        }\n","    },\n","    \"decoder\": {\n","        \"model_type\": \"transformer\",\n","        \"name_or_path\": \"facebook/musicgen-large\",\n","        \"type\": \"TransformerDecoder\",\n","        \"config\": {\n","            \"d_model\": 1024,\n","            \"num_heads\": 16,\n","            \"num_layers\": 24,\n","            \"dropout_rate\": 0.1\n","        }\n","    },\n","    \"training\": {\n","        \"batch_size\": 16,\n","        \"num_epochs\": 100,\n","        \"learning_rate\": 0.0001,\n","        \"weight_decay\": 0.01,\n","        \"gradient_clipping\": 1.0\n","    },\n","    \"generation\": {\n","        \"sample_rate\": 32000,\n","        \"audio_format\": \"wav\",\n","        \"num_samples\": 5,\n","        \"max_duration\": 30.0,\n","        \"temperature\": 1.0,\n","        \"top_k\": 250,\n","        \"top_p\": 0.9\n","    },\n","    \"logging\": {\n","        \"log_tensorboard\": True,\n","        \"log_wandb\": True,\n","        \"wandb_project\": \"music_generation\",\n","        \"log_updates\": 10\n","    },\n","    \"hardware\": {\n","        \"device\": \"cuda\",\n","        \"num_gpus\": 4\n","    }\n","}\n","\n","# 저장 경로 설정\n","output_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/config.json\"\n","\n","# JSON 파일로 저장\n","with open(output_path, \"w\") as json_file:\n","    json.dump(config_data, json_file, indent=4)\n","\n","print(f\"JSON 파일이 저장되었습니다: {output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":2465,"status":"ok","timestamp":1733082484640,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"scl94FroulbH","outputId":"1bd998ca-44c9-49cc-97da-9f3d21f8a8ad"},"outputs":[{"name":"stderr","output_type":"stream","text":["To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   c328672..85bb8f8  main -> main\n","\n","WARNING:huggingface_hub.repository:To https://huggingface.co/choihj0706/musicgen_finetuned_chj\n","   c328672..85bb8f8  main -> main\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/choihj0706/musicgen_finetuned_chj/commit/85bb8f880dac65f9279e8df350144272f2ed99bd'"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi, HfFolder, Repository\n","\n","# 필요한 파일 복사\n","import shutil\n","\n","files_to_upload = [\n","    '/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/config.json'\n","]\n","for file_path in files_to_upload:\n","    shutil.copy(file_path, repository_name)\n","\n","\n","\n","# Hugging Face에 파일 업로드\n","repo.push_to_hub(commit_message=\"Add fine-tuned MusicGen model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":468,"referenced_widgets":["e5a39ac667c34cb6a9743192595b1580","60f36bc310a244b6b86bbc244f5c18da","febd44b0d8514717b8dcd3101d086567","0010b3fe765b497ab0025ce2cc374665","22a95d5220b64c5090942342e82ed69c","f1c2dbbc7bbd49c5bafadc642a268b95","4f424a6764a3498e8d34a891c63dae3e","9eff5cc841e84fb0b2523aec9bb7bef9","b6aefe882eb24ebbac9f50d69b5cc908","493c6c6cb1ce4a909c1c9d79c433b8f0","026228de01cd4a3d924809e2c8dd9cb4"]},"executionInfo":{"elapsed":1860,"status":"error","timestamp":1733082488898,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"0LbfSa25u8dc","outputId":"9d623bb8-e173-4295-8b6c-085d795f348a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5a39ac667c34cb6a9743192595b1580","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OSError","evalue":"choihj0706/musicgen_finetuned_chj does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-b7a71d5e331e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Processor 및 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processor and Model loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1824\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_fast_init\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1826\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3911\u001b[0m                                 )\n\u001b[1;32m   3912\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3913\u001b[0;31m                                 raise EnvironmentError(\n\u001b[0m\u001b[1;32m   3914\u001b[0m                                     \u001b[0;34mf\"{pretrained_model_name_or_path} does not appear to have a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3915\u001b[0m                                     \u001b[0;34mf\" {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: choihj0706/musicgen_finetuned_chj does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack."]}],"source":["from transformers import MusicgenProcessor, MusicgenForConditionalGeneration\n","\n","model_name = \"choihj0706/musicgen_finetuned_chj\"\n","\n","# Processor 및 모델 로드\n","processor = MusicgenProcessor.from_pretrained(model_name)\n","model = MusicgenForConditionalGeneration.from_pretrained(model_name)\n","\n","print(\"Processor and Model loaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"executionInfo":{"elapsed":16776,"status":"error","timestamp":1733082809054,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"gESPFt4wvGcm","outputId":"1872c6f7-f69f-4bfc-8fd7-a3d6ff1bd832"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-39-b91578cdc32d>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(model_path)\n"]},{"ename":"ValueError","evalue":"Config: config.json has to be of type <class 'transformers.models.musicgen.configuration_musicgen.MusicgenConfig'>","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-b91578cdc32d>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Hugging Face 모델 로드 (예: Musicgen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenForConditionalGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/musicgen/modeling_musicgen.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, text_encoder, audio_encoder, decoder)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Config: {config} has to be of type {self.config_class}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention_hidden_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Config: config.json has to be of type <class 'transformers.models.musicgen.configuration_musicgen.MusicgenConfig'>"]}],"source":["import torch\n","from transformers import MusicgenForConditionalGeneration\n","\n","# 모델 및 state_dict 불러오기\n","model_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\"\n","state_dict = torch.load(model_path)\n","\n","# Hugging Face 모델 로드 (예: Musicgen)\n","model = MusicgenForConditionalGeneration(config=\"config.json\")\n","model.load_state_dict(state_dict)\n","\n","# Hugging Face 형식으로 저장\n","output_dir = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune\"\n","model.save_pretrained(output_dir)\n","\n","print(f\"모델이 {output_dir}에 저장되었습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":32622,"status":"error","timestamp":1733082907976,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"vLPSObKyyL6a","outputId":"aa988e07-dbb7-4f3b-8bad-3a9924d53373"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-42-0113009c0eff>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(model_path)\n","Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n","  \"_name_or_path\": \"t5-base\",\n","  \"classifier_dropout\": 0.0,\n","  \"config\": {\n","    \"d_model\": 1024,\n","    \"dropout_rate\": 0.1,\n","    \"num_heads\": 16,\n","    \"num_layers\": 12,\n","    \"vocab_size\": 32128\n","  },\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"num_decoder_layers\": 6,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"transformers_version\": \"4.46.2\",\n","  \"type\": \"T5EncoderModel\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","Config of the audio_encoder: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model'> is overwritten by shared audio_encoder config: Wav2Vec2Config {\n","  \"_name_or_path\": \"facebook/wav2vec2-large\",\n","  \"activation_dropout\": 0.1,\n","  \"adapter_attn_dim\": null,\n","  \"adapter_kernel_size\": 3,\n","  \"adapter_stride\": 2,\n","  \"add_adapter\": false,\n","  \"apply_spec_augment\": true,\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 1,\n","  \"classifier_proj_size\": 256,\n","  \"codevector_dim\": 256,\n","  \"config\": {\n","    \"embedding_size\": 512,\n","    \"num_channels\": 1,\n","    \"sample_rate\": 32000\n","  },\n","  \"contrastive_logits_temperature\": 0.1,\n","  \"conv_bias\": false,\n","  \"conv_dim\": [\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512\n","  ],\n","  \"conv_kernel\": [\n","    10,\n","    3,\n","    3,\n","    3,\n","    3,\n","    2,\n","    2\n","  ],\n","  \"conv_stride\": [\n","    5,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2\n","  ],\n","  \"ctc_loss_reduction\": \"sum\",\n","  \"ctc_zero_infinity\": false,\n","  \"diversity_loss_weight\": 0.1,\n","  \"do_stable_layer_norm\": false,\n","  \"eos_token_id\": 2,\n","  \"feat_extract_activation\": \"gelu\",\n","  \"feat_extract_norm\": \"group\",\n","  \"feat_proj_dropout\": 0.0,\n","  \"feat_quantizer_dropout\": 0.0,\n","  \"final_dropout\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"layerdrop\": 0.1,\n","  \"mask_feature_length\": 10,\n","  \"mask_feature_min_masks\": 0,\n","  \"mask_feature_prob\": 0.0,\n","  \"mask_time_length\": 10,\n","  \"mask_time_min_masks\": 2,\n","  \"mask_time_prob\": 0.05,\n","  \"model_type\": \"wav2vec2\",\n","  \"num_adapter_layers\": 3,\n","  \"num_attention_heads\": 12,\n","  \"num_codevector_groups\": 2,\n","  \"num_codevectors_per_group\": 320,\n","  \"num_conv_pos_embedding_groups\": 16,\n","  \"num_conv_pos_embeddings\": 128,\n","  \"num_feat_extract_layers\": 7,\n","  \"num_hidden_layers\": 12,\n","  \"num_negatives\": 100,\n","  \"output_hidden_size\": 768,\n","  \"pad_token_id\": 0,\n","  \"proj_codevector_dim\": 256,\n","  \"tdnn_dilation\": [\n","    1,\n","    2,\n","    3,\n","    1,\n","    1\n","  ],\n","  \"tdnn_dim\": [\n","    512,\n","    512,\n","    512,\n","    512,\n","    1500\n","  ],\n","  \"tdnn_kernel\": [\n","    5,\n","    3,\n","    3,\n","    1,\n","    1\n","  ],\n","  \"transformers_version\": \"4.46.2\",\n","  \"type\": \"AudioEncoder\",\n","  \"use_weighted_layer_sum\": false,\n","  \"vocab_size\": 32,\n","  \"xvector_output_dim\": 512\n","}\n","\n","Config of the decoder: <class 'transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM'> is overwritten by shared decoder config: MusicgenDecoderConfig {\n","  \"_name_or_path\": \"facebook/musicgen-large\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"attention_dropout\": 0.0,\n","  \"audio_channels\": 1,\n","  \"bos_token_id\": 2048,\n","  \"config\": {\n","    \"d_model\": 1024,\n","    \"dropout_rate\": 0.1,\n","    \"num_heads\": 16,\n","    \"num_layers\": 24\n","  },\n","  \"dropout\": 0.1,\n","  \"ffn_dim\": 4096,\n","  \"hidden_size\": 1024,\n","  \"initializer_factor\": 0.02,\n","  \"layerdrop\": 0.0,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"musicgen_decoder\",\n","  \"num_attention_heads\": 16,\n","  \"num_codebooks\": 4,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 2048,\n","  \"scale_embedding\": false,\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"4.46.2\",\n","  \"type\": \"TransformerDecoder\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 2048\n","}\n","\n"]},{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for MusicgenForConditionalGeneration:\n\tMissing key(s) in state_dict: \"text_encoder.shared.weight\", \"text_encoder.encoder.embed_tokens.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\", \"text_encoder.encoder.block.0.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.0.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.0.layer.1.layer_norm.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.1.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.1.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.1.layer.1.layer_norm.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.2.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.2.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.2.layer.1.layer_norm.weight\", \"text_encoder.encoder.block.3.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.3.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.3.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.3.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.3.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.3.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.3.layer.1.layer_norm.weight\", \"text_encoder.encoder.block.4.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.4.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.4.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.4.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.4.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.4.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.4.layer.1.layer_norm.weight\", \"text_encoder.encoder.block.5.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.5.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.5.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.5.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.5.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.5.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.5.layer.1.layer_norm.weight\", \"text_encoder.encoder.final_layer_norm.weight\", \"audio_encoder.masked_spec_embed\", \"audio_encoder.feature_extractor.conv_layers.0.conv.weight\", \"audio_encoder.feature_extractor.conv_layers.0.layer_norm.weight\", \"audio_encoder.feature_extractor.conv_layers.0.layer_norm.bias\", \"audio_encoder.feature_extractor.conv_layers.1.conv.weight\", \"audio_encoder.feature_extractor.conv_layers.2.conv.weight\", \"audio_encoder.feature_extractor.conv_layers.3.conv.weight\", \"audio_encoder.feature_extractor.conv_layers.4.conv.weight\", \"audio_encoder.feature_extractor.conv_layers.5.conv.weight\", \"audio_encoder.feature_extractor.conv_layers.6.conv.weight\", \"audio_encoder.feature_projection.layer_norm.weight\", \"audio_encoder.feature_projection.layer_norm.bias\", \"audio_encoder.feature_projection.projection.weight\", \"audio_encoder.feature_projection.projection.bias\", \"audio_encoder.encoder.pos_conv_embed.conv.bias\", \"audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original0\", \"audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original1\", \"audio_encoder.encoder.layer_norm.weight\", \"audio_encoder.encoder.layer_norm.bias\", \"audio_encoder.encoder.layers.0.attention.k_proj.weight\", \"audio_encoder.encoder.layers.0.attention.k_proj.bias\", \"audio_encoder.encoder.layers.0.attention.v_proj.weight\", \"audio_encoder.encoder.layers.0.attention.v_proj.bias\", \"audio_encoder.encoder.layers.0.attention.q_proj.weight\", \"audio_encoder.encoder.layers.0.attention.q_proj.bias\", \"audio_encoder.encoder.layers.0.attention.out_proj.weight\", \"audio_encoder.encoder.layers.0.attention.out_proj.bias\", \"audio_encoder.encoder.layers.0.layer_norm.weight\", \"audio_encoder.encoder.layers.0.layer_norm.bias\", \"audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.0.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.0.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.0.final_layer_norm.weight\", \"audio_encoder.encoder.layers.0.final_layer_norm.bias\", \"audio_encoder.encoder.layers.1.attention.k_proj.weight\", \"audio_encoder.encoder.layers.1.attention.k_proj.bias\", \"audio_encoder.encoder.layers.1.attention.v_proj.weight\", \"audio_encoder.encoder.layers.1.attention.v_proj.bias\", \"audio_encoder.encoder.layers.1.attention.q_proj.weight\", \"audio_encoder.encoder.layers.1.attention.q_proj.bias\", \"audio_encoder.encoder.layers.1.attention.out_proj.weight\", \"audio_encoder.encoder.layers.1.attention.out_proj.bias\", \"audio_encoder.encoder.layers.1.layer_norm.weight\", \"audio_encoder.encoder.layers.1.layer_norm.bias\", \"audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.1.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.1.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.1.final_layer_norm.weight\", \"audio_encoder.encoder.layers.1.final_layer_norm.bias\", \"audio_encoder.encoder.layers.2.attention.k_proj.weight\", \"audio_encoder.encoder.layers.2.attention.k_proj.bias\", \"audio_encoder.encoder.layers.2.attention.v_proj.weight\", \"audio_encoder.encoder.layers.2.attention.v_proj.bias\", \"audio_encoder.encoder.layers.2.attention.q_proj.weight\", \"audio_encoder.encoder.layers.2.attention.q_proj.bias\", \"audio_encoder.encoder.layers.2.attention.out_proj.weight\", \"audio_encoder.encoder.layers.2.attention.out_proj.bias\", \"audio_encoder.encoder.layers.2.layer_norm.weight\", \"audio_encoder.encoder.layers.2.layer_norm.bias\", \"audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.2.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.2.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.2.final_layer_norm.weight\", \"audio_encoder.encoder.layers.2.final_layer_norm.bias\", \"audio_encoder.encoder.layers.3.attention.k_proj.weight\", \"audio_encoder.encoder.layers.3.attention.k_proj.bias\", \"audio_encoder.encoder.layers.3.attention.v_proj.weight\", \"audio_encoder.encoder.layers.3.attention.v_proj.bias\", \"audio_encoder.encoder.layers.3.attention.q_proj.weight\", \"audio_encoder.encoder.layers.3.attention.q_proj.bias\", \"audio_encoder.encoder.layers.3.attention.out_proj.weight\", \"audio_encoder.encoder.layers.3.attention.out_proj.bias\", \"audio_encoder.encoder.layers.3.layer_norm.weight\", \"audio_encoder.encoder.layers.3.layer_norm.bias\", \"audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.3.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.3.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.3.final_layer_norm.weight\", \"audio_encoder.encoder.layers.3.final_layer_norm.bias\", \"audio_encoder.encoder.layers.4.attention.k_proj.weight\", \"audio_encoder.encoder.layers.4.attention.k_proj.bias\", \"audio_encoder.encoder.layers.4.attention.v_proj.weight\", \"audio_encoder.encoder.layers.4.attention.v_proj.bias\", \"audio_encoder.encoder.layers.4.attention.q_proj.weight\", \"audio_encoder.encoder.layers.4.attention.q_proj.bias\", \"audio_encoder.encoder.layers.4.attention.out_proj.weight\", \"audio_encoder.encoder.layers.4.attention.out_proj.bias\", \"audio_encoder.encoder.layers.4.layer_norm.weight\", \"audio_encoder.encoder.layers.4.layer_norm.bias\", \"audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.4.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.4.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.4.final_layer_norm.weight\", \"audio_encoder.encoder.layers.4.final_layer_norm.bias\", \"audio_encoder.encoder.layers.5.attention.k_proj.weight\", \"audio_encoder.encoder.layers.5.attention.k_proj.bias\", \"audio_encoder.encoder.layers.5.attention.v_proj.weight\", \"audio_encoder.encoder.layers.5.attention.v_proj.bias\", \"audio_encoder.encoder.layers.5.attention.q_proj.weight\", \"audio_encoder.encoder.layers.5.attention.q_proj.bias\", \"audio_encoder.encoder.layers.5.attention.out_proj.weight\", \"audio_encoder.encoder.layers.5.attention.out_proj.bias\", \"audio_encoder.encoder.layers.5.layer_norm.weight\", \"audio_encoder.encoder.layers.5.layer_norm.bias\", \"audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.5.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.5.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.5.final_layer_norm.weight\", \"audio_encoder.encoder.layers.5.final_layer_norm.bias\", \"audio_encoder.encoder.layers.6.attention.k_proj.weight\", \"audio_encoder.encoder.layers.6.attention.k_proj.bias\", \"audio_encoder.encoder.layers.6.attention.v_proj.weight\", \"audio_encoder.encoder.layers.6.attention.v_proj.bias\", \"audio_encoder.encoder.layers.6.attention.q_proj.weight\", \"audio_encoder.encoder.layers.6.attention.q_proj.bias\", \"audio_encoder.encoder.layers.6.attention.out_proj.weight\", \"audio_encoder.encoder.layers.6.attention.out_proj.bias\", \"audio_encoder.encoder.layers.6.layer_norm.weight\", \"audio_encoder.encoder.layers.6.layer_norm.bias\", \"audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.6.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.6.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.6.final_layer_norm.weight\", \"audio_encoder.encoder.layers.6.final_layer_norm.bias\", \"audio_encoder.encoder.layers.7.attention.k_proj.weight\", \"audio_encoder.encoder.layers.7.attention.k_proj.bias\", \"audio_encoder.encoder.layers.7.attention.v_proj.weight\", \"audio_encoder.encoder.layers.7.attention.v_proj.bias\", \"audio_encoder.encoder.layers.7.attention.q_proj.weight\", \"audio_encoder.encoder.layers.7.attention.q_proj.bias\", \"audio_encoder.encoder.layers.7.attention.out_proj.weight\", \"audio_encoder.encoder.layers.7.attention.out_proj.bias\", \"audio_encoder.encoder.layers.7.layer_norm.weight\", \"audio_encoder.encoder.layers.7.layer_norm.bias\", \"audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.7.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.7.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.7.final_layer_norm.weight\", \"audio_encoder.encoder.layers.7.final_layer_norm.bias\", \"audio_encoder.encoder.layers.8.attention.k_proj.weight\", \"audio_encoder.encoder.layers.8.attention.k_proj.bias\", \"audio_encoder.encoder.layers.8.attention.v_proj.weight\", \"audio_encoder.encoder.layers.8.attention.v_proj.bias\", \"audio_encoder.encoder.layers.8.attention.q_proj.weight\", \"audio_encoder.encoder.layers.8.attention.q_proj.bias\", \"audio_encoder.encoder.layers.8.attention.out_proj.weight\", \"audio_encoder.encoder.layers.8.attention.out_proj.bias\", \"audio_encoder.encoder.layers.8.layer_norm.weight\", \"audio_encoder.encoder.layers.8.layer_norm.bias\", \"audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.8.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.8.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.8.final_layer_norm.weight\", \"audio_encoder.encoder.layers.8.final_layer_norm.bias\", \"audio_encoder.encoder.layers.9.attention.k_proj.weight\", \"audio_encoder.encoder.layers.9.attention.k_proj.bias\", \"audio_encoder.encoder.layers.9.attention.v_proj.weight\", \"audio_encoder.encoder.layers.9.attention.v_proj.bias\", \"audio_encoder.encoder.layers.9.attention.q_proj.weight\", \"audio_encoder.encoder.layers.9.attention.q_proj.bias\", \"audio_encoder.encoder.layers.9.attention.out_proj.weight\", \"audio_encoder.encoder.layers.9.attention.out_proj.bias\", \"audio_encoder.encoder.layers.9.layer_norm.weight\", \"audio_encoder.encoder.layers.9.layer_norm.bias\", \"audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.9.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.9.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.9.final_layer_norm.weight\", \"audio_encoder.encoder.layers.9.final_layer_norm.bias\", \"audio_encoder.encoder.layers.10.attention.k_proj.weight\", \"audio_encoder.encoder.layers.10.attention.k_proj.bias\", \"audio_encoder.encoder.layers.10.attention.v_proj.weight\", \"audio_encoder.encoder.layers.10.attention.v_proj.bias\", \"audio_encoder.encoder.layers.10.attention.q_proj.weight\", \"audio_encoder.encoder.layers.10.attention.q_proj.bias\", \"audio_encoder.encoder.layers.10.attention.out_proj.weight\", \"audio_encoder.encoder.layers.10.attention.out_proj.bias\", \"audio_encoder.encoder.layers.10.layer_norm.weight\", \"audio_encoder.encoder.layers.10.layer_norm.bias\", \"audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.10.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.10.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.10.final_layer_norm.weight\", \"audio_encoder.encoder.layers.10.final_layer_norm.bias\", \"audio_encoder.encoder.layers.11.attention.k_proj.weight\", \"audio_encoder.encoder.layers.11.attention.k_proj.bias\", \"audio_encoder.encoder.layers.11.attention.v_proj.weight\", \"audio_encoder.encoder.layers.11.attention.v_proj.bias\", \"audio_encoder.encoder.layers.11.attention.q_proj.weight\", \"audio_encoder.encoder.layers.11.attention.q_proj.bias\", \"audio_encoder.encoder.layers.11.attention.out_proj.weight\", \"audio_encoder.encoder.layers.11.attention.out_proj.bias\", \"audio_encoder.encoder.layers.11.layer_norm.weight\", \"audio_encoder.encoder.layers.11.layer_norm.bias\", \"audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.weight\", \"audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.bias\", \"audio_encoder.encoder.layers.11.feed_forward.output_dense.weight\", \"audio_encoder.encoder.layers.11.feed_forward.output_dense.bias\", \"audio_encoder.encoder.layers.11.final_layer_norm.weight\", \"audio_encoder.encoder.layers.11.final_layer_norm.bias\", \"decoder.model.decoder.embed_tokens.0.weight\", \"decoder.model.decoder.embed_tokens.1.weight\", \"decoder.model.decoder.embed_tokens.2.weight\", \"decoder.model.decoder.embed_tokens.3.weight\", \"decoder.model.decoder.embed_positions.weights\", \"decoder.model.decoder.layers.0.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.0.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.0.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.0.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.0.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.0.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.0.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.0.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.0.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.0.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.0.fc1.weight\", \"decoder.model.decoder.layers.0.fc2.weight\", \"decoder.model.decoder.layers.0.final_layer_norm.weight\", \"decoder.model.decoder.layers.0.final_layer_norm.bias\", \"decoder.model.decoder.layers.1.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.1.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.1.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.1.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.1.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.1.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.1.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.1.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.1.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.1.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.1.fc1.weight\", \"decoder.model.decoder.layers.1.fc2.weight\", \"decoder.model.decoder.layers.1.final_layer_norm.weight\", \"decoder.model.decoder.layers.1.final_layer_norm.bias\", \"decoder.model.decoder.layers.2.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.2.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.2.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.2.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.2.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.2.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.2.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.2.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.2.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.2.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.2.fc1.weight\", \"decoder.model.decoder.layers.2.fc2.weight\", \"decoder.model.decoder.layers.2.final_layer_norm.weight\", \"decoder.model.decoder.layers.2.final_layer_norm.bias\", \"decoder.model.decoder.layers.3.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.3.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.3.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.3.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.3.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.3.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.3.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.3.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.3.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.3.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.3.fc1.weight\", \"decoder.model.decoder.layers.3.fc2.weight\", \"decoder.model.decoder.layers.3.final_layer_norm.weight\", \"decoder.model.decoder.layers.3.final_layer_norm.bias\", \"decoder.model.decoder.layers.4.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.4.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.4.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.4.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.4.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.4.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.4.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.4.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.4.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.4.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.4.fc1.weight\", \"decoder.model.decoder.layers.4.fc2.weight\", \"decoder.model.decoder.layers.4.final_layer_norm.weight\", \"decoder.model.decoder.layers.4.final_layer_norm.bias\", \"decoder.model.decoder.layers.5.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.5.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.5.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.5.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.5.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.5.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.5.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.5.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.5.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.5.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.5.fc1.weight\", \"decoder.model.decoder.layers.5.fc2.weight\", \"decoder.model.decoder.layers.5.final_layer_norm.weight\", \"decoder.model.decoder.layers.5.final_layer_norm.bias\", \"decoder.model.decoder.layers.6.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.6.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.6.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.6.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.6.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.6.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.6.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.6.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.6.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.6.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.6.fc1.weight\", \"decoder.model.decoder.layers.6.fc2.weight\", \"decoder.model.decoder.layers.6.final_layer_norm.weight\", \"decoder.model.decoder.layers.6.final_layer_norm.bias\", \"decoder.model.decoder.layers.7.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.7.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.7.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.7.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.7.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.7.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.7.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.7.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.7.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.7.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.7.fc1.weight\", \"decoder.model.decoder.layers.7.fc2.weight\", \"decoder.model.decoder.layers.7.final_layer_norm.weight\", \"decoder.model.decoder.layers.7.final_layer_norm.bias\", \"decoder.model.decoder.layers.8.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.8.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.8.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.8.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.8.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.8.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.8.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.8.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.8.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.8.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.8.fc1.weight\", \"decoder.model.decoder.layers.8.fc2.weight\", \"decoder.model.decoder.layers.8.final_layer_norm.weight\", \"decoder.model.decoder.layers.8.final_layer_norm.bias\", \"decoder.model.decoder.layers.9.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.9.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.9.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.9.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.9.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.9.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.9.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.9.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.9.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.9.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.9.fc1.weight\", \"decoder.model.decoder.layers.9.fc2.weight\", \"decoder.model.decoder.layers.9.final_layer_norm.weight\", \"decoder.model.decoder.layers.9.final_layer_norm.bias\", \"decoder.model.decoder.layers.10.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.10.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.10.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.10.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.10.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.10.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.10.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.10.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.10.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.10.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.10.fc1.weight\", \"decoder.model.decoder.layers.10.fc2.weight\", \"decoder.model.decoder.layers.10.final_layer_norm.weight\", \"decoder.model.decoder.layers.10.final_layer_norm.bias\", \"decoder.model.decoder.layers.11.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.11.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.11.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.11.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.11.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.11.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.11.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.11.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.11.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.11.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.11.fc1.weight\", \"decoder.model.decoder.layers.11.fc2.weight\", \"decoder.model.decoder.layers.11.final_layer_norm.weight\", \"decoder.model.decoder.layers.11.final_layer_norm.bias\", \"decoder.model.decoder.layers.12.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.12.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.12.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.12.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.12.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.12.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.12.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.12.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.12.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.12.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.12.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.12.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.12.fc1.weight\", \"decoder.model.decoder.layers.12.fc2.weight\", \"decoder.model.decoder.layers.12.final_layer_norm.weight\", \"decoder.model.decoder.layers.12.final_layer_norm.bias\", \"decoder.model.decoder.layers.13.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.13.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.13.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.13.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.13.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.13.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.13.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.13.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.13.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.13.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.13.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.13.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.13.fc1.weight\", \"decoder.model.decoder.layers.13.fc2.weight\", \"decoder.model.decoder.layers.13.final_layer_norm.weight\", \"decoder.model.decoder.layers.13.final_layer_norm.bias\", \"decoder.model.decoder.layers.14.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.14.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.14.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.14.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.14.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.14.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.14.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.14.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.14.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.14.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.14.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.14.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.14.fc1.weight\", \"decoder.model.decoder.layers.14.fc2.weight\", \"decoder.model.decoder.layers.14.final_layer_norm.weight\", \"decoder.model.decoder.layers.14.final_layer_norm.bias\", \"decoder.model.decoder.layers.15.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.15.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.15.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.15.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.15.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.15.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.15.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.15.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.15.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.15.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.15.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.15.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.15.fc1.weight\", \"decoder.model.decoder.layers.15.fc2.weight\", \"decoder.model.decoder.layers.15.final_layer_norm.weight\", \"decoder.model.decoder.layers.15.final_layer_norm.bias\", \"decoder.model.decoder.layers.16.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.16.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.16.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.16.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.16.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.16.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.16.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.16.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.16.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.16.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.16.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.16.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.16.fc1.weight\", \"decoder.model.decoder.layers.16.fc2.weight\", \"decoder.model.decoder.layers.16.final_layer_norm.weight\", \"decoder.model.decoder.layers.16.final_layer_norm.bias\", \"decoder.model.decoder.layers.17.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.17.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.17.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.17.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.17.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.17.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.17.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.17.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.17.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.17.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.17.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.17.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.17.fc1.weight\", \"decoder.model.decoder.layers.17.fc2.weight\", \"decoder.model.decoder.layers.17.final_layer_norm.weight\", \"decoder.model.decoder.layers.17.final_layer_norm.bias\", \"decoder.model.decoder.layers.18.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.18.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.18.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.18.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.18.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.18.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.18.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.18.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.18.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.18.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.18.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.18.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.18.fc1.weight\", \"decoder.model.decoder.layers.18.fc2.weight\", \"decoder.model.decoder.layers.18.final_layer_norm.weight\", \"decoder.model.decoder.layers.18.final_layer_norm.bias\", \"decoder.model.decoder.layers.19.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.19.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.19.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.19.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.19.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.19.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.19.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.19.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.19.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.19.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.19.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.19.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.19.fc1.weight\", \"decoder.model.decoder.layers.19.fc2.weight\", \"decoder.model.decoder.layers.19.final_layer_norm.weight\", \"decoder.model.decoder.layers.19.final_layer_norm.bias\", \"decoder.model.decoder.layers.20.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.20.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.20.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.20.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.20.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.20.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.20.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.20.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.20.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.20.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.20.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.20.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.20.fc1.weight\", \"decoder.model.decoder.layers.20.fc2.weight\", \"decoder.model.decoder.layers.20.final_layer_norm.weight\", \"decoder.model.decoder.layers.20.final_layer_norm.bias\", \"decoder.model.decoder.layers.21.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.21.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.21.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.21.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.21.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.21.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.21.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.21.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.21.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.21.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.21.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.21.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.21.fc1.weight\", \"decoder.model.decoder.layers.21.fc2.weight\", \"decoder.model.decoder.layers.21.final_layer_norm.weight\", \"decoder.model.decoder.layers.21.final_layer_norm.bias\", \"decoder.model.decoder.layers.22.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.22.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.22.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.22.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.22.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.22.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.22.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.22.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.22.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.22.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.22.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.22.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.22.fc1.weight\", \"decoder.model.decoder.layers.22.fc2.weight\", \"decoder.model.decoder.layers.22.final_layer_norm.weight\", \"decoder.model.decoder.layers.22.final_layer_norm.bias\", \"decoder.model.decoder.layers.23.self_attn.k_proj.weight\", \"decoder.model.decoder.layers.23.self_attn.v_proj.weight\", \"decoder.model.decoder.layers.23.self_attn.q_proj.weight\", \"decoder.model.decoder.layers.23.self_attn.out_proj.weight\", \"decoder.model.decoder.layers.23.self_attn_layer_norm.weight\", \"decoder.model.decoder.layers.23.self_attn_layer_norm.bias\", \"decoder.model.decoder.layers.23.encoder_attn.k_proj.weight\", \"decoder.model.decoder.layers.23.encoder_attn.v_proj.weight\", \"decoder.model.decoder.layers.23.encoder_attn.q_proj.weight\", \"decoder.model.decoder.layers.23.encoder_attn.out_proj.weight\", \"decoder.model.decoder.layers.23.encoder_attn_layer_norm.weight\", \"decoder.model.decoder.layers.23.encoder_attn_layer_norm.bias\", \"decoder.model.decoder.layers.23.fc1.weight\", \"decoder.model.decoder.layers.23.fc2.weight\", \"decoder.model.decoder.layers.23.final_layer_norm.weight\", \"decoder.model.decoder.layers.23.final_layer_norm.bias\", \"decoder.model.decoder.layer_norm.weight\", \"decoder.model.decoder.layer_norm.bias\", \"decoder.lm_heads.0.weight\", \"decoder.lm_heads.1.weight\", \"decoder.lm_heads.2.weight\", \"decoder.lm_heads.3.weight\", \"enc_to_dec_proj.weight\", \"enc_to_dec_proj.bias\". \n\tUnexpected key(s) in state_dict: \"best_state\", \"xp.cfg\", \"version\", \"exported\". ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-0113009c0eff>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 모델 초기화 및 가중치 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusicgenForConditionalGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Hugging Face 형식으로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MusicgenForConditionalGeneration:\n\tMissing key(s) in state_dict: \"text_encoder.shared.weight\", \"text_encoder.encoder.embed_tokens.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\", \"text_encoder.encoder.block.0.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.0.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.0.layer.1.layer_norm.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.1.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.1.layer.0.layer_norm.weight\", \"text_encoder.encoder.block.1.layer.1.DenseReluDense.wi.weight\", \"text_encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight\", \"text_encoder.encoder.block.1.layer.1.layer_norm.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.q.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.k.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.v.weight\", \"text_encoder.encoder.block.2.layer.0.SelfAttention.o.weight\", \"text_encoder.encoder.block.2.layer.0.layer_norm.weight\", \"...\n\tUnexpected key(s) in state_dict: \"best_state\", \"xp.cfg\", \"version\", \"exported\". "]}],"source":["from transformers import MusicgenConfig, MusicgenForConditionalGeneration\n","import torch\n","\n","# Config 파일 경로\n","config_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/config.json\"\n","\n","# MusicgenConfig로 구성 로드\n","config = MusicgenConfig.from_json_file(config_path)\n","\n","# 모델 및 state_dict 불러오기\n","model_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\"\n","state_dict = torch.load(model_path)\n","\n","# 모델 초기화 및 가중치 로드\n","model = MusicgenForConditionalGeneration(config=config)\n","model.load_state_dict(state_dict)\n","\n","# Hugging Face 형식으로 저장\n","output_dir = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune\"\n","model.save_pretrained(output_dir)\n","\n","print(f\"모델이 저장되었습니다: {output_dir}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3660,"status":"ok","timestamp":1733082963189,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"P8WTR2vSyY7f","outputId":"45a46d37-df41-4283-ba3c-64d4a88fb7b1"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-44-671c96994864>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(\"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\")\n"]},{"name":"stdout","output_type":"stream","text":["dict_keys(['best_state', 'xp.cfg', 'version', 'exported'])\n"]}],"source":["import torch\n","state_dict = torch.load(\"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\")\n","print(state_dict.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1733083036717,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"zdING_m4yxNo","outputId":"9a114ca4-61b9-43be-f39a-38a551dad216"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['pretrained', 'exported', 'version'])\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-45-866f06f50708>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(\"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/compression_state_dict.bin\")\n"]}],"source":["import torch\n","state_dict = torch.load(\"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/compression_state_dict.bin\")\n","print(state_dict.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3197,"status":"ok","timestamp":1733083063674,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"Xy4cABbPzHjj","outputId":"7694ef6a-9fe4-4d68-c36f-7c493d145314"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-46-6966a80a1a8c>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(state_dict_path)\n"]},{"name":"stdout","output_type":"stream","text":["Extracted 'best_state' keys: dict_keys(['condition_provider.conditioners.description.output_proj.weight', 'condition_provider.conditioners.description.output_proj.bias', 'emb.0.weight', 'emb.1.weight', 'emb.2.weight', 'emb.3.weight', 'transformer.layers.0.self_attn.in_proj_weight', 'transformer.layers.0.self_attn.out_proj.weight', 'transformer.layers.0.linear1.weight', 'transformer.layers.0.linear2.weight', 'transformer.layers.0.norm1.weight', 'transformer.layers.0.norm1.bias', 'transformer.layers.0.norm2.weight', 'transformer.layers.0.norm2.bias', 'transformer.layers.0.cross_attention.in_proj_weight', 'transformer.layers.0.cross_attention.out_proj.weight', 'transformer.layers.0.norm_cross.weight', 'transformer.layers.0.norm_cross.bias', 'transformer.layers.1.self_attn.in_proj_weight', 'transformer.layers.1.self_attn.out_proj.weight', 'transformer.layers.1.linear1.weight', 'transformer.layers.1.linear2.weight', 'transformer.layers.1.norm1.weight', 'transformer.layers.1.norm1.bias', 'transformer.layers.1.norm2.weight', 'transformer.layers.1.norm2.bias', 'transformer.layers.1.cross_attention.in_proj_weight', 'transformer.layers.1.cross_attention.out_proj.weight', 'transformer.layers.1.norm_cross.weight', 'transformer.layers.1.norm_cross.bias', 'transformer.layers.2.self_attn.in_proj_weight', 'transformer.layers.2.self_attn.out_proj.weight', 'transformer.layers.2.linear1.weight', 'transformer.layers.2.linear2.weight', 'transformer.layers.2.norm1.weight', 'transformer.layers.2.norm1.bias', 'transformer.layers.2.norm2.weight', 'transformer.layers.2.norm2.bias', 'transformer.layers.2.cross_attention.in_proj_weight', 'transformer.layers.2.cross_attention.out_proj.weight', 'transformer.layers.2.norm_cross.weight', 'transformer.layers.2.norm_cross.bias', 'transformer.layers.3.self_attn.in_proj_weight', 'transformer.layers.3.self_attn.out_proj.weight', 'transformer.layers.3.linear1.weight', 'transformer.layers.3.linear2.weight', 'transformer.layers.3.norm1.weight', 'transformer.layers.3.norm1.bias', 'transformer.layers.3.norm2.weight', 'transformer.layers.3.norm2.bias', 'transformer.layers.3.cross_attention.in_proj_weight', 'transformer.layers.3.cross_attention.out_proj.weight', 'transformer.layers.3.norm_cross.weight', 'transformer.layers.3.norm_cross.bias', 'transformer.layers.4.self_attn.in_proj_weight', 'transformer.layers.4.self_attn.out_proj.weight', 'transformer.layers.4.linear1.weight', 'transformer.layers.4.linear2.weight', 'transformer.layers.4.norm1.weight', 'transformer.layers.4.norm1.bias', 'transformer.layers.4.norm2.weight', 'transformer.layers.4.norm2.bias', 'transformer.layers.4.cross_attention.in_proj_weight', 'transformer.layers.4.cross_attention.out_proj.weight', 'transformer.layers.4.norm_cross.weight', 'transformer.layers.4.norm_cross.bias', 'transformer.layers.5.self_attn.in_proj_weight', 'transformer.layers.5.self_attn.out_proj.weight', 'transformer.layers.5.linear1.weight', 'transformer.layers.5.linear2.weight', 'transformer.layers.5.norm1.weight', 'transformer.layers.5.norm1.bias', 'transformer.layers.5.norm2.weight', 'transformer.layers.5.norm2.bias', 'transformer.layers.5.cross_attention.in_proj_weight', 'transformer.layers.5.cross_attention.out_proj.weight', 'transformer.layers.5.norm_cross.weight', 'transformer.layers.5.norm_cross.bias', 'transformer.layers.6.self_attn.in_proj_weight', 'transformer.layers.6.self_attn.out_proj.weight', 'transformer.layers.6.linear1.weight', 'transformer.layers.6.linear2.weight', 'transformer.layers.6.norm1.weight', 'transformer.layers.6.norm1.bias', 'transformer.layers.6.norm2.weight', 'transformer.layers.6.norm2.bias', 'transformer.layers.6.cross_attention.in_proj_weight', 'transformer.layers.6.cross_attention.out_proj.weight', 'transformer.layers.6.norm_cross.weight', 'transformer.layers.6.norm_cross.bias', 'transformer.layers.7.self_attn.in_proj_weight', 'transformer.layers.7.self_attn.out_proj.weight', 'transformer.layers.7.linear1.weight', 'transformer.layers.7.linear2.weight', 'transformer.layers.7.norm1.weight', 'transformer.layers.7.norm1.bias', 'transformer.layers.7.norm2.weight', 'transformer.layers.7.norm2.bias', 'transformer.layers.7.cross_attention.in_proj_weight', 'transformer.layers.7.cross_attention.out_proj.weight', 'transformer.layers.7.norm_cross.weight', 'transformer.layers.7.norm_cross.bias', 'transformer.layers.8.self_attn.in_proj_weight', 'transformer.layers.8.self_attn.out_proj.weight', 'transformer.layers.8.linear1.weight', 'transformer.layers.8.linear2.weight', 'transformer.layers.8.norm1.weight', 'transformer.layers.8.norm1.bias', 'transformer.layers.8.norm2.weight', 'transformer.layers.8.norm2.bias', 'transformer.layers.8.cross_attention.in_proj_weight', 'transformer.layers.8.cross_attention.out_proj.weight', 'transformer.layers.8.norm_cross.weight', 'transformer.layers.8.norm_cross.bias', 'transformer.layers.9.self_attn.in_proj_weight', 'transformer.layers.9.self_attn.out_proj.weight', 'transformer.layers.9.linear1.weight', 'transformer.layers.9.linear2.weight', 'transformer.layers.9.norm1.weight', 'transformer.layers.9.norm1.bias', 'transformer.layers.9.norm2.weight', 'transformer.layers.9.norm2.bias', 'transformer.layers.9.cross_attention.in_proj_weight', 'transformer.layers.9.cross_attention.out_proj.weight', 'transformer.layers.9.norm_cross.weight', 'transformer.layers.9.norm_cross.bias', 'transformer.layers.10.self_attn.in_proj_weight', 'transformer.layers.10.self_attn.out_proj.weight', 'transformer.layers.10.linear1.weight', 'transformer.layers.10.linear2.weight', 'transformer.layers.10.norm1.weight', 'transformer.layers.10.norm1.bias', 'transformer.layers.10.norm2.weight', 'transformer.layers.10.norm2.bias', 'transformer.layers.10.cross_attention.in_proj_weight', 'transformer.layers.10.cross_attention.out_proj.weight', 'transformer.layers.10.norm_cross.weight', 'transformer.layers.10.norm_cross.bias', 'transformer.layers.11.self_attn.in_proj_weight', 'transformer.layers.11.self_attn.out_proj.weight', 'transformer.layers.11.linear1.weight', 'transformer.layers.11.linear2.weight', 'transformer.layers.11.norm1.weight', 'transformer.layers.11.norm1.bias', 'transformer.layers.11.norm2.weight', 'transformer.layers.11.norm2.bias', 'transformer.layers.11.cross_attention.in_proj_weight', 'transformer.layers.11.cross_attention.out_proj.weight', 'transformer.layers.11.norm_cross.weight', 'transformer.layers.11.norm_cross.bias', 'transformer.layers.12.self_attn.in_proj_weight', 'transformer.layers.12.self_attn.out_proj.weight', 'transformer.layers.12.linear1.weight', 'transformer.layers.12.linear2.weight', 'transformer.layers.12.norm1.weight', 'transformer.layers.12.norm1.bias', 'transformer.layers.12.norm2.weight', 'transformer.layers.12.norm2.bias', 'transformer.layers.12.cross_attention.in_proj_weight', 'transformer.layers.12.cross_attention.out_proj.weight', 'transformer.layers.12.norm_cross.weight', 'transformer.layers.12.norm_cross.bias', 'transformer.layers.13.self_attn.in_proj_weight', 'transformer.layers.13.self_attn.out_proj.weight', 'transformer.layers.13.linear1.weight', 'transformer.layers.13.linear2.weight', 'transformer.layers.13.norm1.weight', 'transformer.layers.13.norm1.bias', 'transformer.layers.13.norm2.weight', 'transformer.layers.13.norm2.bias', 'transformer.layers.13.cross_attention.in_proj_weight', 'transformer.layers.13.cross_attention.out_proj.weight', 'transformer.layers.13.norm_cross.weight', 'transformer.layers.13.norm_cross.bias', 'transformer.layers.14.self_attn.in_proj_weight', 'transformer.layers.14.self_attn.out_proj.weight', 'transformer.layers.14.linear1.weight', 'transformer.layers.14.linear2.weight', 'transformer.layers.14.norm1.weight', 'transformer.layers.14.norm1.bias', 'transformer.layers.14.norm2.weight', 'transformer.layers.14.norm2.bias', 'transformer.layers.14.cross_attention.in_proj_weight', 'transformer.layers.14.cross_attention.out_proj.weight', 'transformer.layers.14.norm_cross.weight', 'transformer.layers.14.norm_cross.bias', 'transformer.layers.15.self_attn.in_proj_weight', 'transformer.layers.15.self_attn.out_proj.weight', 'transformer.layers.15.linear1.weight', 'transformer.layers.15.linear2.weight', 'transformer.layers.15.norm1.weight', 'transformer.layers.15.norm1.bias', 'transformer.layers.15.norm2.weight', 'transformer.layers.15.norm2.bias', 'transformer.layers.15.cross_attention.in_proj_weight', 'transformer.layers.15.cross_attention.out_proj.weight', 'transformer.layers.15.norm_cross.weight', 'transformer.layers.15.norm_cross.bias', 'transformer.layers.16.self_attn.in_proj_weight', 'transformer.layers.16.self_attn.out_proj.weight', 'transformer.layers.16.linear1.weight', 'transformer.layers.16.linear2.weight', 'transformer.layers.16.norm1.weight', 'transformer.layers.16.norm1.bias', 'transformer.layers.16.norm2.weight', 'transformer.layers.16.norm2.bias', 'transformer.layers.16.cross_attention.in_proj_weight', 'transformer.layers.16.cross_attention.out_proj.weight', 'transformer.layers.16.norm_cross.weight', 'transformer.layers.16.norm_cross.bias', 'transformer.layers.17.self_attn.in_proj_weight', 'transformer.layers.17.self_attn.out_proj.weight', 'transformer.layers.17.linear1.weight', 'transformer.layers.17.linear2.weight', 'transformer.layers.17.norm1.weight', 'transformer.layers.17.norm1.bias', 'transformer.layers.17.norm2.weight', 'transformer.layers.17.norm2.bias', 'transformer.layers.17.cross_attention.in_proj_weight', 'transformer.layers.17.cross_attention.out_proj.weight', 'transformer.layers.17.norm_cross.weight', 'transformer.layers.17.norm_cross.bias', 'transformer.layers.18.self_attn.in_proj_weight', 'transformer.layers.18.self_attn.out_proj.weight', 'transformer.layers.18.linear1.weight', 'transformer.layers.18.linear2.weight', 'transformer.layers.18.norm1.weight', 'transformer.layers.18.norm1.bias', 'transformer.layers.18.norm2.weight', 'transformer.layers.18.norm2.bias', 'transformer.layers.18.cross_attention.in_proj_weight', 'transformer.layers.18.cross_attention.out_proj.weight', 'transformer.layers.18.norm_cross.weight', 'transformer.layers.18.norm_cross.bias', 'transformer.layers.19.self_attn.in_proj_weight', 'transformer.layers.19.self_attn.out_proj.weight', 'transformer.layers.19.linear1.weight', 'transformer.layers.19.linear2.weight', 'transformer.layers.19.norm1.weight', 'transformer.layers.19.norm1.bias', 'transformer.layers.19.norm2.weight', 'transformer.layers.19.norm2.bias', 'transformer.layers.19.cross_attention.in_proj_weight', 'transformer.layers.19.cross_attention.out_proj.weight', 'transformer.layers.19.norm_cross.weight', 'transformer.layers.19.norm_cross.bias', 'transformer.layers.20.self_attn.in_proj_weight', 'transformer.layers.20.self_attn.out_proj.weight', 'transformer.layers.20.linear1.weight', 'transformer.layers.20.linear2.weight', 'transformer.layers.20.norm1.weight', 'transformer.layers.20.norm1.bias', 'transformer.layers.20.norm2.weight', 'transformer.layers.20.norm2.bias', 'transformer.layers.20.cross_attention.in_proj_weight', 'transformer.layers.20.cross_attention.out_proj.weight', 'transformer.layers.20.norm_cross.weight', 'transformer.layers.20.norm_cross.bias', 'transformer.layers.21.self_attn.in_proj_weight', 'transformer.layers.21.self_attn.out_proj.weight', 'transformer.layers.21.linear1.weight', 'transformer.layers.21.linear2.weight', 'transformer.layers.21.norm1.weight', 'transformer.layers.21.norm1.bias', 'transformer.layers.21.norm2.weight', 'transformer.layers.21.norm2.bias', 'transformer.layers.21.cross_attention.in_proj_weight', 'transformer.layers.21.cross_attention.out_proj.weight', 'transformer.layers.21.norm_cross.weight', 'transformer.layers.21.norm_cross.bias', 'transformer.layers.22.self_attn.in_proj_weight', 'transformer.layers.22.self_attn.out_proj.weight', 'transformer.layers.22.linear1.weight', 'transformer.layers.22.linear2.weight', 'transformer.layers.22.norm1.weight', 'transformer.layers.22.norm1.bias', 'transformer.layers.22.norm2.weight', 'transformer.layers.22.norm2.bias', 'transformer.layers.22.cross_attention.in_proj_weight', 'transformer.layers.22.cross_attention.out_proj.weight', 'transformer.layers.22.norm_cross.weight', 'transformer.layers.22.norm_cross.bias', 'transformer.layers.23.self_attn.in_proj_weight', 'transformer.layers.23.self_attn.out_proj.weight', 'transformer.layers.23.linear1.weight', 'transformer.layers.23.linear2.weight', 'transformer.layers.23.norm1.weight', 'transformer.layers.23.norm1.bias', 'transformer.layers.23.norm2.weight', 'transformer.layers.23.norm2.bias', 'transformer.layers.23.cross_attention.in_proj_weight', 'transformer.layers.23.cross_attention.out_proj.weight', 'transformer.layers.23.norm_cross.weight', 'transformer.layers.23.norm_cross.bias', 'out_norm.weight', 'out_norm.bias', 'linears.0.weight', 'linears.1.weight', 'linears.2.weight', 'linears.3.weight'])\n"]}],"source":["import torch\n","\n","# state_dict 파일 로드\n","state_dict_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\"\n","state_dict = torch.load(state_dict_path)\n","\n","# \"best_state\" 키에서 가중치 추출\n","if \"best_state\" in state_dict:\n","    best_state = state_dict[\"best_state\"]\n","    print(\"Extracted 'best_state' keys:\", best_state.keys())\n","else:\n","    raise ValueError(\"'best_state' 키를 찾을 수 없습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e3b6e2803a2d405ca4ebabc16b715e39","eb4e3728834a4b5c84aaefdde0da8ec1","121faed1dd3440409708e26a5914cefe","310cbb29781d47ab964a9042d44b1e04","60439a7e5ee7476a84097a2080c7e891","eaa7ab4eed7f4a5cb08cb2546932774b","3136c8d368bd4ef5a23de90ca46bee11","fe423cc4b9de480fbbcfbbbcf45bc8ea","46482e1ea1db4b36ad42fdff86b1c474","feabc9588aa545ba9c13ff8974d5c420","89c6619bb8d74a08993d0a7e8f63523b"]},"executionInfo":{"elapsed":23022,"status":"ok","timestamp":1733083193490,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"8_xuR_zXzNbd","outputId":"7e14b78e-240e-405f-a852-992f775afd64"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3b6e2803a2d405ca4ebabc16b715e39","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/7.87k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/encodec/modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n","Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n","  \"_name_or_path\": \"t5-base\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.46.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","Config of the audio_encoder: <class 'transformers.models.encodec.modeling_encodec.EncodecModel'> is overwritten by shared audio_encoder config: EncodecConfig {\n","  \"_name_or_path\": \"facebook/encodec_32khz\",\n","  \"architectures\": [\n","    \"EncodecModel\"\n","  ],\n","  \"audio_channels\": 1,\n","  \"chunk_length_s\": null,\n","  \"codebook_dim\": 128,\n","  \"codebook_size\": 2048,\n","  \"compress\": 2,\n","  \"dilation_growth_rate\": 2,\n","  \"hidden_size\": 128,\n","  \"kernel_size\": 7,\n","  \"last_kernel_size\": 7,\n","  \"model_type\": \"encodec\",\n","  \"norm_type\": \"weight_norm\",\n","  \"normalize\": false,\n","  \"num_filters\": 64,\n","  \"num_lstm_layers\": 2,\n","  \"num_residual_layers\": 1,\n","  \"overlap\": null,\n","  \"pad_mode\": \"reflect\",\n","  \"residual_kernel_size\": 3,\n","  \"sampling_rate\": 32000,\n","  \"target_bandwidths\": [\n","    2.2\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.46.2\",\n","  \"trim_right_ratio\": 1.0,\n","  \"upsampling_ratios\": [\n","    8,\n","    5,\n","    4,\n","    4\n","  ],\n","  \"use_causal_conv\": false,\n","  \"use_conv_shortcut\": false\n","}\n","\n","Config of the decoder: <class 'transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM'> is overwritten by shared decoder config: MusicgenDecoderConfig {\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"attention_dropout\": 0.0,\n","  \"audio_channels\": 1,\n","  \"bos_token_id\": 2048,\n","  \"classifier_dropout\": 0.0,\n","  \"dropout\": 0.1,\n","  \"ffn_dim\": 4096,\n","  \"hidden_size\": 1024,\n","  \"initializer_factor\": 0.02,\n","  \"layerdrop\": 0.0,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"musicgen_decoder\",\n","  \"num_attention_heads\": 16,\n","  \"num_codebooks\": 4,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 2048,\n","  \"scale_embedding\": false,\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"4.46.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 2048\n","}\n","\n"]},{"name":"stdout","output_type":"stream","text":["MusicGen 모델의 예상 가중치 키:\n","odict_keys(['text_encoder.shared.weight', 'text_encoder.encoder.embed_tokens.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'text_encoder.encoder.block.0.layer.0.layer_norm.weight', 'text_encoder.encoder.block.0.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.0.layer.1.layer_norm.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.1.layer.0.layer_norm.weight', 'text_encoder.encoder.block.1.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.1.layer.1.layer_norm.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.2.layer.0.layer_norm.weight', 'text_encoder.encoder.block.2.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.2.layer.1.layer_norm.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.3.layer.0.layer_norm.weight', 'text_encoder.encoder.block.3.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.3.layer.1.layer_norm.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.4.layer.0.layer_norm.weight', 'text_encoder.encoder.block.4.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.4.layer.1.layer_norm.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.5.layer.0.layer_norm.weight', 'text_encoder.encoder.block.5.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.5.layer.1.layer_norm.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.6.layer.0.layer_norm.weight', 'text_encoder.encoder.block.6.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.6.layer.1.layer_norm.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.7.layer.0.layer_norm.weight', 'text_encoder.encoder.block.7.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.7.layer.1.layer_norm.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.8.layer.0.layer_norm.weight', 'text_encoder.encoder.block.8.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.8.layer.1.layer_norm.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.9.layer.0.layer_norm.weight', 'text_encoder.encoder.block.9.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.9.layer.1.layer_norm.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.10.layer.0.layer_norm.weight', 'text_encoder.encoder.block.10.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.10.layer.1.layer_norm.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.11.layer.0.layer_norm.weight', 'text_encoder.encoder.block.11.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.11.layer.1.layer_norm.weight', 'text_encoder.encoder.final_layer_norm.weight', 'audio_encoder.encoder.layers.0.conv.bias', 'audio_encoder.encoder.layers.0.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.0.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.1.block.1.conv.bias', 'audio_encoder.encoder.layers.1.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.1.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.1.block.3.conv.bias', 'audio_encoder.encoder.layers.1.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.1.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.3.conv.bias', 'audio_encoder.encoder.layers.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.4.block.1.conv.bias', 'audio_encoder.encoder.layers.4.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.4.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.4.block.3.conv.bias', 'audio_encoder.encoder.layers.4.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.4.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.6.conv.bias', 'audio_encoder.encoder.layers.6.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.6.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.7.block.1.conv.bias', 'audio_encoder.encoder.layers.7.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.7.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.7.block.3.conv.bias', 'audio_encoder.encoder.layers.7.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.7.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.9.conv.bias', 'audio_encoder.encoder.layers.9.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.9.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.10.block.1.conv.bias', 'audio_encoder.encoder.layers.10.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.10.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.10.block.3.conv.bias', 'audio_encoder.encoder.layers.10.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.10.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.12.conv.bias', 'audio_encoder.encoder.layers.12.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.12.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.13.lstm.weight_ih_l0', 'audio_encoder.encoder.layers.13.lstm.weight_hh_l0', 'audio_encoder.encoder.layers.13.lstm.bias_ih_l0', 'audio_encoder.encoder.layers.13.lstm.bias_hh_l0', 'audio_encoder.encoder.layers.13.lstm.weight_ih_l1', 'audio_encoder.encoder.layers.13.lstm.weight_hh_l1', 'audio_encoder.encoder.layers.13.lstm.bias_ih_l1', 'audio_encoder.encoder.layers.13.lstm.bias_hh_l1', 'audio_encoder.encoder.layers.15.conv.bias', 'audio_encoder.encoder.layers.15.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.15.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.0.conv.bias', 'audio_encoder.decoder.layers.0.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.0.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.1.lstm.weight_ih_l0', 'audio_encoder.decoder.layers.1.lstm.weight_hh_l0', 'audio_encoder.decoder.layers.1.lstm.bias_ih_l0', 'audio_encoder.decoder.layers.1.lstm.bias_hh_l0', 'audio_encoder.decoder.layers.1.lstm.weight_ih_l1', 'audio_encoder.decoder.layers.1.lstm.weight_hh_l1', 'audio_encoder.decoder.layers.1.lstm.bias_ih_l1', 'audio_encoder.decoder.layers.1.lstm.bias_hh_l1', 'audio_encoder.decoder.layers.3.conv.bias', 'audio_encoder.decoder.layers.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.4.block.1.conv.bias', 'audio_encoder.decoder.layers.4.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.4.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.4.block.3.conv.bias', 'audio_encoder.decoder.layers.4.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.4.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.6.conv.bias', 'audio_encoder.decoder.layers.6.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.6.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.7.block.1.conv.bias', 'audio_encoder.decoder.layers.7.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.7.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.7.block.3.conv.bias', 'audio_encoder.decoder.layers.7.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.7.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.9.conv.bias', 'audio_encoder.decoder.layers.9.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.9.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.10.block.1.conv.bias', 'audio_encoder.decoder.layers.10.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.10.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.10.block.3.conv.bias', 'audio_encoder.decoder.layers.10.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.10.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.12.conv.bias', 'audio_encoder.decoder.layers.12.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.12.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.13.block.1.conv.bias', 'audio_encoder.decoder.layers.13.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.13.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.13.block.3.conv.bias', 'audio_encoder.decoder.layers.13.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.13.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.15.conv.bias', 'audio_encoder.decoder.layers.15.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.15.conv.parametrizations.weight.original1', 'audio_encoder.quantizer.layers.0.codebook.inited', 'audio_encoder.quantizer.layers.0.codebook.cluster_size', 'audio_encoder.quantizer.layers.0.codebook.embed', 'audio_encoder.quantizer.layers.0.codebook.embed_avg', 'audio_encoder.quantizer.layers.1.codebook.inited', 'audio_encoder.quantizer.layers.1.codebook.cluster_size', 'audio_encoder.quantizer.layers.1.codebook.embed', 'audio_encoder.quantizer.layers.1.codebook.embed_avg', 'audio_encoder.quantizer.layers.2.codebook.inited', 'audio_encoder.quantizer.layers.2.codebook.cluster_size', 'audio_encoder.quantizer.layers.2.codebook.embed', 'audio_encoder.quantizer.layers.2.codebook.embed_avg', 'audio_encoder.quantizer.layers.3.codebook.inited', 'audio_encoder.quantizer.layers.3.codebook.cluster_size', 'audio_encoder.quantizer.layers.3.codebook.embed', 'audio_encoder.quantizer.layers.3.codebook.embed_avg', 'decoder.model.decoder.embed_tokens.0.weight', 'decoder.model.decoder.embed_tokens.1.weight', 'decoder.model.decoder.embed_tokens.2.weight', 'decoder.model.decoder.embed_tokens.3.weight', 'decoder.model.decoder.embed_positions.weights', 'decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.fc1.weight', 'decoder.model.decoder.layers.0.fc2.weight', 'decoder.model.decoder.layers.0.final_layer_norm.weight', 'decoder.model.decoder.layers.0.final_layer_norm.bias', 'decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.fc1.weight', 'decoder.model.decoder.layers.1.fc2.weight', 'decoder.model.decoder.layers.1.final_layer_norm.weight', 'decoder.model.decoder.layers.1.final_layer_norm.bias', 'decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.fc1.weight', 'decoder.model.decoder.layers.2.fc2.weight', 'decoder.model.decoder.layers.2.final_layer_norm.weight', 'decoder.model.decoder.layers.2.final_layer_norm.bias', 'decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.fc1.weight', 'decoder.model.decoder.layers.3.fc2.weight', 'decoder.model.decoder.layers.3.final_layer_norm.weight', 'decoder.model.decoder.layers.3.final_layer_norm.bias', 'decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.fc1.weight', 'decoder.model.decoder.layers.4.fc2.weight', 'decoder.model.decoder.layers.4.final_layer_norm.weight', 'decoder.model.decoder.layers.4.final_layer_norm.bias', 'decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.fc1.weight', 'decoder.model.decoder.layers.5.fc2.weight', 'decoder.model.decoder.layers.5.final_layer_norm.weight', 'decoder.model.decoder.layers.5.final_layer_norm.bias', 'decoder.model.decoder.layers.6.self_attn.k_proj.weight', 'decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'decoder.model.decoder.layers.6.self_attn.out_proj.weight', 'decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.fc1.weight', 'decoder.model.decoder.layers.6.fc2.weight', 'decoder.model.decoder.layers.6.final_layer_norm.weight', 'decoder.model.decoder.layers.6.final_layer_norm.bias', 'decoder.model.decoder.layers.7.self_attn.k_proj.weight', 'decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'decoder.model.decoder.layers.7.self_attn.out_proj.weight', 'decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.fc1.weight', 'decoder.model.decoder.layers.7.fc2.weight', 'decoder.model.decoder.layers.7.final_layer_norm.weight', 'decoder.model.decoder.layers.7.final_layer_norm.bias', 'decoder.model.decoder.layers.8.self_attn.k_proj.weight', 'decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'decoder.model.decoder.layers.8.self_attn.out_proj.weight', 'decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.fc1.weight', 'decoder.model.decoder.layers.8.fc2.weight', 'decoder.model.decoder.layers.8.final_layer_norm.weight', 'decoder.model.decoder.layers.8.final_layer_norm.bias', 'decoder.model.decoder.layers.9.self_attn.k_proj.weight', 'decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'decoder.model.decoder.layers.9.self_attn.out_proj.weight', 'decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.fc1.weight', 'decoder.model.decoder.layers.9.fc2.weight', 'decoder.model.decoder.layers.9.final_layer_norm.weight', 'decoder.model.decoder.layers.9.final_layer_norm.bias', 'decoder.model.decoder.layers.10.self_attn.k_proj.weight', 'decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'decoder.model.decoder.layers.10.self_attn.out_proj.weight', 'decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.fc1.weight', 'decoder.model.decoder.layers.10.fc2.weight', 'decoder.model.decoder.layers.10.final_layer_norm.weight', 'decoder.model.decoder.layers.10.final_layer_norm.bias', 'decoder.model.decoder.layers.11.self_attn.k_proj.weight', 'decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'decoder.model.decoder.layers.11.self_attn.out_proj.weight', 'decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.fc1.weight', 'decoder.model.decoder.layers.11.fc2.weight', 'decoder.model.decoder.layers.11.final_layer_norm.weight', 'decoder.model.decoder.layers.11.final_layer_norm.bias', 'decoder.model.decoder.layers.12.self_attn.k_proj.weight', 'decoder.model.decoder.layers.12.self_attn.v_proj.weight', 'decoder.model.decoder.layers.12.self_attn.q_proj.weight', 'decoder.model.decoder.layers.12.self_attn.out_proj.weight', 'decoder.model.decoder.layers.12.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.12.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.12.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.12.fc1.weight', 'decoder.model.decoder.layers.12.fc2.weight', 'decoder.model.decoder.layers.12.final_layer_norm.weight', 'decoder.model.decoder.layers.12.final_layer_norm.bias', 'decoder.model.decoder.layers.13.self_attn.k_proj.weight', 'decoder.model.decoder.layers.13.self_attn.v_proj.weight', 'decoder.model.decoder.layers.13.self_attn.q_proj.weight', 'decoder.model.decoder.layers.13.self_attn.out_proj.weight', 'decoder.model.decoder.layers.13.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.13.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.13.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.13.fc1.weight', 'decoder.model.decoder.layers.13.fc2.weight', 'decoder.model.decoder.layers.13.final_layer_norm.weight', 'decoder.model.decoder.layers.13.final_layer_norm.bias', 'decoder.model.decoder.layers.14.self_attn.k_proj.weight', 'decoder.model.decoder.layers.14.self_attn.v_proj.weight', 'decoder.model.decoder.layers.14.self_attn.q_proj.weight', 'decoder.model.decoder.layers.14.self_attn.out_proj.weight', 'decoder.model.decoder.layers.14.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.14.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.14.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.14.fc1.weight', 'decoder.model.decoder.layers.14.fc2.weight', 'decoder.model.decoder.layers.14.final_layer_norm.weight', 'decoder.model.decoder.layers.14.final_layer_norm.bias', 'decoder.model.decoder.layers.15.self_attn.k_proj.weight', 'decoder.model.decoder.layers.15.self_attn.v_proj.weight', 'decoder.model.decoder.layers.15.self_attn.q_proj.weight', 'decoder.model.decoder.layers.15.self_attn.out_proj.weight', 'decoder.model.decoder.layers.15.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.15.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.15.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.15.fc1.weight', 'decoder.model.decoder.layers.15.fc2.weight', 'decoder.model.decoder.layers.15.final_layer_norm.weight', 'decoder.model.decoder.layers.15.final_layer_norm.bias', 'decoder.model.decoder.layers.16.self_attn.k_proj.weight', 'decoder.model.decoder.layers.16.self_attn.v_proj.weight', 'decoder.model.decoder.layers.16.self_attn.q_proj.weight', 'decoder.model.decoder.layers.16.self_attn.out_proj.weight', 'decoder.model.decoder.layers.16.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.16.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.fc1.weight', 'decoder.model.decoder.layers.16.fc2.weight', 'decoder.model.decoder.layers.16.final_layer_norm.weight', 'decoder.model.decoder.layers.16.final_layer_norm.bias', 'decoder.model.decoder.layers.17.self_attn.k_proj.weight', 'decoder.model.decoder.layers.17.self_attn.v_proj.weight', 'decoder.model.decoder.layers.17.self_attn.q_proj.weight', 'decoder.model.decoder.layers.17.self_attn.out_proj.weight', 'decoder.model.decoder.layers.17.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.17.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.17.fc1.weight', 'decoder.model.decoder.layers.17.fc2.weight', 'decoder.model.decoder.layers.17.final_layer_norm.weight', 'decoder.model.decoder.layers.17.final_layer_norm.bias', 'decoder.model.decoder.layers.18.self_attn.k_proj.weight', 'decoder.model.decoder.layers.18.self_attn.v_proj.weight', 'decoder.model.decoder.layers.18.self_attn.q_proj.weight', 'decoder.model.decoder.layers.18.self_attn.out_proj.weight', 'decoder.model.decoder.layers.18.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.18.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.fc1.weight', 'decoder.model.decoder.layers.18.fc2.weight', 'decoder.model.decoder.layers.18.final_layer_norm.weight', 'decoder.model.decoder.layers.18.final_layer_norm.bias', 'decoder.model.decoder.layers.19.self_attn.k_proj.weight', 'decoder.model.decoder.layers.19.self_attn.v_proj.weight', 'decoder.model.decoder.layers.19.self_attn.q_proj.weight', 'decoder.model.decoder.layers.19.self_attn.out_proj.weight', 'decoder.model.decoder.layers.19.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.19.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.fc1.weight', 'decoder.model.decoder.layers.19.fc2.weight', 'decoder.model.decoder.layers.19.final_layer_norm.weight', 'decoder.model.decoder.layers.19.final_layer_norm.bias', 'decoder.model.decoder.layers.20.self_attn.k_proj.weight', 'decoder.model.decoder.layers.20.self_attn.v_proj.weight', 'decoder.model.decoder.layers.20.self_attn.q_proj.weight', 'decoder.model.decoder.layers.20.self_attn.out_proj.weight', 'decoder.model.decoder.layers.20.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.20.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.20.fc1.weight', 'decoder.model.decoder.layers.20.fc2.weight', 'decoder.model.decoder.layers.20.final_layer_norm.weight', 'decoder.model.decoder.layers.20.final_layer_norm.bias', 'decoder.model.decoder.layers.21.self_attn.k_proj.weight', 'decoder.model.decoder.layers.21.self_attn.v_proj.weight', 'decoder.model.decoder.layers.21.self_attn.q_proj.weight', 'decoder.model.decoder.layers.21.self_attn.out_proj.weight', 'decoder.model.decoder.layers.21.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.21.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.21.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.21.fc1.weight', 'decoder.model.decoder.layers.21.fc2.weight', 'decoder.model.decoder.layers.21.final_layer_norm.weight', 'decoder.model.decoder.layers.21.final_layer_norm.bias', 'decoder.model.decoder.layers.22.self_attn.k_proj.weight', 'decoder.model.decoder.layers.22.self_attn.v_proj.weight', 'decoder.model.decoder.layers.22.self_attn.q_proj.weight', 'decoder.model.decoder.layers.22.self_attn.out_proj.weight', 'decoder.model.decoder.layers.22.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.22.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.22.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.22.fc1.weight', 'decoder.model.decoder.layers.22.fc2.weight', 'decoder.model.decoder.layers.22.final_layer_norm.weight', 'decoder.model.decoder.layers.22.final_layer_norm.bias', 'decoder.model.decoder.layers.23.self_attn.k_proj.weight', 'decoder.model.decoder.layers.23.self_attn.v_proj.weight', 'decoder.model.decoder.layers.23.self_attn.q_proj.weight', 'decoder.model.decoder.layers.23.self_attn.out_proj.weight', 'decoder.model.decoder.layers.23.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.23.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.23.fc1.weight', 'decoder.model.decoder.layers.23.fc2.weight', 'decoder.model.decoder.layers.23.final_layer_norm.weight', 'decoder.model.decoder.layers.23.final_layer_norm.bias', 'decoder.model.decoder.layer_norm.weight', 'decoder.model.decoder.layer_norm.bias', 'decoder.lm_heads.0.weight', 'decoder.lm_heads.1.weight', 'decoder.lm_heads.2.weight', 'decoder.lm_heads.3.weight', 'enc_to_dec_proj.weight', 'enc_to_dec_proj.bias'])\n"]}],"source":["from transformers import MusicgenForConditionalGeneration, MusicgenConfig\n","\n","# MusicGen 모델 초기화\n","config = MusicgenConfig.from_pretrained(\"facebook/musicgen-small\")\n","model = MusicgenForConditionalGeneration(config)\n","\n","# MusicGen 모델의 가중치 키 확인\n","expected_keys = model.state_dict().keys()\n","print(\"MusicGen 모델의 예상 가중치 키:\")\n","print(expected_keys)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4829,"status":"ok","timestamp":1733083222518,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"92Opgj7vzoSX","outputId":"d63909bd-4457-4350-a4da-888496c51c7c"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-48-7a3454154e12>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(\"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\")[\"best_state\"]\n"]},{"name":"stdout","output_type":"stream","text":["MusicGen 모델에 없는 키: {'transformer.layers.6.linear2.weight', 'transformer.layers.11.norm2.bias', 'transformer.layers.1.self_attn.in_proj_weight', 'transformer.layers.5.norm1.weight', 'transformer.layers.2.norm2.bias', 'transformer.layers.8.norm_cross.bias', 'transformer.layers.14.linear1.weight', 'transformer.layers.3.norm_cross.bias', 'transformer.layers.10.linear2.weight', 'transformer.layers.19.norm1.bias', 'transformer.layers.18.norm_cross.bias', 'transformer.layers.7.self_attn.out_proj.weight', 'transformer.layers.17.linear1.weight', 'transformer.layers.16.norm_cross.bias', 'transformer.layers.13.self_attn.out_proj.weight', 'transformer.layers.18.self_attn.in_proj_weight', 'emb.3.weight', 'transformer.layers.14.norm1.weight', 'transformer.layers.22.self_attn.in_proj_weight', 'transformer.layers.9.cross_attention.in_proj_weight', 'transformer.layers.4.cross_attention.in_proj_weight', 'transformer.layers.13.norm2.weight', 'transformer.layers.4.norm1.weight', 'transformer.layers.11.linear2.weight', 'transformer.layers.9.cross_attention.out_proj.weight', 'transformer.layers.1.linear2.weight', 'transformer.layers.19.norm_cross.bias', 'transformer.layers.0.self_attn.in_proj_weight', 'transformer.layers.7.norm1.bias', 'transformer.layers.4.norm2.weight', 'transformer.layers.3.norm1.bias', 'out_norm.bias', 'transformer.layers.3.linear1.weight', 'transformer.layers.12.cross_attention.in_proj_weight', 'transformer.layers.6.norm2.bias', 'transformer.layers.15.norm_cross.bias', 'transformer.layers.16.linear1.weight', 'transformer.layers.2.cross_attention.out_proj.weight', 'transformer.layers.14.norm1.bias', 'transformer.layers.18.cross_attention.out_proj.weight', 'transformer.layers.5.norm2.bias', 'transformer.layers.15.norm1.bias', 'transformer.layers.15.norm2.weight', 'transformer.layers.0.norm1.weight', 'transformer.layers.2.linear1.weight', 'transformer.layers.6.norm1.weight', 'transformer.layers.7.self_attn.in_proj_weight', 'transformer.layers.20.norm2.bias', 'transformer.layers.4.cross_attention.out_proj.weight', 'transformer.layers.6.norm_cross.bias', 'transformer.layers.14.self_attn.out_proj.weight', 'transformer.layers.21.norm2.weight', 'linears.0.weight', 'transformer.layers.0.cross_attention.out_proj.weight', 'transformer.layers.7.norm_cross.weight', 'transformer.layers.0.cross_attention.in_proj_weight', 'transformer.layers.18.linear1.weight', 'transformer.layers.10.linear1.weight', 'transformer.layers.18.norm1.bias', 'transformer.layers.22.cross_attention.in_proj_weight', 'transformer.layers.6.self_attn.in_proj_weight', 'transformer.layers.1.norm1.bias', 'transformer.layers.10.norm2.bias', 'transformer.layers.7.cross_attention.in_proj_weight', 'transformer.layers.8.norm2.weight', 'transformer.layers.18.self_attn.out_proj.weight', 'transformer.layers.23.linear1.weight', 'transformer.layers.3.self_attn.out_proj.weight', 'transformer.layers.23.norm2.weight', 'condition_provider.conditioners.description.output_proj.bias', 'transformer.layers.5.linear1.weight', 'transformer.layers.8.cross_attention.in_proj_weight', 'transformer.layers.12.linear1.weight', 'transformer.layers.1.cross_attention.out_proj.weight', 'transformer.layers.6.norm1.bias', 'transformer.layers.11.norm1.weight', 'transformer.layers.17.norm_cross.bias', 'transformer.layers.4.self_attn.out_proj.weight', 'transformer.layers.17.self_attn.out_proj.weight', 'transformer.layers.23.cross_attention.out_proj.weight', 'transformer.layers.20.cross_attention.out_proj.weight', 'transformer.layers.6.cross_attention.in_proj_weight', 'transformer.layers.11.self_attn.in_proj_weight', 'transformer.layers.17.norm2.weight', 'transformer.layers.15.self_attn.in_proj_weight', 'transformer.layers.8.linear2.weight', 'transformer.layers.20.norm_cross.weight', 'transformer.layers.16.norm1.weight', 'transformer.layers.10.norm1.bias', 'transformer.layers.10.cross_attention.out_proj.weight', 'transformer.layers.13.linear1.weight', 'transformer.layers.13.norm1.bias', 'transformer.layers.10.norm1.weight', 'transformer.layers.17.linear2.weight', 'transformer.layers.3.self_attn.in_proj_weight', 'transformer.layers.15.linear2.weight', 'transformer.layers.19.linear2.weight', 'transformer.layers.15.cross_attention.in_proj_weight', 'transformer.layers.15.self_attn.out_proj.weight', 'transformer.layers.21.self_attn.out_proj.weight', 'transformer.layers.16.linear2.weight', 'transformer.layers.22.norm1.bias', 'transformer.layers.23.norm1.weight', 'transformer.layers.13.cross_attention.out_proj.weight', 'transformer.layers.21.self_attn.in_proj_weight', 'transformer.layers.14.norm_cross.weight', 'transformer.layers.1.self_attn.out_proj.weight', 'transformer.layers.2.norm_cross.bias', 'transformer.layers.14.self_attn.in_proj_weight', 'transformer.layers.7.norm2.weight', 'transformer.layers.12.norm_cross.bias', 'transformer.layers.18.norm1.weight', 'transformer.layers.21.norm_cross.bias', 'transformer.layers.3.norm2.bias', 'transformer.layers.11.cross_attention.out_proj.weight', 'transformer.layers.0.norm2.bias', 'transformer.layers.10.self_attn.out_proj.weight', 'linears.2.weight', 'transformer.layers.19.self_attn.out_proj.weight', 'transformer.layers.20.norm1.weight', 'transformer.layers.22.norm_cross.weight', 'transformer.layers.9.norm1.weight', 'transformer.layers.4.norm1.bias', 'transformer.layers.2.linear2.weight', 'transformer.layers.23.self_attn.out_proj.weight', 'transformer.layers.22.norm1.weight', 'transformer.layers.12.norm2.bias', 'transformer.layers.1.norm_cross.bias', 'transformer.layers.12.self_attn.in_proj_weight', 'transformer.layers.7.linear1.weight', 'linears.1.weight', 'transformer.layers.3.cross_attention.out_proj.weight', 'transformer.layers.5.norm_cross.weight', 'transformer.layers.6.norm2.weight', 'transformer.layers.22.self_attn.out_proj.weight', 'transformer.layers.1.norm2.weight', 'transformer.layers.3.norm_cross.weight', 'transformer.layers.2.self_attn.out_proj.weight', 'transformer.layers.20.self_attn.out_proj.weight', 'transformer.layers.1.norm_cross.weight', 'transformer.layers.3.norm1.weight', 'transformer.layers.8.norm2.bias', 'transformer.layers.19.self_attn.in_proj_weight', 'transformer.layers.4.self_attn.in_proj_weight', 'transformer.layers.14.cross_attention.in_proj_weight', 'transformer.layers.5.norm2.weight', 'transformer.layers.3.norm2.weight', 'transformer.layers.17.cross_attention.in_proj_weight', 'transformer.layers.19.norm1.weight', 'transformer.layers.21.norm1.weight', 'transformer.layers.22.linear1.weight', 'transformer.layers.5.cross_attention.in_proj_weight', 'transformer.layers.17.self_attn.in_proj_weight', 'transformer.layers.2.norm1.weight', 'transformer.layers.12.linear2.weight', 'transformer.layers.12.norm1.weight', 'transformer.layers.18.linear2.weight', 'transformer.layers.23.cross_attention.in_proj_weight', 'transformer.layers.19.norm_cross.weight', 'transformer.layers.5.self_attn.out_proj.weight', 'transformer.layers.10.norm_cross.bias', 'transformer.layers.2.norm1.bias', 'transformer.layers.7.norm1.weight', 'transformer.layers.9.norm_cross.bias', 'transformer.layers.15.cross_attention.out_proj.weight', 'transformer.layers.0.norm1.bias', 'transformer.layers.11.norm1.bias', 'transformer.layers.9.self_attn.out_proj.weight', 'transformer.layers.12.cross_attention.out_proj.weight', 'transformer.layers.4.norm2.bias', 'transformer.layers.13.norm2.bias', 'transformer.layers.14.linear2.weight', 'transformer.layers.1.cross_attention.in_proj_weight', 'transformer.layers.17.norm2.bias', 'transformer.layers.8.norm1.bias', 'transformer.layers.21.norm2.bias', 'transformer.layers.8.self_attn.in_proj_weight', 'transformer.layers.11.norm_cross.bias', 'transformer.layers.13.self_attn.in_proj_weight', 'transformer.layers.16.norm_cross.weight', 'transformer.layers.23.norm_cross.bias', 'transformer.layers.8.self_attn.out_proj.weight', 'transformer.layers.15.norm1.weight', 'transformer.layers.10.self_attn.in_proj_weight', 'transformer.layers.5.norm_cross.bias', 'transformer.layers.16.cross_attention.in_proj_weight', 'transformer.layers.5.self_attn.in_proj_weight', 'transformer.layers.9.norm_cross.weight', 'transformer.layers.11.linear1.weight', 'transformer.layers.13.linear2.weight', 'transformer.layers.15.linear1.weight', 'transformer.layers.6.norm_cross.weight', 'transformer.layers.16.norm2.bias', 'transformer.layers.4.linear1.weight', 'transformer.layers.19.cross_attention.out_proj.weight', 'transformer.layers.21.cross_attention.in_proj_weight', 'transformer.layers.5.linear2.weight', 'linears.3.weight', 'transformer.layers.21.cross_attention.out_proj.weight', 'transformer.layers.9.norm2.weight', 'transformer.layers.22.norm2.bias', 'transformer.layers.15.norm_cross.weight', 'transformer.layers.23.norm2.bias', 'transformer.layers.0.linear1.weight', 'out_norm.weight', 'transformer.layers.7.linear2.weight', 'transformer.layers.8.linear1.weight', 'transformer.layers.16.cross_attention.out_proj.weight', 'transformer.layers.6.linear1.weight', 'transformer.layers.2.cross_attention.in_proj_weight', 'transformer.layers.11.norm_cross.weight', 'transformer.layers.19.cross_attention.in_proj_weight', 'transformer.layers.0.norm_cross.weight', 'transformer.layers.6.cross_attention.out_proj.weight', 'emb.0.weight', 'transformer.layers.9.norm2.bias', 'transformer.layers.19.norm2.weight', 'emb.2.weight', 'transformer.layers.6.self_attn.out_proj.weight', 'transformer.layers.12.norm_cross.weight', 'transformer.layers.4.norm_cross.weight', 'transformer.layers.1.linear1.weight', 'transformer.layers.7.norm2.bias', 'transformer.layers.18.norm2.weight', 'transformer.layers.20.self_attn.in_proj_weight', 'transformer.layers.11.cross_attention.in_proj_weight', 'transformer.layers.20.linear1.weight', 'transformer.layers.10.norm_cross.weight', 'transformer.layers.20.cross_attention.in_proj_weight', 'transformer.layers.13.cross_attention.in_proj_weight', 'transformer.layers.22.norm2.weight', 'transformer.layers.13.norm_cross.weight', 'transformer.layers.14.norm2.weight', 'transformer.layers.0.norm_cross.bias', 'transformer.layers.2.self_attn.in_proj_weight', 'transformer.layers.5.norm1.bias', 'transformer.layers.10.norm2.weight', 'transformer.layers.10.cross_attention.in_proj_weight', 'transformer.layers.14.norm_cross.bias', 'transformer.layers.11.norm2.weight', 'transformer.layers.17.cross_attention.out_proj.weight', 'transformer.layers.12.norm1.bias', 'transformer.layers.1.norm1.weight', 'transformer.layers.3.linear2.weight', 'transformer.layers.17.norm1.bias', 'transformer.layers.20.norm2.weight', 'transformer.layers.9.linear2.weight', 'transformer.layers.20.linear2.weight', 'transformer.layers.4.norm_cross.bias', 'transformer.layers.19.linear1.weight', 'transformer.layers.11.self_attn.out_proj.weight', 'transformer.layers.15.norm2.bias', 'transformer.layers.0.self_attn.out_proj.weight', 'transformer.layers.17.norm1.weight', 'transformer.layers.22.linear2.weight', 'transformer.layers.14.norm2.bias', 'transformer.layers.14.cross_attention.out_proj.weight', 'transformer.layers.21.norm_cross.weight', 'transformer.layers.18.norm_cross.weight', 'transformer.layers.23.self_attn.in_proj_weight', 'transformer.layers.17.norm_cross.weight', 'transformer.layers.23.linear2.weight', 'transformer.layers.2.norm_cross.weight', 'transformer.layers.8.norm_cross.weight', 'transformer.layers.23.norm_cross.weight', 'transformer.layers.9.linear1.weight', 'transformer.layers.13.norm1.weight', 'transformer.layers.20.norm1.bias', 'transformer.layers.21.linear2.weight', 'transformer.layers.21.norm1.bias', 'transformer.layers.2.norm2.weight', 'transformer.layers.16.norm2.weight', 'transformer.layers.21.linear1.weight', 'transformer.layers.22.cross_attention.out_proj.weight', 'transformer.layers.7.cross_attention.out_proj.weight', 'transformer.layers.7.norm_cross.bias', 'transformer.layers.16.self_attn.in_proj_weight', 'transformer.layers.19.norm2.bias', 'transformer.layers.12.norm2.weight', 'condition_provider.conditioners.description.output_proj.weight', 'transformer.layers.4.linear2.weight', 'transformer.layers.5.cross_attention.out_proj.weight', 'emb.1.weight', 'transformer.layers.0.norm2.weight', 'transformer.layers.22.norm_cross.bias', 'transformer.layers.16.norm1.bias', 'transformer.layers.9.self_attn.in_proj_weight', 'transformer.layers.8.norm1.weight', 'transformer.layers.8.cross_attention.out_proj.weight', 'transformer.layers.20.norm_cross.bias', 'transformer.layers.3.cross_attention.in_proj_weight', 'transformer.layers.18.norm2.bias', 'transformer.layers.12.self_attn.out_proj.weight', 'transformer.layers.23.norm1.bias', 'transformer.layers.18.cross_attention.in_proj_weight', 'transformer.layers.1.norm2.bias', 'transformer.layers.13.norm_cross.bias', 'transformer.layers.16.self_attn.out_proj.weight', 'transformer.layers.0.linear2.weight', 'transformer.layers.9.norm1.bias'}\n","MusicGen 모델에서 필요한데 없는 키: {'decoder.model.decoder.layers.20.final_layer_norm.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.7.self_attn.out_proj.weight', 'decoder.model.decoder.layers.18.self_attn.k_proj.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.17.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.k_proj.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.model.decoder.layers.16.fc2.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.7.fc1.weight', 'decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.10.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.4.block.3.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.6.fc2.weight', 'audio_encoder.encoder.layers.3.conv.bias', 'audio_encoder.quantizer.layers.3.codebook.cluster_size', 'decoder.model.decoder.layers.15.self_attn.v_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.13.self_attn.v_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.o.weight', 'audio_encoder.decoder.layers.9.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.13.block.3.conv.parametrizations.weight.original1', 'decoder.model.decoder.embed_tokens.1.weight', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.23.fc2.weight', 'decoder.model.decoder.layers.20.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'decoder.model.decoder.layers.14.self_attn.out_proj.weight', 'decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'text_encoder.encoder.block.5.layer.1.DenseReluDense.wi.weight', 'audio_encoder.quantizer.layers.1.codebook.inited', 'decoder.model.decoder.layers.18.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.9.fc1.weight', 'decoder.model.decoder.layers.17.final_layer_norm.bias', 'text_encoder.encoder.block.10.layer.1.layer_norm.weight', 'decoder.model.decoder.layers.10.self_attn.out_proj.weight', 'audio_encoder.quantizer.layers.3.codebook.inited', 'decoder.model.decoder.layers.13.self_attn.q_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.self_attn.out_proj.weight', 'text_encoder.encoder.block.7.layer.1.layer_norm.weight', 'text_encoder.encoder.embed_tokens.weight', 'text_encoder.encoder.block.5.layer.1.layer_norm.weight', 'audio_encoder.encoder.layers.13.lstm.bias_ih_l0', 'audio_encoder.encoder.layers.13.lstm.weight_ih_l1', 'audio_encoder.encoder.layers.13.lstm.bias_hh_l0', 'decoder.model.decoder.layers.12.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.13.final_layer_norm.weight', 'audio_encoder.decoder.layers.0.conv.bias', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.weight', 'audio_encoder.encoder.layers.13.lstm.bias_hh_l1', 'decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.19.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.fc2.weight', 'decoder.model.decoder.embed_tokens.2.weight', 'audio_encoder.encoder.layers.4.block.1.conv.bias', 'decoder.model.decoder.layers.20.self_attn_layer_norm.weight', 'audio_encoder.quantizer.layers.1.codebook.cluster_size', 'audio_encoder.quantizer.layers.2.codebook.embed_avg', 'text_encoder.encoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.5.fc2.weight', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.14.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.10.block.1.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.12.self_attn.k_proj.weight', 'text_encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'audio_encoder.decoder.layers.13.block.1.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.3.fc1.weight', 'decoder.model.decoder.layers.17.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.1.block.1.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.self_attn_layer_norm.bias', 'enc_to_dec_proj.weight', 'text_encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'audio_encoder.decoder.layers.13.block.1.conv.bias', 'decoder.model.decoder.layers.15.fc2.weight', 'decoder.lm_heads.3.weight', 'decoder.model.decoder.layers.6.final_layer_norm.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.q.weight', 'audio_encoder.encoder.layers.7.block.1.conv.bias', 'audio_encoder.encoder.layers.1.block.3.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.22.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.10.block.3.conv.bias', 'decoder.model.decoder.layers.16.self_attn.q_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'audio_encoder.encoder.layers.10.block.1.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.23.final_layer_norm.weight', 'text_encoder.encoder.block.2.layer.1.layer_norm.weight', 'audio_encoder.decoder.layers.0.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.19.final_layer_norm.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.bias', 'text_encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'audio_encoder.encoder.layers.4.block.3.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.2.fc1.weight', 'audio_encoder.quantizer.layers.2.codebook.cluster_size', 'decoder.model.decoder.layers.15.self_attn.out_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'audio_encoder.encoder.layers.6.conv.bias', 'text_encoder.encoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.final_layer_norm.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.self_attn.out_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.10.fc2.weight', 'decoder.model.decoder.layers.4.final_layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.8.final_layer_norm.bias', 'decoder.model.decoder.layers.15.encoder_attn.k_proj.weight', 'text_encoder.encoder.block.6.layer.0.layer_norm.weight', 'text_encoder.encoder.block.11.layer.1.layer_norm.weight', 'audio_encoder.encoder.layers.1.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.10.block.3.conv.parametrizations.weight.original1', 'text_encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.model.decoder.layers.16.self_attn.v_proj.weight', 'decoder.model.decoder.layers.5.fc1.weight', 'decoder.model.decoder.layers.18.fc1.weight', 'decoder.model.decoder.layers.15.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.22.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.10.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.12.final_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'text_encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'audio_encoder.decoder.layers.13.block.3.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.fc1.weight', 'audio_encoder.decoder.layers.6.conv.parametrizations.weight.original0', 'text_encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.1.lstm.weight_ih_l0', 'decoder.model.decoder.layers.21.fc1.weight', 'audio_encoder.quantizer.layers.2.codebook.inited', 'decoder.model.decoder.layers.13.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.7.block.1.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.16.fc1.weight', 'decoder.model.decoder.layers.12.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.final_layer_norm.bias', 'decoder.model.decoder.layers.21.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.22.final_layer_norm.weight', 'decoder.model.decoder.layers.14.fc2.weight', 'audio_encoder.encoder.layers.0.conv.bias', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'text_encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.1.final_layer_norm.bias', 'decoder.model.decoder.layers.3.final_layer_norm.weight', 'decoder.model.decoder.layers.1.fc2.weight', 'decoder.model.decoder.layers.19.fc1.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'audio_encoder.quantizer.layers.0.codebook.inited', 'decoder.model.decoder.layers.18.self_attn.out_proj.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.22.final_layer_norm.bias', 'decoder.model.decoder.layers.2.fc2.weight', 'decoder.model.decoder.layers.8.fc1.weight', 'audio_encoder.encoder.layers.6.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.self_attn.out_proj.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.3.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.self_attn.q_proj.weight', 'text_encoder.encoder.block.0.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.20.self_attn.q_proj.weight', 'decoder.model.decoder.layers.5.final_layer_norm.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.0.conv.parametrizations.weight.original0', 'text_encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'audio_encoder.decoder.layers.1.lstm.bias_ih_l1', 'decoder.model.decoder.embed_tokens.3.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.o.weight', 'audio_encoder.decoder.layers.15.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.22.fc2.weight', 'decoder.model.decoder.layers.17.final_layer_norm.weight', 'decoder.model.decoder.layers.14.encoder_attn.out_proj.weight', 'text_encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.13.fc1.weight', 'decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.self_attn_layer_norm.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.16.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.19.final_layer_norm.bias', 'decoder.lm_heads.2.weight', 'decoder.model.decoder.layers.9.fc2.weight', 'audio_encoder.quantizer.layers.3.codebook.embed', 'audio_encoder.decoder.layers.13.block.1.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.encoder_attn.k_proj.weight', 'enc_to_dec_proj.bias', 'audio_encoder.encoder.layers.12.conv.bias', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.10.block.1.conv.parametrizations.weight.original0', 'text_encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.15.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.20.fc1.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.fc2.weight', 'audio_encoder.encoder.layers.15.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.13.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.4.block.1.conv.bias', 'audio_encoder.encoder.layers.7.block.3.conv.bias', 'text_encoder.encoder.block.4.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.final_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'text_encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.6.layer.1.DenseReluDense.wi.weight', 'decoder.model.decoder.layers.23.fc1.weight', 'decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'text_encoder.encoder.block.9.layer.1.layer_norm.weight', 'decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.7.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.9.conv.bias', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.16.self_attn.k_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.bias', 'text_encoder.encoder.block.1.layer.0.layer_norm.weight', 'audio_encoder.encoder.layers.13.lstm.weight_hh_l1', 'audio_encoder.decoder.layers.7.block.1.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.22.self_attn.q_proj.weight', 'decoder.model.decoder.layers.23.self_attn.k_proj.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.bias', 'text_encoder.encoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'text_encoder.encoder.block.2.layer.1.DenseReluDense.wi.weight', 'decoder.model.decoder.layers.15.self_attn.k_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.10.block.1.conv.bias', 'decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.17.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.1.lstm.weight_hh_l1', 'audio_encoder.decoder.layers.7.block.3.conv.parametrizations.weight.original1', 'text_encoder.encoder.block.9.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.7.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.6.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.1.lstm.weight_ih_l1', 'decoder.model.decoder.layers.17.fc2.weight', 'decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'audio_encoder.quantizer.layers.1.codebook.embed', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.1.lstm.bias_hh_l1', 'decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'audio_encoder.decoder.layers.7.block.3.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.0.fc1.weight', 'decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.21.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.18.fc2.weight', 'decoder.model.decoder.layers.23.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.1.layer.1.layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.15.self_attn.q_proj.weight', 'decoder.model.decoder.layers.21.self_attn.v_proj.weight', 'text_encoder.encoder.block.1.layer.1.DenseReluDense.wi.weight', 'decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.13.lstm.weight_hh_l0', 'audio_encoder.decoder.layers.1.lstm.bias_hh_l0', 'decoder.model.decoder.layers.21.self_attn.out_proj.weight', 'text_encoder.encoder.block.0.layer.1.DenseReluDense.wi.weight', 'decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.18.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.4.block.1.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.13.lstm.weight_ih_l0', 'decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.1.lstm.bias_ih_l0', 'decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'decoder.model.decoder.layers.18.final_layer_norm.weight', 'decoder.model.decoder.layers.15.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.12.self_attn.out_proj.weight', 'text_encoder.encoder.block.6.layer.1.layer_norm.weight', 'decoder.model.decoder.layers.23.self_attn_layer_norm.bias', 'decoder.model.decoder.layer_norm.bias', 'audio_encoder.quantizer.layers.0.codebook.embed', 'decoder.model.decoder.layers.8.fc2.weight', 'decoder.model.decoder.layers.20.self_attn.out_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.8.self_attn.k_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.14.final_layer_norm.weight', 'decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'decoder.model.decoder.layers.6.self_attn.k_proj.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.model.decoder.layers.18.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.bias', 'audio_encoder.quantizer.layers.3.codebook.embed_avg', 'decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'decoder.model.decoder.layers.17.fc1.weight', 'decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.3.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.14.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.self_attn.k_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.15.final_layer_norm.weight', 'text_encoder.encoder.block.3.layer.1.layer_norm.weight', 'audio_encoder.encoder.layers.10.block.1.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.15.self_attn_layer_norm.bias', 'text_encoder.shared.weight', 'decoder.model.decoder.layers.8.final_layer_norm.weight', 'decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'audio_encoder.encoder.layers.6.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'text_encoder.encoder.block.8.layer.1.DenseReluDense.wi.weight', 'decoder.model.decoder.layers.16.self_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'text_encoder.encoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.21.encoder_attn.q_proj.weight', 'audio_encoder.encoder.layers.0.conv.parametrizations.weight.original1', 'audio_encoder.quantizer.layers.0.codebook.cluster_size', 'decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.13.self_attn.out_proj.weight', 'decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.k_proj.weight', 'audio_encoder.encoder.layers.9.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.4.block.3.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.final_layer_norm.bias', 'decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'decoder.model.decoder.layers.9.final_layer_norm.weight', 'decoder.model.decoder.layers.11.fc2.weight', 'decoder.model.decoder.layers.5.final_layer_norm.bias', 'audio_encoder.encoder.layers.10.block.1.conv.bias', 'audio_encoder.decoder.layers.4.block.1.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.14.self_attn.k_proj.weight', 'decoder.model.decoder.layers.10.fc1.weight', 'decoder.model.decoder.layers.0.final_layer_norm.weight', 'decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.19.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.14.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.15.conv.bias', 'decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.14.final_layer_norm.bias', 'decoder.model.decoder.layers.19.fc2.weight', 'decoder.model.decoder.layers.22.self_attn.out_proj.weight', 'decoder.model.decoder.layers.10.final_layer_norm.bias', 'audio_encoder.encoder.layers.15.conv.bias', 'text_encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.model.decoder.layers.22.self_attn.v_proj.weight', 'decoder.model.decoder.layers.18.final_layer_norm.bias', 'decoder.model.decoder.layers.16.self_attn_layer_norm.bias', 'text_encoder.encoder.block.5.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.7.final_layer_norm.weight', 'decoder.model.decoder.layers.12.fc2.weight', 'audio_encoder.decoder.layers.3.conv.bias', 'text_encoder.encoder.block.10.layer.0.layer_norm.weight', 'audio_encoder.decoder.layers.4.block.1.conv.parametrizations.weight.original1', 'text_encoder.encoder.block.3.layer.0.SelfAttention.o.weight', 'audio_encoder.encoder.layers.1.block.3.conv.bias', 'decoder.lm_heads.0.weight', 'decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'text_encoder.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'decoder.model.decoder.layers.17.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.11.fc1.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.15.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.17.self_attn.k_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.model.decoder.layers.21.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'decoder.model.decoder.layers.23.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.7.block.3.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.final_layer_norm.bias', 'text_encoder.encoder.block.5.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.7.self_attn.k_proj.weight', 'decoder.model.decoder.layers.14.self_attn.v_proj.weight', 'decoder.model.decoder.layers.17.self_attn.q_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'decoder.model.decoder.layers.6.final_layer_norm.bias', 'decoder.model.decoder.layers.18.self_attn.q_proj.weight', 'decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'decoder.model.decoder.layers.12.fc1.weight', 'decoder.model.decoder.layers.23.final_layer_norm.bias', 'decoder.model.decoder.layers.20.fc2.weight', 'text_encoder.encoder.final_layer_norm.weight', 'decoder.model.decoder.layers.20.encoder_attn.q_proj.weight', 'text_encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'audio_encoder.encoder.layers.12.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.13.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.model.decoder.layer_norm.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.19.self_attn.q_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.10.block.3.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.14.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.8.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.12.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.final_layer_norm.bias', 'audio_encoder.encoder.layers.4.block.3.conv.bias', 'decoder.model.decoder.layers.17.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.23.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.12.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.17.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.22.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.10.block.3.conv.bias', 'decoder.model.decoder.layers.11.self_attn.out_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.19.self_attn.k_proj.weight', 'decoder.model.decoder.layers.21.final_layer_norm.weight', 'decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.fc2.weight', 'decoder.model.decoder.layers.13.self_attn_layer_norm.weight', 'decoder.lm_heads.1.weight', 'decoder.model.decoder.layers.16.final_layer_norm.weight', 'audio_encoder.decoder.layers.7.block.3.conv.bias', 'decoder.model.decoder.layers.12.self_attn.v_proj.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'audio_encoder.encoder.layers.9.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'text_encoder.encoder.block.0.layer.1.layer_norm.weight', 'decoder.model.decoder.layers.1.fc1.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.12.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'audio_encoder.encoder.layers.0.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.19.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.fc1.weight', 'text_encoder.encoder.block.8.layer.1.layer_norm.weight', 'decoder.model.decoder.layers.19.self_attn.v_proj.weight', 'decoder.model.decoder.layers.12.final_layer_norm.bias', 'audio_encoder.decoder.layers.13.block.3.conv.bias', 'audio_encoder.decoder.layers.12.conv.bias', 'decoder.model.decoder.layers.13.final_layer_norm.bias', 'decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.9.conv.parametrizations.weight.original1', 'text_encoder.encoder.block.4.layer.1.layer_norm.weight', 'decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'text_encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'decoder.model.decoder.layers.11.final_layer_norm.weight', 'decoder.model.decoder.layers.9.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.3.conv.parametrizations.weight.original0', 'text_encoder.encoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.4.layer.1.DenseReluDense.wi.weight', 'audio_encoder.decoder.layers.1.lstm.weight_hh_l0', 'decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.21.final_layer_norm.bias', 'decoder.model.decoder.layers.13.fc2.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.final_layer_norm.bias', 'decoder.model.decoder.embed_positions.weights', 'text_encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.9.self_attn.out_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'text_encoder.encoder.block.9.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'text_encoder.encoder.block.11.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.21.self_attn.k_proj.weight', 'decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'decoder.model.decoder.layers.14.fc1.weight', 'audio_encoder.decoder.layers.9.conv.bias', 'decoder.model.decoder.layers.12.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.model.decoder.layers.13.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.4.block.3.conv.bias', 'decoder.model.decoder.layers.22.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.1.block.1.conv.bias', 'decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'decoder.model.decoder.layers.22.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.12.encoder_attn.k_proj.weight', 'audio_encoder.encoder.layers.4.block.1.conv.parametrizations.weight.original1', 'text_encoder.encoder.block.9.layer.0.SelfAttention.k.weight', 'audio_encoder.quantizer.layers.2.codebook.embed', 'text_encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'audio_encoder.encoder.layers.1.block.1.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.4.fc2.weight', 'text_encoder.encoder.block.2.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.12.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.7.block.3.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.model.decoder.layers.12.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.18.self_attn.v_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.self_attn.v_proj.weight', 'decoder.model.decoder.layers.21.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.3.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.2.final_layer_norm.bias', 'decoder.model.decoder.layers.14.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.6.conv.bias', 'decoder.model.decoder.layers.20.self_attn.k_proj.weight', 'decoder.model.decoder.layers.15.final_layer_norm.bias', 'decoder.model.decoder.layers.2.final_layer_norm.weight', 'audio_encoder.quantizer.layers.0.codebook.embed_avg', 'audio_encoder.encoder.layers.13.lstm.bias_ih_l1', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.bias', 'decoder.model.decoder.embed_tokens.0.weight', 'decoder.model.decoder.layers.10.self_attn.k_proj.weight', 'text_encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'audio_encoder.encoder.layers.10.block.3.conv.parametrizations.weight.original0', 'decoder.model.decoder.layers.20.final_layer_norm.bias', 'decoder.model.decoder.layers.7.fc2.weight', 'decoder.model.decoder.layers.1.final_layer_norm.weight', 'audio_encoder.encoder.layers.4.block.3.conv.parametrizations.weight.original1', 'decoder.model.decoder.layers.23.self_attn_layer_norm.weight', 'text_encoder.encoder.block.11.layer.1.DenseReluDense.wi.weight', 'decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'text_encoder.encoder.block.3.layer.1.DenseReluDense.wi.weight', 'audio_encoder.encoder.layers.7.block.1.conv.parametrizations.weight.original1', 'audio_encoder.quantizer.layers.1.codebook.embed_avg', 'decoder.model.decoder.layers.20.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.7.block.1.conv.bias', 'decoder.model.decoder.layers.15.fc1.weight', 'decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'decoder.model.decoder.layers.9.final_layer_norm.bias', 'decoder.model.decoder.layers.14.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'text_encoder.encoder.block.8.layer.0.layer_norm.weight', 'decoder.model.decoder.layers.22.fc1.weight', 'text_encoder.encoder.block.7.layer.1.DenseReluDense.wi.weight'}\n"]}],"source":["import torch\n","\n","# 기존 state_dict 로드\n","state_dict = torch.load(\"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\")[\"best_state\"]\n","\n","# 예상 키와 비교\n","missing_keys = set(expected_keys) - set(state_dict.keys())\n","unexpected_keys = set(state_dict.keys()) - set(expected_keys)\n","\n","print(\"MusicGen 모델에 없는 키:\", unexpected_keys)\n","print(\"MusicGen 모델에서 필요한데 없는 키:\", missing_keys)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4B4aF8BJzz0E"},"outputs":[],"source":["def convert_state_dict(state_dict):\n","    converted_state_dict = {}\n","    for key, value in state_dict.items():\n","        if key.startswith(\"transformer.layers.\"):\n","            # 예: transformer.layers.0.self_attn.in_proj_weight -> decoder.model.decoder.layers.0.self_attn.k_proj.weight\n","            new_key = key.replace(\"transformer.layers.\", \"decoder.model.decoder.layers.\")\n","            new_key = new_key.replace(\"self_attn.in_proj_weight\", \"self_attn.k_proj.weight\")\n","            new_key = new_key.replace(\"self_attn.out_proj.weight\", \"self_attn.out_proj.weight\")\n","            # 추가 변환 규칙\n","            converted_state_dict[new_key] = value\n","        elif key.startswith(\"emb.\"):\n","            # 예: emb.0.weight -> decoder.model.decoder.embed_tokens.0.weight\n","            new_key = key.replace(\"emb.\", \"decoder.model.decoder.embed_tokens.\")\n","            converted_state_dict[new_key] = value\n","        elif key.startswith(\"out_norm\"):\n","            # 예: out_norm.weight -> decoder.model.decoder.layer_norm.weight\n","            new_key = key.replace(\"out_norm\", \"decoder.model.decoder.layer_norm\")\n","            converted_state_dict[new_key] = value\n","        elif key.startswith(\"condition_provider.conditioners.description.output_proj\"):\n","            # 필요하지 않을 수 있으므로 무시\n","            continue\n","        else:\n","            # 예상치 못한 키는 로그로 출력\n","            print(f\"Unexpected key: {key}\")\n","\n","    return converted_state_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":23305,"status":"error","timestamp":1733083397848,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"qvH6d7XD0KJW","outputId":"35689729-93ea-4e7e-a9b5-1733cef0fe98"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-50-561216c46b1a>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  original_state_dict = torch.load(state_dict_path)[\"best_state\"]\n"]},{"name":"stdout","output_type":"stream","text":["Unexpected key: linears.0.weight\n","Unexpected key: linears.1.weight\n","Unexpected key: linears.2.weight\n","Unexpected key: linears.3.weight\n"]},{"name":"stderr","output_type":"stream","text":["Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n","  \"_name_or_path\": \"t5-base\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.46.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","Config of the audio_encoder: <class 'transformers.models.encodec.modeling_encodec.EncodecModel'> is overwritten by shared audio_encoder config: EncodecConfig {\n","  \"_name_or_path\": \"facebook/encodec_32khz\",\n","  \"architectures\": [\n","    \"EncodecModel\"\n","  ],\n","  \"audio_channels\": 1,\n","  \"chunk_length_s\": null,\n","  \"codebook_dim\": 128,\n","  \"codebook_size\": 2048,\n","  \"compress\": 2,\n","  \"dilation_growth_rate\": 2,\n","  \"hidden_size\": 128,\n","  \"kernel_size\": 7,\n","  \"last_kernel_size\": 7,\n","  \"model_type\": \"encodec\",\n","  \"norm_type\": \"weight_norm\",\n","  \"normalize\": false,\n","  \"num_filters\": 64,\n","  \"num_lstm_layers\": 2,\n","  \"num_residual_layers\": 1,\n","  \"overlap\": null,\n","  \"pad_mode\": \"reflect\",\n","  \"residual_kernel_size\": 3,\n","  \"sampling_rate\": 32000,\n","  \"target_bandwidths\": [\n","    2.2\n","  ],\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.46.2\",\n","  \"trim_right_ratio\": 1.0,\n","  \"upsampling_ratios\": [\n","    8,\n","    5,\n","    4,\n","    4\n","  ],\n","  \"use_causal_conv\": false,\n","  \"use_conv_shortcut\": false\n","}\n","\n","Config of the decoder: <class 'transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM'> is overwritten by shared decoder config: MusicgenDecoderConfig {\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"attention_dropout\": 0.0,\n","  \"audio_channels\": 1,\n","  \"bos_token_id\": 2048,\n","  \"classifier_dropout\": 0.0,\n","  \"dropout\": 0.1,\n","  \"ffn_dim\": 4096,\n","  \"hidden_size\": 1024,\n","  \"initializer_factor\": 0.02,\n","  \"layerdrop\": 0.0,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"musicgen_decoder\",\n","  \"num_attention_heads\": 16,\n","  \"num_codebooks\": 4,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 2048,\n","  \"scale_embedding\": false,\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"4.46.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 2048\n","}\n","\n"]},{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for MusicgenForConditionalGeneration:\n\tsize mismatch for decoder.model.decoder.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.20.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.21.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.22.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.23.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-561216c46b1a>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 변환된 state_dict 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MusicgenForConditionalGeneration:\n\tsize mismatch for decoder.model.decoder.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.20.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.21.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.22.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.model.decoder.layers.23.self_attn.k_proj.weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024])."]}],"source":["from transformers import MusicgenForConditionalGeneration, MusicgenConfig\n","\n","# 기존 state_dict 로드\n","state_dict_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune/state_dict.bin\"\n","original_state_dict = torch.load(state_dict_path)[\"best_state\"]\n","\n","# 키 변환\n","converted_state_dict = convert_state_dict(original_state_dict)\n","\n","# MusicGen 모델 초기화\n","config = MusicgenConfig.from_pretrained(\"facebook/musicgen-small\")\n","model = MusicgenForConditionalGeneration(config)\n","\n","# 변환된 state_dict 로드\n","model.load_state_dict(converted_state_dict, strict=False)\n","\n","# 모델 저장\n","output_dir = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune\"\n","model.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5910,"status":"ok","timestamp":1733084617112,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"z03Iug4a0aGg","outputId":"31d4bbd0-8e77-4205-f49d-f97ab3347dc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.0.linear1.weight\n","Skipping key: decoder.model.decoder.layers.0.linear2.weight\n","Skipping key: decoder.model.decoder.layers.0.norm1.weight\n","Skipping key: decoder.model.decoder.layers.0.norm1.bias\n","Skipping key: decoder.model.decoder.layers.0.norm2.weight\n","Skipping key: decoder.model.decoder.layers.0.norm2.bias\n","Skipping key: decoder.model.decoder.layers.0.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.0.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.0.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.0.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.1.linear1.weight\n","Skipping key: decoder.model.decoder.layers.1.linear2.weight\n","Skipping key: decoder.model.decoder.layers.1.norm1.weight\n","Skipping key: decoder.model.decoder.layers.1.norm1.bias\n","Skipping key: decoder.model.decoder.layers.1.norm2.weight\n","Skipping key: decoder.model.decoder.layers.1.norm2.bias\n","Skipping key: decoder.model.decoder.layers.1.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.1.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.1.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.1.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.2.linear1.weight\n","Skipping key: decoder.model.decoder.layers.2.linear2.weight\n","Skipping key: decoder.model.decoder.layers.2.norm1.weight\n","Skipping key: decoder.model.decoder.layers.2.norm1.bias\n","Skipping key: decoder.model.decoder.layers.2.norm2.weight\n","Skipping key: decoder.model.decoder.layers.2.norm2.bias\n","Skipping key: decoder.model.decoder.layers.2.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.2.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.2.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.2.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.3.linear1.weight\n","Skipping key: decoder.model.decoder.layers.3.linear2.weight\n","Skipping key: decoder.model.decoder.layers.3.norm1.weight\n","Skipping key: decoder.model.decoder.layers.3.norm1.bias\n","Skipping key: decoder.model.decoder.layers.3.norm2.weight\n","Skipping key: decoder.model.decoder.layers.3.norm2.bias\n","Skipping key: decoder.model.decoder.layers.3.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.3.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.3.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.3.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.4.linear1.weight\n","Skipping key: decoder.model.decoder.layers.4.linear2.weight\n","Skipping key: decoder.model.decoder.layers.4.norm1.weight\n","Skipping key: decoder.model.decoder.layers.4.norm1.bias\n","Skipping key: decoder.model.decoder.layers.4.norm2.weight\n","Skipping key: decoder.model.decoder.layers.4.norm2.bias\n","Skipping key: decoder.model.decoder.layers.4.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.4.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.4.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.4.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.5.linear1.weight\n","Skipping key: decoder.model.decoder.layers.5.linear2.weight\n","Skipping key: decoder.model.decoder.layers.5.norm1.weight\n","Skipping key: decoder.model.decoder.layers.5.norm1.bias\n","Skipping key: decoder.model.decoder.layers.5.norm2.weight\n","Skipping key: decoder.model.decoder.layers.5.norm2.bias\n","Skipping key: decoder.model.decoder.layers.5.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.5.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.5.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.5.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.6.linear1.weight\n","Skipping key: decoder.model.decoder.layers.6.linear2.weight\n","Skipping key: decoder.model.decoder.layers.6.norm1.weight\n","Skipping key: decoder.model.decoder.layers.6.norm1.bias\n","Skipping key: decoder.model.decoder.layers.6.norm2.weight\n","Skipping key: decoder.model.decoder.layers.6.norm2.bias\n","Skipping key: decoder.model.decoder.layers.6.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.6.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.6.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.6.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.7.linear1.weight\n","Skipping key: decoder.model.decoder.layers.7.linear2.weight\n","Skipping key: decoder.model.decoder.layers.7.norm1.weight\n","Skipping key: decoder.model.decoder.layers.7.norm1.bias\n","Skipping key: decoder.model.decoder.layers.7.norm2.weight\n","Skipping key: decoder.model.decoder.layers.7.norm2.bias\n","Skipping key: decoder.model.decoder.layers.7.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.7.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.7.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.7.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.8.linear1.weight\n","Skipping key: decoder.model.decoder.layers.8.linear2.weight\n","Skipping key: decoder.model.decoder.layers.8.norm1.weight\n","Skipping key: decoder.model.decoder.layers.8.norm1.bias\n","Skipping key: decoder.model.decoder.layers.8.norm2.weight\n","Skipping key: decoder.model.decoder.layers.8.norm2.bias\n","Skipping key: decoder.model.decoder.layers.8.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.8.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.8.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.8.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.9.linear1.weight\n","Skipping key: decoder.model.decoder.layers.9.linear2.weight\n","Skipping key: decoder.model.decoder.layers.9.norm1.weight\n","Skipping key: decoder.model.decoder.layers.9.norm1.bias\n","Skipping key: decoder.model.decoder.layers.9.norm2.weight\n","Skipping key: decoder.model.decoder.layers.9.norm2.bias\n","Skipping key: decoder.model.decoder.layers.9.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.9.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.9.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.9.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.10.linear1.weight\n","Skipping key: decoder.model.decoder.layers.10.linear2.weight\n","Skipping key: decoder.model.decoder.layers.10.norm1.weight\n","Skipping key: decoder.model.decoder.layers.10.norm1.bias\n","Skipping key: decoder.model.decoder.layers.10.norm2.weight\n","Skipping key: decoder.model.decoder.layers.10.norm2.bias\n","Skipping key: decoder.model.decoder.layers.10.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.10.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.10.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.10.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.11.linear1.weight\n","Skipping key: decoder.model.decoder.layers.11.linear2.weight\n","Skipping key: decoder.model.decoder.layers.11.norm1.weight\n","Skipping key: decoder.model.decoder.layers.11.norm1.bias\n","Skipping key: decoder.model.decoder.layers.11.norm2.weight\n","Skipping key: decoder.model.decoder.layers.11.norm2.bias\n","Skipping key: decoder.model.decoder.layers.11.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.11.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.11.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.11.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.12.linear1.weight\n","Skipping key: decoder.model.decoder.layers.12.linear2.weight\n","Skipping key: decoder.model.decoder.layers.12.norm1.weight\n","Skipping key: decoder.model.decoder.layers.12.norm1.bias\n","Skipping key: decoder.model.decoder.layers.12.norm2.weight\n","Skipping key: decoder.model.decoder.layers.12.norm2.bias\n","Skipping key: decoder.model.decoder.layers.12.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.12.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.12.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.12.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.13.linear1.weight\n","Skipping key: decoder.model.decoder.layers.13.linear2.weight\n","Skipping key: decoder.model.decoder.layers.13.norm1.weight\n","Skipping key: decoder.model.decoder.layers.13.norm1.bias\n","Skipping key: decoder.model.decoder.layers.13.norm2.weight\n","Skipping key: decoder.model.decoder.layers.13.norm2.bias\n","Skipping key: decoder.model.decoder.layers.13.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.13.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.13.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.13.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.14.linear1.weight\n","Skipping key: decoder.model.decoder.layers.14.linear2.weight\n","Skipping key: decoder.model.decoder.layers.14.norm1.weight\n","Skipping key: decoder.model.decoder.layers.14.norm1.bias\n","Skipping key: decoder.model.decoder.layers.14.norm2.weight\n","Skipping key: decoder.model.decoder.layers.14.norm2.bias\n","Skipping key: decoder.model.decoder.layers.14.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.14.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.14.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.14.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.15.linear1.weight\n","Skipping key: decoder.model.decoder.layers.15.linear2.weight\n","Skipping key: decoder.model.decoder.layers.15.norm1.weight\n","Skipping key: decoder.model.decoder.layers.15.norm1.bias\n","Skipping key: decoder.model.decoder.layers.15.norm2.weight\n","Skipping key: decoder.model.decoder.layers.15.norm2.bias\n","Skipping key: decoder.model.decoder.layers.15.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.15.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.15.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.15.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.16.linear1.weight\n","Skipping key: decoder.model.decoder.layers.16.linear2.weight\n","Skipping key: decoder.model.decoder.layers.16.norm1.weight\n","Skipping key: decoder.model.decoder.layers.16.norm1.bias\n","Skipping key: decoder.model.decoder.layers.16.norm2.weight\n","Skipping key: decoder.model.decoder.layers.16.norm2.bias\n","Skipping key: decoder.model.decoder.layers.16.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.16.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.16.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.16.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.17.linear1.weight\n","Skipping key: decoder.model.decoder.layers.17.linear2.weight\n","Skipping key: decoder.model.decoder.layers.17.norm1.weight\n","Skipping key: decoder.model.decoder.layers.17.norm1.bias\n","Skipping key: decoder.model.decoder.layers.17.norm2.weight\n","Skipping key: decoder.model.decoder.layers.17.norm2.bias\n","Skipping key: decoder.model.decoder.layers.17.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.17.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.17.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.17.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.18.linear1.weight\n","Skipping key: decoder.model.decoder.layers.18.linear2.weight\n","Skipping key: decoder.model.decoder.layers.18.norm1.weight\n","Skipping key: decoder.model.decoder.layers.18.norm1.bias\n","Skipping key: decoder.model.decoder.layers.18.norm2.weight\n","Skipping key: decoder.model.decoder.layers.18.norm2.bias\n","Skipping key: decoder.model.decoder.layers.18.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.18.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.18.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.18.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.19.linear1.weight\n","Skipping key: decoder.model.decoder.layers.19.linear2.weight\n","Skipping key: decoder.model.decoder.layers.19.norm1.weight\n","Skipping key: decoder.model.decoder.layers.19.norm1.bias\n","Skipping key: decoder.model.decoder.layers.19.norm2.weight\n","Skipping key: decoder.model.decoder.layers.19.norm2.bias\n","Skipping key: decoder.model.decoder.layers.19.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.19.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.19.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.19.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.20.linear1.weight\n","Skipping key: decoder.model.decoder.layers.20.linear2.weight\n","Skipping key: decoder.model.decoder.layers.20.norm1.weight\n","Skipping key: decoder.model.decoder.layers.20.norm1.bias\n","Skipping key: decoder.model.decoder.layers.20.norm2.weight\n","Skipping key: decoder.model.decoder.layers.20.norm2.bias\n","Skipping key: decoder.model.decoder.layers.20.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.20.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.20.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.20.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.21.linear1.weight\n","Skipping key: decoder.model.decoder.layers.21.linear2.weight\n","Skipping key: decoder.model.decoder.layers.21.norm1.weight\n","Skipping key: decoder.model.decoder.layers.21.norm1.bias\n","Skipping key: decoder.model.decoder.layers.21.norm2.weight\n","Skipping key: decoder.model.decoder.layers.21.norm2.bias\n","Skipping key: decoder.model.decoder.layers.21.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.21.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.21.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.21.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.22.linear1.weight\n","Skipping key: decoder.model.decoder.layers.22.linear2.weight\n","Skipping key: decoder.model.decoder.layers.22.norm1.weight\n","Skipping key: decoder.model.decoder.layers.22.norm1.bias\n","Skipping key: decoder.model.decoder.layers.22.norm2.weight\n","Skipping key: decoder.model.decoder.layers.22.norm2.bias\n","Skipping key: decoder.model.decoder.layers.22.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.22.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.22.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.22.norm_cross.bias\n","Resizing tensor from torch.Size([3072, 1024]) to torch.Size([1024, 1024])\n","Skipping key: decoder.model.decoder.layers.23.linear1.weight\n","Skipping key: decoder.model.decoder.layers.23.linear2.weight\n","Skipping key: decoder.model.decoder.layers.23.norm1.weight\n","Skipping key: decoder.model.decoder.layers.23.norm1.bias\n","Skipping key: decoder.model.decoder.layers.23.norm2.weight\n","Skipping key: decoder.model.decoder.layers.23.norm2.bias\n","Skipping key: decoder.model.decoder.layers.23.cross_attention.in_proj_weight\n","Skipping key: decoder.model.decoder.layers.23.cross_attention.out_proj.weight\n","Skipping key: decoder.model.decoder.layers.23.norm_cross.weight\n","Skipping key: decoder.model.decoder.layers.23.norm_cross.bias\n"]},{"data":{"text/plain":["_IncompatibleKeys(missing_keys=['text_encoder.shared.weight', 'text_encoder.encoder.embed_tokens.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'text_encoder.encoder.block.0.layer.0.layer_norm.weight', 'text_encoder.encoder.block.0.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.0.layer.1.layer_norm.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.1.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.1.layer.0.layer_norm.weight', 'text_encoder.encoder.block.1.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.1.layer.1.layer_norm.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.2.layer.0.layer_norm.weight', 'text_encoder.encoder.block.2.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.2.layer.1.layer_norm.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.3.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.3.layer.0.layer_norm.weight', 'text_encoder.encoder.block.3.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.3.layer.1.layer_norm.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.4.layer.0.layer_norm.weight', 'text_encoder.encoder.block.4.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.4.layer.1.layer_norm.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.5.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.5.layer.0.layer_norm.weight', 'text_encoder.encoder.block.5.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.5.layer.1.layer_norm.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.6.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.6.layer.0.layer_norm.weight', 'text_encoder.encoder.block.6.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.6.layer.1.layer_norm.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.7.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.7.layer.0.layer_norm.weight', 'text_encoder.encoder.block.7.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.7.layer.1.layer_norm.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.8.layer.0.layer_norm.weight', 'text_encoder.encoder.block.8.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.8.layer.1.layer_norm.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.9.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.9.layer.0.layer_norm.weight', 'text_encoder.encoder.block.9.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.9.layer.1.layer_norm.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.10.layer.0.layer_norm.weight', 'text_encoder.encoder.block.10.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.10.layer.1.layer_norm.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.q.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.v.weight', 'text_encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'text_encoder.encoder.block.11.layer.0.layer_norm.weight', 'text_encoder.encoder.block.11.layer.1.DenseReluDense.wi.weight', 'text_encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'text_encoder.encoder.block.11.layer.1.layer_norm.weight', 'text_encoder.encoder.final_layer_norm.weight', 'audio_encoder.encoder.layers.0.conv.bias', 'audio_encoder.encoder.layers.0.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.0.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.1.block.1.conv.bias', 'audio_encoder.encoder.layers.1.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.1.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.1.block.3.conv.bias', 'audio_encoder.encoder.layers.1.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.1.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.3.conv.bias', 'audio_encoder.encoder.layers.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.4.block.1.conv.bias', 'audio_encoder.encoder.layers.4.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.4.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.4.block.3.conv.bias', 'audio_encoder.encoder.layers.4.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.4.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.6.conv.bias', 'audio_encoder.encoder.layers.6.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.6.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.7.block.1.conv.bias', 'audio_encoder.encoder.layers.7.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.7.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.7.block.3.conv.bias', 'audio_encoder.encoder.layers.7.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.7.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.9.conv.bias', 'audio_encoder.encoder.layers.9.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.9.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.10.block.1.conv.bias', 'audio_encoder.encoder.layers.10.block.1.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.10.block.1.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.10.block.3.conv.bias', 'audio_encoder.encoder.layers.10.block.3.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.10.block.3.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.12.conv.bias', 'audio_encoder.encoder.layers.12.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.12.conv.parametrizations.weight.original1', 'audio_encoder.encoder.layers.13.lstm.weight_ih_l0', 'audio_encoder.encoder.layers.13.lstm.weight_hh_l0', 'audio_encoder.encoder.layers.13.lstm.bias_ih_l0', 'audio_encoder.encoder.layers.13.lstm.bias_hh_l0', 'audio_encoder.encoder.layers.13.lstm.weight_ih_l1', 'audio_encoder.encoder.layers.13.lstm.weight_hh_l1', 'audio_encoder.encoder.layers.13.lstm.bias_ih_l1', 'audio_encoder.encoder.layers.13.lstm.bias_hh_l1', 'audio_encoder.encoder.layers.15.conv.bias', 'audio_encoder.encoder.layers.15.conv.parametrizations.weight.original0', 'audio_encoder.encoder.layers.15.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.0.conv.bias', 'audio_encoder.decoder.layers.0.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.0.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.1.lstm.weight_ih_l0', 'audio_encoder.decoder.layers.1.lstm.weight_hh_l0', 'audio_encoder.decoder.layers.1.lstm.bias_ih_l0', 'audio_encoder.decoder.layers.1.lstm.bias_hh_l0', 'audio_encoder.decoder.layers.1.lstm.weight_ih_l1', 'audio_encoder.decoder.layers.1.lstm.weight_hh_l1', 'audio_encoder.decoder.layers.1.lstm.bias_ih_l1', 'audio_encoder.decoder.layers.1.lstm.bias_hh_l1', 'audio_encoder.decoder.layers.3.conv.bias', 'audio_encoder.decoder.layers.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.4.block.1.conv.bias', 'audio_encoder.decoder.layers.4.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.4.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.4.block.3.conv.bias', 'audio_encoder.decoder.layers.4.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.4.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.6.conv.bias', 'audio_encoder.decoder.layers.6.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.6.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.7.block.1.conv.bias', 'audio_encoder.decoder.layers.7.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.7.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.7.block.3.conv.bias', 'audio_encoder.decoder.layers.7.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.7.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.9.conv.bias', 'audio_encoder.decoder.layers.9.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.9.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.10.block.1.conv.bias', 'audio_encoder.decoder.layers.10.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.10.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.10.block.3.conv.bias', 'audio_encoder.decoder.layers.10.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.10.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.12.conv.bias', 'audio_encoder.decoder.layers.12.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.12.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.13.block.1.conv.bias', 'audio_encoder.decoder.layers.13.block.1.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.13.block.1.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.13.block.3.conv.bias', 'audio_encoder.decoder.layers.13.block.3.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.13.block.3.conv.parametrizations.weight.original1', 'audio_encoder.decoder.layers.15.conv.bias', 'audio_encoder.decoder.layers.15.conv.parametrizations.weight.original0', 'audio_encoder.decoder.layers.15.conv.parametrizations.weight.original1', 'audio_encoder.quantizer.layers.0.codebook.inited', 'audio_encoder.quantizer.layers.0.codebook.cluster_size', 'audio_encoder.quantizer.layers.0.codebook.embed', 'audio_encoder.quantizer.layers.0.codebook.embed_avg', 'audio_encoder.quantizer.layers.1.codebook.inited', 'audio_encoder.quantizer.layers.1.codebook.cluster_size', 'audio_encoder.quantizer.layers.1.codebook.embed', 'audio_encoder.quantizer.layers.1.codebook.embed_avg', 'audio_encoder.quantizer.layers.2.codebook.inited', 'audio_encoder.quantizer.layers.2.codebook.cluster_size', 'audio_encoder.quantizer.layers.2.codebook.embed', 'audio_encoder.quantizer.layers.2.codebook.embed_avg', 'audio_encoder.quantizer.layers.3.codebook.inited', 'audio_encoder.quantizer.layers.3.codebook.cluster_size', 'audio_encoder.quantizer.layers.3.codebook.embed', 'audio_encoder.quantizer.layers.3.codebook.embed_avg', 'decoder.model.decoder.embed_positions.weights', 'decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.fc1.weight', 'decoder.model.decoder.layers.0.fc2.weight', 'decoder.model.decoder.layers.0.final_layer_norm.weight', 'decoder.model.decoder.layers.0.final_layer_norm.bias', 'decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.fc1.weight', 'decoder.model.decoder.layers.1.fc2.weight', 'decoder.model.decoder.layers.1.final_layer_norm.weight', 'decoder.model.decoder.layers.1.final_layer_norm.bias', 'decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.fc1.weight', 'decoder.model.decoder.layers.2.fc2.weight', 'decoder.model.decoder.layers.2.final_layer_norm.weight', 'decoder.model.decoder.layers.2.final_layer_norm.bias', 'decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.fc1.weight', 'decoder.model.decoder.layers.3.fc2.weight', 'decoder.model.decoder.layers.3.final_layer_norm.weight', 'decoder.model.decoder.layers.3.final_layer_norm.bias', 'decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.fc1.weight', 'decoder.model.decoder.layers.4.fc2.weight', 'decoder.model.decoder.layers.4.final_layer_norm.weight', 'decoder.model.decoder.layers.4.final_layer_norm.bias', 'decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.fc1.weight', 'decoder.model.decoder.layers.5.fc2.weight', 'decoder.model.decoder.layers.5.final_layer_norm.weight', 'decoder.model.decoder.layers.5.final_layer_norm.bias', 'decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.fc1.weight', 'decoder.model.decoder.layers.6.fc2.weight', 'decoder.model.decoder.layers.6.final_layer_norm.weight', 'decoder.model.decoder.layers.6.final_layer_norm.bias', 'decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.fc1.weight', 'decoder.model.decoder.layers.7.fc2.weight', 'decoder.model.decoder.layers.7.final_layer_norm.weight', 'decoder.model.decoder.layers.7.final_layer_norm.bias', 'decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.fc1.weight', 'decoder.model.decoder.layers.8.fc2.weight', 'decoder.model.decoder.layers.8.final_layer_norm.weight', 'decoder.model.decoder.layers.8.final_layer_norm.bias', 'decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.fc1.weight', 'decoder.model.decoder.layers.9.fc2.weight', 'decoder.model.decoder.layers.9.final_layer_norm.weight', 'decoder.model.decoder.layers.9.final_layer_norm.bias', 'decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.fc1.weight', 'decoder.model.decoder.layers.10.fc2.weight', 'decoder.model.decoder.layers.10.final_layer_norm.weight', 'decoder.model.decoder.layers.10.final_layer_norm.bias', 'decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.fc1.weight', 'decoder.model.decoder.layers.11.fc2.weight', 'decoder.model.decoder.layers.11.final_layer_norm.weight', 'decoder.model.decoder.layers.11.final_layer_norm.bias', 'decoder.model.decoder.layers.12.self_attn.v_proj.weight', 'decoder.model.decoder.layers.12.self_attn.q_proj.weight', 'decoder.model.decoder.layers.12.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.12.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.12.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.12.fc1.weight', 'decoder.model.decoder.layers.12.fc2.weight', 'decoder.model.decoder.layers.12.final_layer_norm.weight', 'decoder.model.decoder.layers.12.final_layer_norm.bias', 'decoder.model.decoder.layers.13.self_attn.v_proj.weight', 'decoder.model.decoder.layers.13.self_attn.q_proj.weight', 'decoder.model.decoder.layers.13.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.13.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.13.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.13.fc1.weight', 'decoder.model.decoder.layers.13.fc2.weight', 'decoder.model.decoder.layers.13.final_layer_norm.weight', 'decoder.model.decoder.layers.13.final_layer_norm.bias', 'decoder.model.decoder.layers.14.self_attn.v_proj.weight', 'decoder.model.decoder.layers.14.self_attn.q_proj.weight', 'decoder.model.decoder.layers.14.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.14.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.14.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.14.fc1.weight', 'decoder.model.decoder.layers.14.fc2.weight', 'decoder.model.decoder.layers.14.final_layer_norm.weight', 'decoder.model.decoder.layers.14.final_layer_norm.bias', 'decoder.model.decoder.layers.15.self_attn.v_proj.weight', 'decoder.model.decoder.layers.15.self_attn.q_proj.weight', 'decoder.model.decoder.layers.15.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.15.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.15.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.15.fc1.weight', 'decoder.model.decoder.layers.15.fc2.weight', 'decoder.model.decoder.layers.15.final_layer_norm.weight', 'decoder.model.decoder.layers.15.final_layer_norm.bias', 'decoder.model.decoder.layers.16.self_attn.v_proj.weight', 'decoder.model.decoder.layers.16.self_attn.q_proj.weight', 'decoder.model.decoder.layers.16.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.16.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.fc1.weight', 'decoder.model.decoder.layers.16.fc2.weight', 'decoder.model.decoder.layers.16.final_layer_norm.weight', 'decoder.model.decoder.layers.16.final_layer_norm.bias', 'decoder.model.decoder.layers.17.self_attn.v_proj.weight', 'decoder.model.decoder.layers.17.self_attn.q_proj.weight', 'decoder.model.decoder.layers.17.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.17.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.17.fc1.weight', 'decoder.model.decoder.layers.17.fc2.weight', 'decoder.model.decoder.layers.17.final_layer_norm.weight', 'decoder.model.decoder.layers.17.final_layer_norm.bias', 'decoder.model.decoder.layers.18.self_attn.v_proj.weight', 'decoder.model.decoder.layers.18.self_attn.q_proj.weight', 'decoder.model.decoder.layers.18.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.18.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.fc1.weight', 'decoder.model.decoder.layers.18.fc2.weight', 'decoder.model.decoder.layers.18.final_layer_norm.weight', 'decoder.model.decoder.layers.18.final_layer_norm.bias', 'decoder.model.decoder.layers.19.self_attn.v_proj.weight', 'decoder.model.decoder.layers.19.self_attn.q_proj.weight', 'decoder.model.decoder.layers.19.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.19.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.fc1.weight', 'decoder.model.decoder.layers.19.fc2.weight', 'decoder.model.decoder.layers.19.final_layer_norm.weight', 'decoder.model.decoder.layers.19.final_layer_norm.bias', 'decoder.model.decoder.layers.20.self_attn.v_proj.weight', 'decoder.model.decoder.layers.20.self_attn.q_proj.weight', 'decoder.model.decoder.layers.20.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.20.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.20.fc1.weight', 'decoder.model.decoder.layers.20.fc2.weight', 'decoder.model.decoder.layers.20.final_layer_norm.weight', 'decoder.model.decoder.layers.20.final_layer_norm.bias', 'decoder.model.decoder.layers.21.self_attn.v_proj.weight', 'decoder.model.decoder.layers.21.self_attn.q_proj.weight', 'decoder.model.decoder.layers.21.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.21.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.21.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.21.fc1.weight', 'decoder.model.decoder.layers.21.fc2.weight', 'decoder.model.decoder.layers.21.final_layer_norm.weight', 'decoder.model.decoder.layers.21.final_layer_norm.bias', 'decoder.model.decoder.layers.22.self_attn.v_proj.weight', 'decoder.model.decoder.layers.22.self_attn.q_proj.weight', 'decoder.model.decoder.layers.22.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.22.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.22.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.22.fc1.weight', 'decoder.model.decoder.layers.22.fc2.weight', 'decoder.model.decoder.layers.22.final_layer_norm.weight', 'decoder.model.decoder.layers.22.final_layer_norm.bias', 'decoder.model.decoder.layers.23.self_attn.v_proj.weight', 'decoder.model.decoder.layers.23.self_attn.q_proj.weight', 'decoder.model.decoder.layers.23.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.23.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.23.fc1.weight', 'decoder.model.decoder.layers.23.fc2.weight', 'decoder.model.decoder.layers.23.final_layer_norm.weight', 'decoder.model.decoder.layers.23.final_layer_norm.bias', 'decoder.lm_heads.0.weight', 'decoder.lm_heads.1.weight', 'decoder.lm_heads.2.weight', 'decoder.lm_heads.3.weight', 'enc_to_dec_proj.weight', 'enc_to_dec_proj.bias'], unexpected_keys=['decoder.model.decoder.layers.0.linear1.weight', 'decoder.model.decoder.layers.0.linear2.weight', 'decoder.model.decoder.layers.0.norm1.weight', 'decoder.model.decoder.layers.0.norm1.bias', 'decoder.model.decoder.layers.0.norm2.weight', 'decoder.model.decoder.layers.0.norm2.bias', 'decoder.model.decoder.layers.0.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.0.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.0.norm_cross.weight', 'decoder.model.decoder.layers.0.norm_cross.bias', 'decoder.model.decoder.layers.1.linear1.weight', 'decoder.model.decoder.layers.1.linear2.weight', 'decoder.model.decoder.layers.1.norm1.weight', 'decoder.model.decoder.layers.1.norm1.bias', 'decoder.model.decoder.layers.1.norm2.weight', 'decoder.model.decoder.layers.1.norm2.bias', 'decoder.model.decoder.layers.1.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.1.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.1.norm_cross.weight', 'decoder.model.decoder.layers.1.norm_cross.bias', 'decoder.model.decoder.layers.2.linear1.weight', 'decoder.model.decoder.layers.2.linear2.weight', 'decoder.model.decoder.layers.2.norm1.weight', 'decoder.model.decoder.layers.2.norm1.bias', 'decoder.model.decoder.layers.2.norm2.weight', 'decoder.model.decoder.layers.2.norm2.bias', 'decoder.model.decoder.layers.2.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.2.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.2.norm_cross.weight', 'decoder.model.decoder.layers.2.norm_cross.bias', 'decoder.model.decoder.layers.3.linear1.weight', 'decoder.model.decoder.layers.3.linear2.weight', 'decoder.model.decoder.layers.3.norm1.weight', 'decoder.model.decoder.layers.3.norm1.bias', 'decoder.model.decoder.layers.3.norm2.weight', 'decoder.model.decoder.layers.3.norm2.bias', 'decoder.model.decoder.layers.3.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.3.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.3.norm_cross.weight', 'decoder.model.decoder.layers.3.norm_cross.bias', 'decoder.model.decoder.layers.4.linear1.weight', 'decoder.model.decoder.layers.4.linear2.weight', 'decoder.model.decoder.layers.4.norm1.weight', 'decoder.model.decoder.layers.4.norm1.bias', 'decoder.model.decoder.layers.4.norm2.weight', 'decoder.model.decoder.layers.4.norm2.bias', 'decoder.model.decoder.layers.4.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.4.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.4.norm_cross.weight', 'decoder.model.decoder.layers.4.norm_cross.bias', 'decoder.model.decoder.layers.5.linear1.weight', 'decoder.model.decoder.layers.5.linear2.weight', 'decoder.model.decoder.layers.5.norm1.weight', 'decoder.model.decoder.layers.5.norm1.bias', 'decoder.model.decoder.layers.5.norm2.weight', 'decoder.model.decoder.layers.5.norm2.bias', 'decoder.model.decoder.layers.5.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.5.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.5.norm_cross.weight', 'decoder.model.decoder.layers.5.norm_cross.bias', 'decoder.model.decoder.layers.6.linear1.weight', 'decoder.model.decoder.layers.6.linear2.weight', 'decoder.model.decoder.layers.6.norm1.weight', 'decoder.model.decoder.layers.6.norm1.bias', 'decoder.model.decoder.layers.6.norm2.weight', 'decoder.model.decoder.layers.6.norm2.bias', 'decoder.model.decoder.layers.6.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.6.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.6.norm_cross.weight', 'decoder.model.decoder.layers.6.norm_cross.bias', 'decoder.model.decoder.layers.7.linear1.weight', 'decoder.model.decoder.layers.7.linear2.weight', 'decoder.model.decoder.layers.7.norm1.weight', 'decoder.model.decoder.layers.7.norm1.bias', 'decoder.model.decoder.layers.7.norm2.weight', 'decoder.model.decoder.layers.7.norm2.bias', 'decoder.model.decoder.layers.7.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.7.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.7.norm_cross.weight', 'decoder.model.decoder.layers.7.norm_cross.bias', 'decoder.model.decoder.layers.8.linear1.weight', 'decoder.model.decoder.layers.8.linear2.weight', 'decoder.model.decoder.layers.8.norm1.weight', 'decoder.model.decoder.layers.8.norm1.bias', 'decoder.model.decoder.layers.8.norm2.weight', 'decoder.model.decoder.layers.8.norm2.bias', 'decoder.model.decoder.layers.8.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.8.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.8.norm_cross.weight', 'decoder.model.decoder.layers.8.norm_cross.bias', 'decoder.model.decoder.layers.9.linear1.weight', 'decoder.model.decoder.layers.9.linear2.weight', 'decoder.model.decoder.layers.9.norm1.weight', 'decoder.model.decoder.layers.9.norm1.bias', 'decoder.model.decoder.layers.9.norm2.weight', 'decoder.model.decoder.layers.9.norm2.bias', 'decoder.model.decoder.layers.9.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.9.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.9.norm_cross.weight', 'decoder.model.decoder.layers.9.norm_cross.bias', 'decoder.model.decoder.layers.10.linear1.weight', 'decoder.model.decoder.layers.10.linear2.weight', 'decoder.model.decoder.layers.10.norm1.weight', 'decoder.model.decoder.layers.10.norm1.bias', 'decoder.model.decoder.layers.10.norm2.weight', 'decoder.model.decoder.layers.10.norm2.bias', 'decoder.model.decoder.layers.10.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.10.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.10.norm_cross.weight', 'decoder.model.decoder.layers.10.norm_cross.bias', 'decoder.model.decoder.layers.11.linear1.weight', 'decoder.model.decoder.layers.11.linear2.weight', 'decoder.model.decoder.layers.11.norm1.weight', 'decoder.model.decoder.layers.11.norm1.bias', 'decoder.model.decoder.layers.11.norm2.weight', 'decoder.model.decoder.layers.11.norm2.bias', 'decoder.model.decoder.layers.11.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.11.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.11.norm_cross.weight', 'decoder.model.decoder.layers.11.norm_cross.bias', 'decoder.model.decoder.layers.12.linear1.weight', 'decoder.model.decoder.layers.12.linear2.weight', 'decoder.model.decoder.layers.12.norm1.weight', 'decoder.model.decoder.layers.12.norm1.bias', 'decoder.model.decoder.layers.12.norm2.weight', 'decoder.model.decoder.layers.12.norm2.bias', 'decoder.model.decoder.layers.12.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.12.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.12.norm_cross.weight', 'decoder.model.decoder.layers.12.norm_cross.bias', 'decoder.model.decoder.layers.13.linear1.weight', 'decoder.model.decoder.layers.13.linear2.weight', 'decoder.model.decoder.layers.13.norm1.weight', 'decoder.model.decoder.layers.13.norm1.bias', 'decoder.model.decoder.layers.13.norm2.weight', 'decoder.model.decoder.layers.13.norm2.bias', 'decoder.model.decoder.layers.13.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.13.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.13.norm_cross.weight', 'decoder.model.decoder.layers.13.norm_cross.bias', 'decoder.model.decoder.layers.14.linear1.weight', 'decoder.model.decoder.layers.14.linear2.weight', 'decoder.model.decoder.layers.14.norm1.weight', 'decoder.model.decoder.layers.14.norm1.bias', 'decoder.model.decoder.layers.14.norm2.weight', 'decoder.model.decoder.layers.14.norm2.bias', 'decoder.model.decoder.layers.14.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.14.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.14.norm_cross.weight', 'decoder.model.decoder.layers.14.norm_cross.bias', 'decoder.model.decoder.layers.15.linear1.weight', 'decoder.model.decoder.layers.15.linear2.weight', 'decoder.model.decoder.layers.15.norm1.weight', 'decoder.model.decoder.layers.15.norm1.bias', 'decoder.model.decoder.layers.15.norm2.weight', 'decoder.model.decoder.layers.15.norm2.bias', 'decoder.model.decoder.layers.15.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.15.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.15.norm_cross.weight', 'decoder.model.decoder.layers.15.norm_cross.bias', 'decoder.model.decoder.layers.16.linear1.weight', 'decoder.model.decoder.layers.16.linear2.weight', 'decoder.model.decoder.layers.16.norm1.weight', 'decoder.model.decoder.layers.16.norm1.bias', 'decoder.model.decoder.layers.16.norm2.weight', 'decoder.model.decoder.layers.16.norm2.bias', 'decoder.model.decoder.layers.16.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.16.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.16.norm_cross.weight', 'decoder.model.decoder.layers.16.norm_cross.bias', 'decoder.model.decoder.layers.17.linear1.weight', 'decoder.model.decoder.layers.17.linear2.weight', 'decoder.model.decoder.layers.17.norm1.weight', 'decoder.model.decoder.layers.17.norm1.bias', 'decoder.model.decoder.layers.17.norm2.weight', 'decoder.model.decoder.layers.17.norm2.bias', 'decoder.model.decoder.layers.17.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.17.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.17.norm_cross.weight', 'decoder.model.decoder.layers.17.norm_cross.bias', 'decoder.model.decoder.layers.18.linear1.weight', 'decoder.model.decoder.layers.18.linear2.weight', 'decoder.model.decoder.layers.18.norm1.weight', 'decoder.model.decoder.layers.18.norm1.bias', 'decoder.model.decoder.layers.18.norm2.weight', 'decoder.model.decoder.layers.18.norm2.bias', 'decoder.model.decoder.layers.18.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.18.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.18.norm_cross.weight', 'decoder.model.decoder.layers.18.norm_cross.bias', 'decoder.model.decoder.layers.19.linear1.weight', 'decoder.model.decoder.layers.19.linear2.weight', 'decoder.model.decoder.layers.19.norm1.weight', 'decoder.model.decoder.layers.19.norm1.bias', 'decoder.model.decoder.layers.19.norm2.weight', 'decoder.model.decoder.layers.19.norm2.bias', 'decoder.model.decoder.layers.19.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.19.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.19.norm_cross.weight', 'decoder.model.decoder.layers.19.norm_cross.bias', 'decoder.model.decoder.layers.20.linear1.weight', 'decoder.model.decoder.layers.20.linear2.weight', 'decoder.model.decoder.layers.20.norm1.weight', 'decoder.model.decoder.layers.20.norm1.bias', 'decoder.model.decoder.layers.20.norm2.weight', 'decoder.model.decoder.layers.20.norm2.bias', 'decoder.model.decoder.layers.20.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.20.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.20.norm_cross.weight', 'decoder.model.decoder.layers.20.norm_cross.bias', 'decoder.model.decoder.layers.21.linear1.weight', 'decoder.model.decoder.layers.21.linear2.weight', 'decoder.model.decoder.layers.21.norm1.weight', 'decoder.model.decoder.layers.21.norm1.bias', 'decoder.model.decoder.layers.21.norm2.weight', 'decoder.model.decoder.layers.21.norm2.bias', 'decoder.model.decoder.layers.21.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.21.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.21.norm_cross.weight', 'decoder.model.decoder.layers.21.norm_cross.bias', 'decoder.model.decoder.layers.22.linear1.weight', 'decoder.model.decoder.layers.22.linear2.weight', 'decoder.model.decoder.layers.22.norm1.weight', 'decoder.model.decoder.layers.22.norm1.bias', 'decoder.model.decoder.layers.22.norm2.weight', 'decoder.model.decoder.layers.22.norm2.bias', 'decoder.model.decoder.layers.22.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.22.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.22.norm_cross.weight', 'decoder.model.decoder.layers.22.norm_cross.bias', 'decoder.model.decoder.layers.23.linear1.weight', 'decoder.model.decoder.layers.23.linear2.weight', 'decoder.model.decoder.layers.23.norm1.weight', 'decoder.model.decoder.layers.23.norm1.bias', 'decoder.model.decoder.layers.23.norm2.weight', 'decoder.model.decoder.layers.23.norm2.bias', 'decoder.model.decoder.layers.23.cross_attention.in_proj_weight', 'decoder.model.decoder.layers.23.cross_attention.out_proj.weight', 'decoder.model.decoder.layers.23.norm_cross.weight', 'decoder.model.decoder.layers.23.norm_cross.bias'])"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","def resize_tensor(tensor, target_shape):\n","    \"\"\"\n","    기존 텐서를 주어진 target_shape로 리사이즈합니다.\n","    \"\"\"\n","    if tensor.shape == target_shape:\n","        return tensor\n","    else:\n","        print(f\"Resizing tensor from {tensor.shape} to {target_shape}\")\n","        return torch.nn.functional.interpolate(\n","            tensor.unsqueeze(0), size=target_shape, mode=\"linear\"\n","        ).squeeze(0)\n","\n","import torch\n","\n","def adjust_state_dict(state_dict, model):\n","    new_state_dict = state_dict.copy()\n","    for key, value in state_dict.items():\n","        if key in model.state_dict():\n","            model_value = model.state_dict()[key]\n","            if value.shape != model_value.shape:\n","                print(f\"Resizing tensor from {value.shape} to {model_value.shape}\")\n","                # 텐서가 2D일 경우 크기 변경\n","                if len(value.shape) == 2:\n","                    resized_value = value[:model_value.shape[0], :model_value.shape[1]]\n","                else:\n","                    # 3D 이상일 경우 보간\n","                    resized_value = torch.nn.functional.interpolate(\n","                        value.unsqueeze(0).unsqueeze(0),\n","                        size=model_value.shape,\n","                        mode='nearest'\n","                    ).squeeze()\n","                new_state_dict[key] = resized_value\n","        else:\n","            print(f\"Skipping key: {key}\")\n","    return new_state_dict\n","# 모델 가중치 조정\n","converted_state_dict = adjust_state_dict(converted_state_dict, model)\n","\n","# 수정된 가중치 로드\n","model.load_state_dict(converted_state_dict, strict=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"elapsed":297,"status":"error","timestamp":1733084567201,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"6f0EWqbA4AFx","outputId":"0a726eb6-6e63-42e6-8ab2-31c623a98bd4"},"outputs":[{"ename":"NameError","evalue":"name 'adjusted_state_dict' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-48b8f8e71a2a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madjusted_state_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key}: {value.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'adjusted_state_dict' is not defined"]}],"source":["for key, value in adjusted_state_dict.items():\n","    print(f\"{key}: {value.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":488,"status":"ok","timestamp":1733084916878,"user":{"displayName":"최혜정","userId":"05709380919630694733"},"user_tz":-540},"id":"cvlcvxnU6PBM","outputId":"cf3e459a-e8ee-4790-c51a-46796c8696b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/projects/carecruise_intern/audiocraft\n"]}],"source":["cd /content/drive/MyDrive/projects/carecruise_intern/audiocraft"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"bMy_TGbv6TFQ","outputId":"419320e3-c172-4f94-d2f1-becc4e668dbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,224 kB]\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n","Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,514 kB]\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,619 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,513 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n","Fetched 19.5 MB in 8s (2,438 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libsndfile1 is already the newest version (1.0.31-2ubuntu0.1).\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n","Obtaining file:///content/drive/MyDrive/projects/carecruise_intern/audiocraft\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting av==11.0.0 (from audiocraft==1.4.0a1)\n","  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (0.8.0)\n","Collecting flashy>=0.0.1 (from audiocraft==1.4.0a1)\n","  Downloading flashy-0.0.2.tar.gz (72 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting hydra-core>=1.1 (from audiocraft==1.4.0a1)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting hydra_colorlog (from audiocraft==1.4.0a1)\n","  Downloading hydra_colorlog-1.2.0-py3-none-any.whl.metadata (949 bytes)\n","Collecting julius (from audiocraft==1.4.0a1)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting num2words (from audiocraft==1.4.0a1)\n","  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (1.26.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (0.2.0)\n","Collecting spacy==3.7.6 (from audiocraft==1.4.0a1)\n","  Downloading spacy-3.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n","Collecting torch==2.1.0 (from audiocraft==1.4.0a1)\n","  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchaudio<2.1.2,>=2.0.0 (from audiocraft==1.4.0a1)\n","  Downloading torchaudio-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (0.26.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (4.66.6)\n","Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (4.46.2)\n","Collecting xformers<0.0.23 (from audiocraft==1.4.0a1)\n","  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Collecting demucs (from audiocraft==1.4.0a1)\n","  Downloading demucs-4.0.1.tar.gz (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (0.10.2.post1)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (0.12.1)\n","\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/gradio/\u001b[0m\u001b[33m\n","\u001b[0mCollecting gradio (from audiocraft==1.4.0a1)\n","  Downloading gradio-5.7.1-py3-none-any.whl.metadata (16 kB)\n","Collecting torchmetrics (from audiocraft==1.4.0a1)\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Collecting encodec (from audiocraft==1.4.0a1)\n","  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from audiocraft==1.4.0a1) (4.25.5)\n","Collecting torchvision==0.16.0 (from audiocraft==1.4.0a1)\n","  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torchtext==0.16.0 (from audiocraft==1.4.0a1)\n","  Downloading torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\n","Collecting pesq (from audiocraft==1.4.0a1)\n","  Downloading pesq-0.0.4.tar.gz (38 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pystoi (from audiocraft==1.4.0a1)\n","  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (0.13.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.6->audiocraft==1.4.0a1) (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->audiocraft==1.4.0a1) (3.16.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->audiocraft==1.4.0a1) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->audiocraft==1.4.0a1) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->audiocraft==1.4.0a1) (3.4.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->audiocraft==1.4.0a1) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.1.0 (from torch==2.1.0->audiocraft==1.4.0a1)\n","  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Collecting torchdata==0.7.0 (from torchtext==0.16.0->audiocraft==1.4.0a1)\n","  Downloading torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0->audiocraft==1.4.0a1) (11.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->audiocraft==1.4.0a1) (12.6.77)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext==0.16.0->audiocraft==1.4.0a1) (2.2.3)\n","Collecting dora_search (from flashy>=0.0.1->audiocraft==1.4.0a1)\n","  Downloading dora_search-0.1.12.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.1/87.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting colorlog (from flashy>=0.0.1->audiocraft==1.4.0a1)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.1->audiocraft==1.4.0a1)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->audiocraft==1.4.0a1)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchaudio<2.1.2,>=2.0.0 (from audiocraft==1.4.0a1)\n","  Downloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.7 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft==1.4.0a1) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft==1.4.0a1) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft==1.4.0a1) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft==1.4.0a1) (0.20.3)\n","Collecting lameenc>=1.2 (from demucs->audiocraft==1.4.0a1)\n","  Downloading lameenc-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (803 bytes)\n","Collecting openunmix (from demucs->audiocraft==1.4.0a1)\n","  Downloading openunmix-1.3.0-py3-none-any.whl.metadata (17 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio->audiocraft==1.4.0a1)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft==1.4.0a1) (3.7.1)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio->audiocraft==1.4.0a1)\n","  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio->audiocraft==1.4.0a1)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.5.0 (from gradio->audiocraft==1.4.0a1)\n","  Downloading gradio_client-1.5.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft==1.4.0a1) (0.27.2)\n","Collecting markupsafe~=2.0 (from gradio->audiocraft==1.4.0a1)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft==1.4.0a1) (3.10.11)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft==1.4.0a1) (2.2.2)\n","Collecting pydub (from gradio->audiocraft==1.4.0a1)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart==0.0.12 (from gradio->audiocraft==1.4.0a1)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Collecting ruff>=0.2.2 (from gradio->audiocraft==1.4.0a1)\n","  Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx<1.0,>=0.1.1 (from gradio->audiocraft==1.4.0a1)\n","  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting semantic-version~=2.0 (from gradio->audiocraft==1.4.0a1)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio->audiocraft==1.4.0a1)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Collecting tomlkit==0.12.0 (from gradio->audiocraft==1.4.0a1)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Collecting uvicorn>=0.14.0 (from gradio->audiocraft==1.4.0a1)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.5.0->gradio->audiocraft==1.4.0a1)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (1.13.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (1.5.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (0.60.0)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (0.5.0.post1)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft==1.4.0a1) (1.1.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->audiocraft==1.4.0a1) (1.17.1)\n","Collecting docopt>=0.6.2 (from num2words->audiocraft==1.4.0a1)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics->audiocraft==1.4.0a1)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->audiocraft==1.4.0a1) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->audiocraft==1.4.0a1) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->audiocraft==1.4.0a1) (1.2.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->audiocraft==1.4.0a1) (2.22)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->audiocraft==1.4.0a1) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->audiocraft==1.4.0a1) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio->audiocraft==1.4.0a1) (0.14.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.6->audiocraft==1.4.0a1) (1.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->audiocraft==1.4.0a1) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->audiocraft==1.4.0a1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->audiocraft==1.4.0a1) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->audiocraft==1.4.0a1) (2024.2)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->audiocraft==1.4.0a1) (4.3.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6->audiocraft==1.4.0a1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6->audiocraft==1.4.0a1) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6->audiocraft==1.4.0a1) (3.4.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->audiocraft==1.4.0a1) (3.5.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6->audiocraft==1.4.0a1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6->audiocraft==1.4.0a1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6->audiocraft==1.4.0a1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6->audiocraft==1.4.0a1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6->audiocraft==1.4.0a1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6->audiocraft==1.4.0a1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6->audiocraft==1.4.0a1) (7.0.5)\n","Collecting retrying (from dora_search->flashy>=0.0.1->audiocraft==1.4.0a1)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Collecting submitit (from dora_search->flashy>=0.0.1->audiocraft==1.4.0a1)\n","  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n","Collecting treetable (from dora_search->flashy>=0.0.1->audiocraft==1.4.0a1)\n","  Downloading treetable-0.2.5.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->audiocraft==1.4.0a1) (1.3.0)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.6->audiocraft==1.4.0a1) (1.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio->audiocraft==1.4.0a1) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6->audiocraft==1.4.0a1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6->audiocraft==1.4.0a1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.6->audiocraft==1.4.0a1) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->dora_search->flashy>=0.0.1->audiocraft==1.4.0a1) (3.1.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6->audiocraft==1.4.0a1) (0.1.2)\n","\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'spacy' candidate (version 3.7.6 at https://files.pythonhosted.org/packages/e0/70/99e35ffa162db57ddb20080243d07afe3031eb361035759c3124424569b0/spacy-3.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/spacy/) (requires-python:>=3.7))\n","Reason for being yanked: Incorrect compatibility for transformer models\u001b[0m\u001b[33m\n","\u001b[0mDownloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spacy-3.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m648.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m603.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m576.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m166.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio-5.7.1-py3-none-any.whl (57.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m993.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.5.0-py3-none-any.whl (320 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading hydra_colorlog-1.2.0-py3-none-any.whl (3.6 kB)\n","Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n","Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lameenc-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (239 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.8/239.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading openunmix-1.3.0-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Downloading submitit-1.5.2-py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: flashy, antlr4-python3-runtime, demucs, julius, encodec, pesq, docopt, dora_search, treetable\n","  Building wheel for flashy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flashy: filename=flashy-0.0.2-py3-none-any.whl size=34528 sha256=f24386ad797150870813a9715bd99d0d27339e5e7f45dc317a1c071faad02648\n","  Stored in directory: /root/.cache/pip/wheels/07/bd/3d/16c6bc059203299f37b6014643b739afb7f6d1be13a94fc2f7\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=e16be303588f7936ee1fd52e9eab9255a0cadff12c9760f1d66dd77ba405bae2\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","  Building wheel for demucs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for demucs: filename=demucs-4.0.1-py3-none-any.whl size=78388 sha256=c82ef42ebdd2222e57a2654cdc8e67f5a5f6b9be257b374b00a30310e28add73\n","  Stored in directory: /root/.cache/pip/wheels/2a/65/a1/6cc0e525a84375af3b09823b3326b0ece53c4e68302c054548\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21869 sha256=5088b3b4f902a248f832422158c1c7260f76a11a83ab16bfe391ec46b5f2c8d2\n","  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n","  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=5d6128cf7beee526371ba763c71a9e0d70c64eeb8666b78742a2a445905d6904\n","  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n","  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pesq: filename=pesq-0.0.4-cp310-cp310-linux_x86_64.whl size=262946 sha256=815b1e388aa42ef09d673592672fea079c7540c5810369222d3251148a59783e\n","  Stored in directory: /root/.cache/pip/wheels/c5/4e/2c/251524370c0fdd659e99639a0fbd0ca5a782c3aafcd456b28d\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=86f28bbe3e1b1f702751c28e723b0df79f3144983a9eb6d18292ad0eea45db94\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for dora_search (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dora_search: filename=dora_search-0.1.12-py3-none-any.whl size=75096 sha256=0e6af62b3adbd16aaf436ec9560a325c6819b4ecd2219acaf21d8496e28c4f35\n","  Stored in directory: /root/.cache/pip/wheels/b1/c2/c0/bea5cc405497284d584b958f293ef32c23bad42ae5e44d973c\n","  Building wheel for treetable (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for treetable: filename=treetable-0.2.5-py3-none-any.whl size=7334 sha256=aea0f906dc9dc337c7af7548bcb87eb38d5d386ba2deb1eb03c1e1d973fa1c8c\n","  Stored in directory: /root/.cache/pip/wheels/72/55/0e/91c3655bdb162446f8a7cd477579397544454a63ae7c599c0c\n","Successfully built flashy antlr4-python3-runtime demucs julius encodec pesq docopt dora_search treetable\n","Installing collected packages: pydub, pesq, lameenc, docopt, antlr4-python3-runtime, websockets, uvicorn, triton, treetable, tomlkit, submitit, semantic-version, ruff, retrying, python-multipart, omegaconf, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, num2words, markupsafe, lightning-utilities, ffmpy, colorlog, av, aiofiles, starlette, pystoi, nvidia-cusolver-cu12, nvidia-cudnn-cu12, hydra-core, torch, safehttpx, hydra_colorlog, gradio-client, fastapi, xformers, torchvision, torchmetrics, torchdata, torchaudio, julius, gradio, dora_search, torchtext, spacy, openunmix, flashy, encodec, demucs, audiocraft\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n","    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n","    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.20.1+cu121\n","    Uninstalling torchvision-0.20.1+cu121:\n","      Successfully uninstalled torchvision-0.20.1+cu121\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.5.1+cu121\n","    Uninstalling torchaudio-2.5.1+cu121:\n","      Successfully uninstalled torchaudio-2.5.1+cu121\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.7.5\n","    Uninstalling spacy-3.7.5:\n","      Successfully uninstalled spacy-3.7.5\n","  Running setup.py develop for audiocraft\n","Successfully installed aiofiles-23.2.1 antlr4-python3-runtime-4.9.3 audiocraft-1.4.0a1 av-11.0.0 colorlog-6.9.0 demucs-4.0.1 docopt-0.6.2 dora_search-0.1.12 encodec-0.1.1 fastapi-0.115.5 ffmpy-0.4.0 flashy-0.0.2 gradio-5.7.1 gradio-client-1.5.0 hydra-core-1.3.2 hydra_colorlog-1.2.0 julius-0.2.7 lameenc-1.7.0 lightning-utilities-0.11.9 markupsafe-2.1.5 num2words-0.5.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 openunmix-1.3.0 pesq-0.0.4 pydub-0.25.1 pystoi-0.4.1 python-multipart-0.0.12 retrying-1.3.4 ruff-0.8.1 safehttpx-0.1.1 semantic-version-2.10.0 spacy-3.7.6 starlette-0.41.3 submitit-1.5.2 tomlkit-0.12.0 torch-2.1.0 torchaudio-2.1.0 torchdata-0.7.0 torchmetrics-1.6.0 torchtext-0.16.0 torchvision-0.16.0 treetable-0.2.5 triton-2.1.0 uvicorn-0.32.1 websockets-12.0 xformers-0.0.22.post7\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"1130fbad149840b9af2d5bf3a7c20d39","pip_warning":{"packages":["markupsafe","pydevd_plugins","torch","torchgen","torchvision"]}}},"metadata":{},"output_type":"display_data"}],"source":["# Install necessary system dependencies\n","!sudo apt-get update && sudo apt-get install -y ffmpeg libsndfile1\n","\n","!pip install -e ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fs2oUClT49N0","outputId":"e626239e-8802-47c5-d1c2-933d735ada70"},"outputs":[{"ename":"OSError","evalue":"/usr/local/lib/python3.10/dist-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN2at4_ops10zeros_like4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEENS6_INS5_12MemoryFormatEEE","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-feeca14eea84>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maudiocraft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMusicGen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maudiocraft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio_write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/projects/carecruise_intern/audiocraft/audiocraft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.4.0a1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/projects/carecruise_intern/audiocraft/audiocraft/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_audio_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmusic_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msound_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/projects/carecruise_intern/audiocraft/audiocraft/data/audio.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maudio_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf32_pcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/projects/carecruise_intern/audiocraft/audiocraft/data/audio_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjulius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from . import (  # noqa: F401\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0m_extension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcompliance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfunctional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_extension/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0m_IS_ALIGN_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IS_TORCHAUDIO_EXT_AVAILABLE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libtorchaudio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchaudio\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_extension/utils.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.10/dist-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN2at4_ops10zeros_like4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEENS6_INS5_12MemoryFormatEEE"]}],"source":["import torch\n","from audiocraft.models import MusicGen\n","from audiocraft.data.audio import audio_write\n","import os\n","\n","# 모델 로드\n","def load_musicgen_model(checkpoint_path=None, pretrained_model=\"facebook/musicgen-small\"):\n","    \"\"\"\n","    Audiocraft의 MusicGen 모델을 로드합니다.\n","    checkpoint_path가 제공되면 사용자 학습 모델을 로드합니다.\n","    \"\"\"\n","    if checkpoint_path and os.path.exists(checkpoint_path):\n","        print(f\"Loading fine-tuned model from: {checkpoint_path}\")\n","        model = MusicGen.get_pretrained(checkpoint_path)\n","    else:\n","        print(f\"Loading pre-trained model: {pretrained_model}\")\n","        model = MusicGen.get_pretrained(pretrained_model)\n","    return model\n","\n","# 음악 생성\n","def generate_music(prompts, model, duration=10, sample_rate=32000, output_dir=\"generated_audio\"):\n","    \"\"\"\n","    텍스트 프롬프트로 음악을 생성합니다.\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    model.set_generation_params(duration=duration)\n","\n","    print(f\"Generating music for prompts: {prompts}\")\n","    wavs = model.generate(prompts)\n","\n","    # 생성된 음악 저장 및 반환\n","    audio_paths = []\n","    for idx, wav in enumerate(wavs):\n","        output_path = os.path.join(output_dir, f\"output_{idx}.wav\")\n","        audio_write(output_path, wav.cpu(), sample_rate, strategy=\"loudness\", loudness_compressor=True)\n","        audio_paths.append(output_path)\n","        print(f\"Saved generated audio to: {output_path}\")\n","\n","    return audio_paths\n","\n","# 사용자 코드 실행\n","if __name__ == \"__main__\":\n","    # 설정\n","    checkpoint_path = \"/content/drive/MyDrive/projects/carecruise_intern/audiocraft/logs/efa9cb0e/checkpoints/finetune\"  # Fine-tuned 모델 경로 (optional)\n","    prompts = [\n","        \"a happy disco track with funky bass and synths\",\n","        \"an epic orchestral piece with strings and brass\",\n","        \"a relaxing ambient soundscape with nature sounds\",\n","        \"a high-energy rock song with electric guitars\"\n","    ]\n","    duration = 15  # 초 단위\n","    output_dir = \"generated_audio\"\n","\n","    # 모델 로드\n","    model = load_musicgen_model(checkpoint_path=checkpoint_path)\n","\n","    # 음악 생성\n","    generated_paths = generate_music(prompts, model, duration=duration, output_dir=output_dir)\n","\n","    print(\"Music generation completed.\")"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyM5ccsyPGzJWns4JZftKO7z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0010b3fe765b497ab0025ce2cc374665":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_493c6c6cb1ce4a909c1c9d79c433b8f0","placeholder":"​","style":"IPY_MODEL_026228de01cd4a3d924809e2c8dd9cb4","value":" 1.53k/1.53k [00:00&lt;00:00, 29.2kB/s]"}},"026228de01cd4a3d924809e2c8dd9cb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"121faed1dd3440409708e26a5914cefe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe423cc4b9de480fbbcfbbbcf45bc8ea","max":7866,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46482e1ea1db4b36ad42fdff86b1c474","value":7866}},"12eee4d7b785495d9dd55c99d6ef119e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e8232c8d486405e9ef994a902012561":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"212e440165ce49f8bb240812c87cae21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdb22e1bb65541889e0d046d2ba79ddf","placeholder":"​","style":"IPY_MODEL_f71b0fbf998e4063af5aa8ac4ceff3b4","value":" 261k/261k [00:00&lt;00:00, 2.90MB/s]"}},"22877f700d44481da7841573481758cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fed0a4a7cbd142abb4213f4bd0eceee5","placeholder":"​","style":"IPY_MODEL_919cbdecebfb47a0848456c6942eade6","value":"tokenizer_config.json: 100%"}},"22a95d5220b64c5090942342e82ed69c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aa87deafc174738b09dd59e66c44e1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12eee4d7b785495d9dd55c99d6ef119e","placeholder":"​","style":"IPY_MODEL_3aa4ec6f6bc74dc38c09321506d62bde","value":"config.json: 100%"}},"2d25baf756bc46638edce4da132b85f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f36f83e19624e7a97c21af2b35afa8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2fab30faf5d04d4fbbc755a9af69a773":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"310cbb29781d47ab964a9042d44b1e04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_feabc9588aa545ba9c13ff8974d5c420","placeholder":"​","style":"IPY_MODEL_89c6619bb8d74a08993d0a7e8f63523b","value":" 7.87k/7.87k [00:00&lt;00:00, 151kB/s]"}},"3136c8d368bd4ef5a23de90ca46bee11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"328c535197b84b35b59810e24402bf3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e101c846d3084e83934559e27696ab05","IPY_MODEL_6699f9546e38463aa538e2ae3a7101c8","IPY_MODEL_212e440165ce49f8bb240812c87cae21"],"layout":"IPY_MODEL_f916402be3b8414589de99f17cf3ffe9"}},"3562ec7d1eba4cb6a8ea0563462fa03d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"374ccbd720b24a43a803058a9351bdcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22877f700d44481da7841573481758cb","IPY_MODEL_d84736bbb0c44bc090dec69f1a0d84e7","IPY_MODEL_c7da6872ecb24197a05696cfdfd8edf4"],"layout":"IPY_MODEL_60c9d82417474c3389ebbed28633719a"}},"3aa4ec6f6bc74dc38c09321506d62bde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43a7fddbd2a44a6883ba449e7a71dd09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c63462ef11a4deab699c0a97d9902dc","IPY_MODEL_9a4c76392fd24d3e845af63107c51326","IPY_MODEL_6a2298fcc4bb4ba596def63e570397e9"],"layout":"IPY_MODEL_2d25baf756bc46638edce4da132b85f5"}},"4511f0011720407ca8680769b9b785a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46482e1ea1db4b36ad42fdff86b1c474":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"493c6c6cb1ce4a909c1c9d79c433b8f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f424a6764a3498e8d34a891c63dae3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51996b96a93a4aaf89287e1c61a09146":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c666c1df7d7840998684ad53a20257b3","placeholder":"​","style":"IPY_MODEL_9b999984ba4743e98d3123b2d63ef3db","value":" 7.19k/7.19k [00:00&lt;00:00, 237kB/s]"}},"5b918b2e20cc4dbd9fade2c93a0722bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2aa87deafc174738b09dd59e66c44e1b","IPY_MODEL_5c4f485df6784c2e8f94480ea5f4c6cf","IPY_MODEL_51996b96a93a4aaf89287e1c61a09146"],"layout":"IPY_MODEL_c3868c30833e47bc85ab2669cefe8941"}},"5c4f485df6784c2e8f94480ea5f4c6cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3562ec7d1eba4cb6a8ea0563462fa03d","max":7192,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf1b25ce568d490f90d6d7f34722af26","value":7192}},"5c63462ef11a4deab699c0a97d9902dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abc65f3ca99e4d7897287dd881469520","placeholder":"​","style":"IPY_MODEL_d3d800e788774ef096e3014ec83674a4","value":"preprocessor_config.json: 100%"}},"60439a7e5ee7476a84097a2080c7e891":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c9d82417474c3389ebbed28633719a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60f36bc310a244b6b86bbc244f5c18da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1c2dbbc7bbd49c5bafadc642a268b95","placeholder":"​","style":"IPY_MODEL_4f424a6764a3498e8d34a891c63dae3e","value":"config.json: 100%"}},"6699f9546e38463aa538e2ae3a7101c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e8232c8d486405e9ef994a902012561","max":261295,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e75773f1c42b43df8599bf0d3f3cc5ca","value":261295}},"696975d59eda4655bf07aa2117b0bb06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a2298fcc4bb4ba596def63e570397e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbd031723227448abd28e6eeffa3a78e","placeholder":"​","style":"IPY_MODEL_c9a6a348142b48f782c24312e88eb23b","value":" 220/220 [00:00&lt;00:00, 2.50kB/s]"}},"89c6619bb8d74a08993d0a7e8f63523b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"919cbdecebfb47a0848456c6942eade6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a4c76392fd24d3e845af63107c51326":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8652ae1921f4f609ec11d90fa09e05f","max":220,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4511f0011720407ca8680769b9b785a0","value":220}},"9b999984ba4743e98d3123b2d63ef3db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9eff5cc841e84fb0b2523aec9bb7bef9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abc65f3ca99e4d7897287dd881469520":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1883356adf94d4c822b957cc1d5001c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6aefe882eb24ebbac9f50d69b5cc908":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8a4b2fffb1045b6a99ebbfb19fe6b24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf1b25ce568d490f90d6d7f34722af26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3868c30833e47bc85ab2669cefe8941":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c666c1df7d7840998684ad53a20257b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7da6872ecb24197a05696cfdfd8edf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fab30faf5d04d4fbbc755a9af69a773","placeholder":"​","style":"IPY_MODEL_696975d59eda4655bf07aa2117b0bb06","value":" 333/333 [00:00&lt;00:00, 14.2kB/s]"}},"c8652ae1921f4f609ec11d90fa09e05f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9a6a348142b48f782c24312e88eb23b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3d800e788774ef096e3014ec83674a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d84736bbb0c44bc090dec69f1a0d84e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1883356adf94d4c822b957cc1d5001c","max":333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f36f83e19624e7a97c21af2b35afa8b","value":333}},"e101c846d3084e83934559e27696ab05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8a4b2fffb1045b6a99ebbfb19fe6b24","placeholder":"​","style":"IPY_MODEL_efb19dfc296541ac96157016035dbce2","value":"spiece.model: 100%"}},"e3b6e2803a2d405ca4ebabc16b715e39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb4e3728834a4b5c84aaefdde0da8ec1","IPY_MODEL_121faed1dd3440409708e26a5914cefe","IPY_MODEL_310cbb29781d47ab964a9042d44b1e04"],"layout":"IPY_MODEL_60439a7e5ee7476a84097a2080c7e891"}},"e5a39ac667c34cb6a9743192595b1580":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60f36bc310a244b6b86bbc244f5c18da","IPY_MODEL_febd44b0d8514717b8dcd3101d086567","IPY_MODEL_0010b3fe765b497ab0025ce2cc374665"],"layout":"IPY_MODEL_22a95d5220b64c5090942342e82ed69c"}},"e75773f1c42b43df8599bf0d3f3cc5ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eaa7ab4eed7f4a5cb08cb2546932774b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb4e3728834a4b5c84aaefdde0da8ec1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaa7ab4eed7f4a5cb08cb2546932774b","placeholder":"​","style":"IPY_MODEL_3136c8d368bd4ef5a23de90ca46bee11","value":"config.json: 100%"}},"efb19dfc296541ac96157016035dbce2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1c2dbbc7bbd49c5bafadc642a268b95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f71b0fbf998e4063af5aa8ac4ceff3b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f916402be3b8414589de99f17cf3ffe9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbd031723227448abd28e6eeffa3a78e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb22e1bb65541889e0d046d2ba79ddf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe423cc4b9de480fbbcfbbbcf45bc8ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"feabc9588aa545ba9c13ff8974d5c420":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"febd44b0d8514717b8dcd3101d086567":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9eff5cc841e84fb0b2523aec9bb7bef9","max":1534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6aefe882eb24ebbac9f50d69b5cc908","value":1534}},"fed0a4a7cbd142abb4213f4bd0eceee5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3a84c5a7a49456f86cc94753a1116e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5825f23a0ec248e09943910922cb194d","IPY_MODEL_8e0f45393cbd406897a8cce44f04870f","IPY_MODEL_6b7e5c39e1164cdf8f2a58565f752554"],"layout":"IPY_MODEL_60dd222cfb1f4e0290ecfa049a8dd0f2"}},"5825f23a0ec248e09943910922cb194d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01cce141af454b6bbffc413ed5aaf67e","placeholder":"​","style":"IPY_MODEL_e42384a0b13545c1b32d1129ee7969f3","value":"model.safetensors: 100%"}},"8e0f45393cbd406897a8cce44f04870f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4aa41cf499d04e5c9a57fbabb89870ab","max":891646390,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e01f844a06924068881a0b0417547410","value":891646390}},"6b7e5c39e1164cdf8f2a58565f752554":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dc0e953514a49bf81861856b7c76447","placeholder":"​","style":"IPY_MODEL_6aec00a7bfb54364a7c7d811b68a4297","value":" 892M/892M [00:17&lt;00:00, 54.3MB/s]"}},"60dd222cfb1f4e0290ecfa049a8dd0f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01cce141af454b6bbffc413ed5aaf67e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e42384a0b13545c1b32d1129ee7969f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4aa41cf499d04e5c9a57fbabb89870ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e01f844a06924068881a0b0417547410":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1dc0e953514a49bf81861856b7c76447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aec00a7bfb54364a7c7d811b68a4297":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}